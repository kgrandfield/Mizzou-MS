{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project Notebook\n",
    "\n",
    "### NOTE: If you build and run a notebook in the cloud, just copy it down in place of this one!  \n",
    "#### Be sure to have all your output captured within the notebook!\n",
    "#### <span style=\"background:yellow\">Be sure to save your work early and often!</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Specific_Project_1.png MISSING](../images/Specific_Project_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add code as needed in the cells below to produce your analytical products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## setting up postgres database in GCP\n",
    "gcloud sql connect finalproject-kg37m --user=postgres\n",
    "\n",
    "CREATE DATABASE reddit;\n",
    "\n",
    "\\connect reddit;\n",
    "\n",
    "CREATE TABLE entries (item VARCHAR(500), date TIMESTAMP, title VARCHAR(250), summary TEXT, link VARCHAR(500), sentiment_score VARCAHR(50), sentiment_magnitude FLOAT);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## setting up VM machine\n",
    "sudo apt update && sudo apt install python3 python3-dev python3-venv\n",
    "sudo apt install wget\n",
    "wget https://bootstrap.pypa.io/get-pip.py\n",
    "sudo python3 get-pip.py\n",
    "mkdir testproj\n",
    "cd testproj\n",
    "python3 -m venv env\n",
    "source env/bin/activate\n",
    "pip install google-cloud-storage\n",
    "pip install --upgrade google-cloud-storage\n",
    "pip3 install feedparser\n",
    "pip3 install bs4\n",
    "pip3 install BeautifulSoup4\n",
    "vim pythonscraper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## scrape Reddit RSS into JSON files\n",
    "import json\n",
    "import random\n",
    "import logging\n",
    "import os\n",
    "import feedparser\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "from google.cloud import storage\n",
    "\n",
    "a_reddit_rss_url = 'http://www.reddit.com/new/.rss?sort=new'\n",
    "\n",
    "def tag_visible(element):\n",
    "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def text_from_html(body):\n",
    "    soup = BeautifulSoup(body, 'html.parser')\n",
    "    texts = soup.findAll(text=True)\n",
    "    visible_texts = filter(tag_visible, texts)  \n",
    "    return u\" \".join(t.strip() for t in visible_texts)\n",
    "\n",
    "project = 'UMC DSA 8420 FS2021'\n",
    "bucket = 'reddit-kg37m'\n",
    "feed = feedparser.parse(a_reddit_rss_url)\n",
    "\n",
    "dict_keys = ['dttm', 'title', 'summary_text', 'link']\n",
    "temp_dict = dict()\n",
    "for key in dict_keys:\n",
    "    temp_dict[key] = []\n",
    "    \n",
    "if (feed['bozo'] == 1):\n",
    "    print(\"Error Reading/Parsing Feed XML Data\")\n",
    "\n",
    "else:\n",
    "    for item in feed[ \"items\" ]:\n",
    "        temp_dict['dttm'].append(item[ \"date\" ])\n",
    "        temp_dict['title'].append(item[ \"title\" ])\n",
    "        temp_dict['summary_text'].append(text_from_html(item[ \"summary\" ]))\n",
    "        temp_dict['link'].append(item[ \"link\" ])\n",
    "\n",
    "reddit_string = json.dumps(temp_dict)\n",
    "\n",
    "filename = 'file'+str(random.randint(1,1000))+'.json'\n",
    "\n",
    "blob = bucket.blob(filename)\n",
    "\n",
    "blob.upload_from_string(reddit_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## more VM setup\n",
    "sudo apt-get install apt-transport-https ca-certificates gnupg\n",
    "echo \"deb https://packages.cloud.google.com/apt cloud-sdk main\" | sudo tee -a /etc/apt/sources.list.d/google-cloud-sdk.list\n",
    "curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\n",
    "sudo apt-get update && sudo apt-get install google-cloud-sdk\n",
    "gcloud init\n",
    "wget https://dl.google.com/dl/cloudsdk/channels/rapid/downloads/google-cloud-sdk-182.0.0-linux-x86_64.tar.gz\n",
    "gcloud auth application-default login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## process files from the Storage Bucket with Natural Language API\n",
    "from google.cloud import storage\n",
    "from google.cloud import language_v1\n",
    "import json\n",
    "import csv\n",
    "\n",
    "project = 'UMC DSA 8420 FS2021'\n",
    "bucket = 'reddit-kg37m'\n",
    "\n",
    "def get_blob(_blob):\n",
    "    storage_client = storage.Client(project=project)\n",
    "    bucket = storage_client.get_bucket(_bucket_name)\n",
    "    blob = bucket.get_blob(_blob)\n",
    "    return blob.download_as_string()\n",
    "\n",
    "def list_blobs(bucket_name):\n",
    "    storage_client = storage.Client(project=project)\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    blobs = bucket.list_blobs()\n",
    "    return blobs\n",
    "\n",
    "for blob in list_blobs(bucket):\n",
    "    text = get_blob(blob)\n",
    "    document = language_v1.Document(content=text, type_=language_v1.Document.Type.PLAIN_TEXT)\n",
    "    sentiment = client.analyze_sentiment(request={\"document\": document}).document_sentiment\n",
    "    file = open(blob, \"a\")\n",
    "    file.write(\"sentiment_score: {}, sentiment_magnitude: {}\".format(sentiment.score, sentiment.magnitude))\n",
    "    file.close()\n",
    "    \n",
    "## put files together for upload\n",
    "files=list_blobs(bucket)\n",
    "\n",
    "def merge_JsonFiles(filename):\n",
    "    result = list()\n",
    "    for f1 in filename:\n",
    "        with open(f1, 'r') as infile:\n",
    "            result.append(json.load(infile))\n",
    "\n",
    "    with open('allblobs.json', 'w') as output_file:\n",
    "        json.dump(result, output_file)\n",
    "\n",
    "merge_JsonFiles(files)\n",
    "\n",
    "with open('allblobs.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    " \n",
    "csv_file = open('data_file.csv', 'w')\n",
    " \n",
    "# create the csv writer object\n",
    "csv_writer = csv.writer(data)\n",
    " \n",
    "# Counter variable used for writing\n",
    "# headers to the CSV file\n",
    "count = 0\n",
    " \n",
    "for entry in data:\n",
    "    if count == 0:\n",
    "\n",
    "        header = keys()\n",
    "        csv_writer.writerow(header)\n",
    "        count += 1\n",
    " \n",
    "    csv_writer.writerow(emp.values())\n",
    " \n",
    "csv_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## populate database with files\n",
    "gcloud sql connect finalproject-kg37m --user=postgres\n",
    "\n",
    "\\connect reddit;\n",
    "\n",
    "COPY *\n",
    "FROM 'data_file.csv'\n",
    "DELIMITER ','\n",
    "CSV HEADER;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## connect to postgres instance\n",
    "wget http://ipv4.whatismyv6.com/ -O getip\n",
    "grep -a1 \"Address of\" getip | grep '[0-9]\\{1,3\\}\\.[0-9]\\{1,3\\}\\.[0-9]\\{1,3\\}\\.[0-9]\\{1,3\\}'\n",
    "\n",
    "psql -h 35.223.210.188 -U postgres postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "#mypasswd = 'PASSWORD'\n",
    "mypasswd = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## pull data in tabular format\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "connection = psycopg2.connect(database = 'reddit', \n",
    "                              user = 'postgres', \n",
    "                              host = '35.223.210.188', \n",
    "                              password = mypasswd)\n",
    "with connection, connection.cursor() as cursor:\n",
    "    cursor.execute(\"SELECT * FROM entries\")\n",
    "    results = cursor.fetchall()\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## tabular data analysis\n",
    "df['sentiment_score'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## display data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df.sort_value('ddtm')\n",
    "\n",
    "plt.plot(df['ddtm'], df['sentiment_magnitude'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing your submission\n",
    "\n",
    "### Deliverables: \n",
    "   1. This or a replacement Notebook\n",
    "   1. An aggregateion of data in tabular format that conveyes something interesting about the Reddit RSS feed during your scraping.\n",
    "     * The table can be embedded or uploaded into this folder (CSV or Excel)\n",
    "   1. One or more data visualizations\n",
    "\n",
    "Imbed your image into this page by saving your data visualization as: `FINAL_PROJECT_IMAGE.png`  \n",
    "Upload it to the `module8/exercises/` folder.\n",
    "\n",
    "If you need to, change the file type to `.jpg` or `.jpeg` or ... whatever, then update the link in this cell (double click to edit).  \n",
    "Then re-run this markdown cell to see it.\n",
    "\n",
    "![FINAL_PROJECT_IMAGE.png MISSING](./exercises/FINAL_PROJECT_IMAGE.png)\n",
    "\n",
    "---\n",
    "## Summarize in the fields below\n",
    " 1. Describe the overall process and components you used for the project.\n",
    " 2. What is the key insight from the tabularization?\n",
    " 3. What is the key insight from the visualization?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### 1. \n",
    "# --------------------------\n",
    "I created a SQL database, a VM instance, and a bucket in GCP. Then, the I used the VM instance to run a Python program pulling posts from Reddit into text files stored in my bucket. These files were then processed with Google's Natural Language API, and then the text was used to poplate the SQL Database. From there, the data was pulled back out of the database into Python on Jupyter in order to conduct tabular and visual data analysis."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### 2. \n",
    "# --------------------------\n",
    "The tabularization would have shown counts by sentiment, giving a snapshot of sentiment distribution."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### 3.\n",
    "# --------------------------\n",
    "The visualization would show sentiment magnitude over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Save your Notebook!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
