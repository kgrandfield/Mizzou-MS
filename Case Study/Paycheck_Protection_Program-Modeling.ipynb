{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Modeling\n",
    "\n",
    "Due to the size of the dataset we will break it into manageable chunks for modeling. \n",
    "\n",
    "With that, our group has decided to parse out the states of Florida and Maryland for our analysis. These were chosen in favor of the senators from those states who were charimen of the committee overseeing the PPP. \n",
    "\n",
    "From there, we will conduct a RandomForestRegression to model out Cost Per Job as the dependant variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read back in Dataset\n",
    "\n",
    "ppp = pd.read_csv('PPP_DATASET.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn libraries\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Next we can split the datasets into specific states\n",
    "\n",
    "ppp_FL = ppp[ppp['ProjectState_FL'] == 1] #<---------- Grab only Florida\n",
    "ppp_MD = ppp[ppp['ProjectState_MD'] == 1] #<---------- Grab only Maryland\n",
    "\n",
    "# Since we no longer need project states we can drop those columns easily using a Regex\n",
    "\n",
    "ppp_FL.drop(list(ppp_FL.filter(regex = 'ProjectState')), axis = 1, inplace = True)\n",
    "ppp_MD.drop(list(ppp_MD.filter(regex = 'ProjectState')), axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the variables above, there are others that will not be pertinent to our model that we can drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppp_FL.drop(['Phase', 'BusinessAgeDescription_Change of Ownership',\n",
    "                      'BusinessAgeDescription_Startup, Loan Funds will Open Business', 'Race_Eskimo & Aleut', \n",
    "                      'Race_Multi Group', 'Race_Puerto Rican', 'BusinessType_501(c) – Non Profit except 3,4,6,', \n",
    "                      'BusinessType_501(c)19 – Non Profit Veterans',\n",
    "                      'BusinessType_501(c)3 – Non Profit', 'BusinessType_501(c)6 – Non Profit Membership', \n",
    "                      'BusinessType_Cooperative', 'BusinessType_Employee Stock Ownership Plan(ESOP)',\n",
    "                      'BusinessType_Housing Co-op', 'BusinessType_Independent Contractors', \n",
    "                      'BusinessType_Joint Venture', 'BusinessType_Limited Liability Partnership', \n",
    "                      'BusinessType_Non-Profit Childcare Center', 'BusinessType_Non-Profit Organization',\n",
    "                      'BusinessType_Partnership', 'BusinessType_Professional Association',\n",
    "                      'BusinessType_Qualified Joint-Venture (spouses)', 'BusinessType_Rollover as Business Start-Ups (ROB', \n",
    "                      'BusinessType_Self-Employed Individuals', 'BusinessType_Single Member LLC', \n",
    "                      'BusinessType_Sole Proprietorship', 'BusinessType_Tenant in Common', 'BusinessType_Tribal Concerns',\n",
    "                      'BusinessType_Trust', 'CurrentApprovalAmount', 'ForgivenessAmount'], \n",
    "            axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the models\n",
    "\n",
    "As stated above there are 2 models we can end up building that would be important for our presenation. These would be for the states of Florida and Maryland.\n",
    "\n",
    "Below we have set up the initial models for both Florida and Maryland, and we will dive deeper into the Florida model below as we consider what features are valuable for us to consider to improve accuracy of model and decrease concerns around over-fitting.\n",
    "\n",
    "### Florida Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ppp_FL.drop('CostPerJob', axis = 1)\n",
    "y = ppp_FL['CostPerJob']\n",
    "\n",
    "XFL_train, XFL_test, yFL_train, yFL_test = train_test_split(X, y, test_size=.1)\n",
    "yFL_test = np.array(yFL_test)\n",
    "\n",
    "rf_FL = RandomForestRegressor()\n",
    "rf_FL.fit(XFL_train, yFL_train)\n",
    "\n",
    "rf_FL.score(XFL_train, yFL_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maryland Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ppp_MD.drop('CostPerJob', axis = 1)\n",
    "y = ppp_MD['CostPerJob']\n",
    "\n",
    "XMD_train, XMD_test, yMD_train, yMD_test = train_test_split(X, y, test_size=.1)\n",
    "\n",
    "rf_MD = RandomForestRegressor()\n",
    "rf_MD.fit(XMD_train, yMD_train)\n",
    "\n",
    "rf_MD.score(XMD_train, yMD_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting our Random Forest Models\n",
    "\n",
    "In order to interpret our models we will be using material to interpret the random forest developed here: https://coderzcolumn.com/tutorials/machine-learning/treeinterpreter-interpreting-tree-based-models-prediction-of-individual-sample. Information about treeinterpreter can be found here: https://github.com/andosa/treeinterpreter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install treeinterpreter\n",
    "\n",
    "from treeinterpreter import treeinterpreter as ti\n",
    "\n",
    "preds, bias, contributions = ti.predict(rf_FL, XFL_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Bias For Sample 0                        : %.2f\"%bias[0])\n",
    "print(\"Constributions For Sample 0              : %s\"%contributions[0])\n",
    "print(\"Prediction Based on Bias & Contributions : %.2f\"%(bias[0] + contributions[0].sum()))\n",
    "print(\"Actual Target Value                      : %.2f\"%yFL_test[0])\n",
    "print(\"Target Value As Per Treeinterpreter      : %.2f\"%preds[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random_sample = random.randint(1, len(XFL_test))\n",
    "print(\"Selected Sample     : %d\"%random_sample)\n",
    "print(\"Actual Target Value : %.2f\"%yFL_test[random_sample])\n",
    "print(\"Predicted Value     : %.2f\"%preds[random_sample][0])\n",
    "\n",
    "def create_contrbutions_df(contributions, random_sample, feature_names):\n",
    "    contribs = contributions[random_sample].tolist()\n",
    "    contribs.insert(0, bias[random_sample])\n",
    "    contribs = np.array(contribs)\n",
    "    contrib_df = pd.DataFrame(data=contribs, index=[\"Base\"] + feature_names, columns=[\"Contributions\"])\n",
    "    prediction = contrib_df.Contributions.sum()\n",
    "    contrib_df.loc[\"Prediction\"] = prediction\n",
    "    return contrib_df\n",
    "\n",
    "contrib_df = create_contrbutions_df(contributions, random_sample, ppp_FL.columns)\n",
    "contrib_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "def create_waterfall_chart(contrib_df, prediction):\n",
    "    fig = go.Figure(go.Waterfall(\n",
    "        name = \"Prediction\", #orientation = \"h\", \n",
    "        measure = [\"relative\"] * (len(contrib_df)-1) + [\"total\"],\n",
    "        x = contrib_df.index,\n",
    "        y = contrib_df.Contributions,\n",
    "        connector = {\"mode\":\"between\", \"line\":{\"width\":4, \"color\":\"rgb(0, 0, 0)\", \"dash\":\"solid\"}}\n",
    "    ))\n",
    "\n",
    "    fig.update_layout(title = \"Prediction : %s\"%prediction)\n",
    "\n",
    "    return fig\n",
    "\n",
    "create_waterfall_chart(contrib_df, contrib_df.loc[\"Prediction\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on what we see here, we can improve our model even further by reducing the variables and still get contributions. Using the contributions table above we will remove some features that are not contributing to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppp_FL.drop(['InitialApprovalAmount', 'RENT_PROCEED', 'REFINANCE_EIDL_PROCEED',\n",
    "                     'HEALTH_CARE_PROCEED', 'DEBT_INTEREST_PROCEED', 'FranchiseYN', 'ForgivenYN', \n",
    "                     'Party_R', 'BusinessAgeDescription_Existing or more than 2 years old',\n",
    "                     'Race_American Indian or Alaska Native', 'Race_Asian', 'Race_Native Hawaiian or Other Pacific Islander',\n",
    "                     'Race_Unanswered', 'Race_White', 'Ethnicity_Hispanic or Latino', 'Ethnicity_Not Hispanic or Latino', \n",
    "                     'Ethnicity_Unknown/NotStated', 'BusinessType_Subchapter S Corporation', 'Gender_Female Owned',\n",
    "                     'Veteran_Veteran', 'RuralYN_R', 'RuralYN_U'], axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have trimmed the set to more valuable features we can re-run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ppp_FL.drop('CostPerJob', axis = 1)\n",
    "y = ppp_FL['CostPerJob']\n",
    "\n",
    "XFL_train, XFL_test, yFL_train, yFL_test = train_test_split(X, y, test_size=.1)\n",
    "yFL_test = np.array(yFL_test)\n",
    "\n",
    "rf_FL = RandomForestRegressor()\n",
    "rf_FL.fit(XFL_train, yFL_train)\n",
    "\n",
    "rf_FL.score(XFL_train, yFL_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Let's consider the feature importance for the Florida model:\n",
    "\n",
    "importanceFL = rf_FL.feature_importances_\n",
    "importanceFL\n",
    "\n",
    "for i,v in enumerate(importanceFL):\n",
    "    print('Feature: %0d, Score: %.5f' % (i,v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "\n",
    "pyplot.bar([x for x in range(len(importanceFL))], importanceFL)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importancesFL = pd.DataFrame({'feature':XFL_train.columns, 'importance': np.round(rf_FL.feature_importances_,3)})\n",
    "importancesFL = importancesFL.sort_values('importance',ascending=False).set_index('feature')\n",
    "\n",
    "importancesFL.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importancesFL.plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is some verbiage taken from a great piece about treeinterpreter (https://coderzcolumn.com/tutorials/machine-learning/treeinterpreter-interpreting-tree-based-models-prediction-of-individual-sample).  This is meant just to help with interpretation and is a direct quote:\n",
    "\n",
    "\"The treeinterpreter is based on a concept that when making a particular prediction decision tree or random forest follows a particular path to come to that prediction. Each node in the decision tree represent some feature and makes decisions based on the feature value in the sample. The treeinterpreter divides prediction region space into regions the same as the number of leaves present in that tree. At each internal node in a tree, the prediction value will be the average of all possible predictions in data from the path going through that node. We'll have the average value for the root node as well this way which will be the average of all predictions. This way we'll have some prediction value at each node in the tree. The treeinterpreter uses these values to find out the contributions of each feature in prediction by finding out the difference in prediction by a particular node and the node in the path before it. It follows the same process for the random forest where there is more than one tree and the final prediction is taken based on an average of all trees predictions....The treeinterpreter takes as input tree-based model and samples and returns the base value for each sample, contributions of each feature into a prediction of each sample, and predictions for each sample.\"\n",
    "\n",
    "\"The treeinterpreter has a single method named predict() which takes as input model instance and dataset for which we need explanations. It returns three arrays as output.\n",
    "\n",
    "- The first array is predictions for a number of samples passed to the method.\n",
    "- The second array is bias or base value for each sample of data to which individual feature contribution will be added to generate a final prediction.\n",
    "- The third array is of size (#samples x #no_of_features) as it has the contribution of each feature for each sample which gets added to base/bias value to generate predictions.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, bias, contributions = ti.predict(rf_FL, XFL_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Bias For Sample 0                        : %.2f\"%bias[0])\n",
    "print(\"Constributions For Sample 0              : %s\"%contributions[0])\n",
    "print(\"Prediction Based on Bias & Contributions : %.2f\"%(bias[0] + contributions[0].sum()))\n",
    "print(\"Actual Target Value                      : %.2f\"%yFL_test[0])\n",
    "print(\"Target Value As Per Treeinterpreter      : %.2f\"%preds[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_sample = random.randint(1, len(XFL_test))\n",
    "print(\"Selected Sample     : %d\"%random_sample)\n",
    "print(\"Actual Target Value : %.2f\"%yFL_test[random_sample])\n",
    "print(\"Predicted Value     : %.2f\"%preds[random_sample][0])\n",
    "\n",
    "\"\"\"def create_contrbutions_df(contributions, random_sample, feature_names):\n",
    "    contribs = contributions[random_sample].tolist()\n",
    "    contribs.insert(0, bias[random_sample])\n",
    "    contribs = np.array(contribs)\n",
    "    contrib_df = pd.DataFrame(data=contribs, index=[\"Base\"] + feature_names, columns=[\"Contributions\"])\n",
    "    prediction = contrib_df.Contributions.sum()\n",
    "    contrib_df.loc[\"Prediction\"] = prediction\n",
    "    return contrib_df\"\"\"\n",
    "\n",
    "contrib_df = create_contrbutions_df(contributions, random_sample, ppp_FL.columns)\n",
    "contrib_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_waterfall_chart(contrib_df, contrib_df.loc[\"Prediction\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
