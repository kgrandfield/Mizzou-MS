,id,title,url,author,subreddit,timestamp,flair,points,comments,text,sentiment
0,q5fi12,[D] Machine Learning - WAYR (What Are You Reading) - Week 123,https://www.reddit.com/r/MachineLearning/comments/q5fi12/d_machine_learning_wayr_what_are_you_reading_week/,ML_WAYR_bot,MachineLearning,1970-01-01 00:00:01.633896005,Discussion,11,3,"This is a place to share machine learning research papers, journals, and articles that you're reading this week. If it relates to what you're researching, by all means elaborate and give us your insight, otherwise it could just be an interesting paper you've read.Please try to provide some insight from your understanding and please don't post things which are present in wiki.Preferably you should link the arxiv page (not the PDF, you can easily access the PDF from the summary page but not the other way around) or any other pertinent links.Previous weeks :|1-10|11-20|21-30|31-40|41-50|51-60|61-70|71-80|81-90|91-100|101-110|111-120|121-130||----|-----|-----|-----|-----|-----|-----|-----|-----|------|-------|-------|-------||[Week 1](https://www.reddit.com/4qyjiq)|[Week 11](https://www.reddit.com/57xw56)|[Week 21](https://www.reddit.com/60ildf)|[Week 31](https://www.reddit.com/6s0k1u)|[Week 41](https://www.reddit.com/7tn2ax)|[Week 51](https://reddit.com/9s9el5)|[Week 61](https://reddit.com/bfsx4z)|[Week 71](https://reddit.com/d7vno3)|[Week 81](https://reddit.com/f1f0iq)|[Week 91](https://reddit.com/hlt38o)|[Week 101](https://reddit.com/k81ywb)|[Week 111](https://reddit.com/myg8sm)|[Week 121](https://reddit.com/pmzx3g)||||||||||||[Week 2](https://www.reddit.com/4s2xqm)|[Week 12](https://www.reddit.com/5acb1t)|[Week 22](https://www.reddit.com/64jwde)|[Week 32](https://www.reddit.com/72ab5y)|[Week 42](https://www.reddit.com/7wvjfk)|[Week 52](https://reddit.com/a4opot)|[Week 62](https://reddit.com/bl29ov)|[Week 72](https://reddit.com/de8h48)|[Week 82](https://reddit.com/f8fs6z)|[Week 92](https://reddit.com/hu6zq9)|[Week 102](https://reddit.com/kh27nx)|[Week 112](https://reddit.com/n8m6ds)|[Week 122](https://reddit.com/pw14z5)|||[Week 3](https://www.reddit.com/4t7mqm)|[Week 13](https://www.reddit.com/5cwfb6)|[Week 23](https://www.reddit.com/674331)|[Week 33](https://www.reddit.com/75405d)|[Week 43](https://www.reddit.com/807ex4)|[Week 53](https://reddit.com/a8yaro)|[Week 63](https://reddit.com/bqlb3v)|[Week 73](https://reddit.com/dkox1s)|[Week 83](https://reddit.com/ffi41b)|[Week 93](https://reddit.com/iaz892)|[Week 103](https://reddit.com/kpsxtc)|[Week 113](https://reddit.com/njfsc6)|||[Week 4](https://www.reddit.com/4ub2kw)|[Week 14](https://www.reddit.com/5fc5mh)|[Week 24](https://www.reddit.com/68hhhb)|[Week 34](https://www.reddit.com/782js9)|[Week 44](https://reddit.com/8aluhs)|[Week 54](https://reddit.com/ad9ssz)|[Week 64](https://reddit.com/bw1jm7)|[Week 74](https://reddit.com/dr6nca)|[Week 84](https://reddit.com/fn62r1)|[Week 94](https://reddit.com/ijjcep)|[Week 104](https://reddit.com/kzevku)|[Week 114](https://reddit.com/ntu6lq)|||[Week 5](https://www.reddit.com/4xomf7)|[Week 15](https://www.reddit.com/5hy4ur)|[Week 25](https://www.reddit.com/69teiz)|[Week 35](https://www.reddit.com/7b0av0)|[Week 45](https://reddit.com/8tnnez)|[Week 55](https://reddit.com/ai29gi)|[Week 65](https://reddit.com/c7itkk)|[Week 75](https://reddit.com/dxshkg)|[Week 85](https://reddit.com/fvk7j6)|[Week 95](https://reddit.com/is5hj9)|[Week 105](https://reddit.com/l9lvgs)|[Week 115](https://reddit.com/o4dph1)|||[Week 6](https://www.reddit.com/4zcyvk)|[Week 16](https://www.reddit.com/5kd6vd)|[Week 26](https://www.reddit.com/6d7nb1)|[Week 36](https://www.reddit.com/7e3fx6)|[Week 46](https://reddit.com/8x48oj)|[Week 56](https://reddit.com/ap8ctk)|[Week 66](https://reddit.com/cd7gko)|[Week 76](https://reddit.com/e4nmyk)|[Week 86](https://reddit.com/g4eavg)|[Week 96](https://reddit.com/j0xr24)|[Week 106](https://reddit.com/ljx92n)|[Week 116](https://reddit.com/odrudt)|||[Week 7](https://www.reddit.com/52t6mo)|[Week 17](https://www.reddit.com/5ob7dx)|[Week 27](https://www.reddit.com/6gngwc)|[Week 37](https://www.reddit.com/7hcc2c)|[Week 47](https://reddit.com/910jmh)|[Week 57](https://reddit.com/auci7c)|[Week 67](https://reddit.com/cj0kyc)|[Week 77](https://reddit.com/eb4lxk)|[Week 87](https://reddit.com/gcx3uf)|[Week 97](https://reddit.com/j9cbfs)|[Week 107](https://reddit.com/luqbxl)|[Week 117](https://reddit.com/omy345)|||[Week 8](https://www.reddit.com/53heol)|[Week 18](https://www.reddit.com/5r14yd)|[Week 28](https://www.reddit.com/6jgdva)|[Week 38](https://www.reddit.com/7kgcqr)|[Week 48](https://reddit.com/94up0g)|[Week 58](https://reddit.com/azjoht)|[Week 68](https://reddit.com/cp1jex)|[Week 78](https://reddit.com/ehbfst)|[Week 88](https://reddit.com/glm6sv)|[Week 98](https://reddit.com/jhzz9v)|[Week 108](https://reddit.com/m52u5z)|[Week 118](https://reddit.com/ovz52j)|||[Week 9](https://www.reddit.com/54kvsu)|[Week 19](https://www.reddit.com/5tt9cz)|[Week 29](https://www.reddit.com/6m9l1v)|[Week 39](https://www.reddit.com/7nayri)|[Week 49](https://reddit.com/98n2rt)|[Week 59](https://reddit.com/b50r5y)|[Week 69](https://reddit.com/cvde5a)|[Week 79](https://reddit.com/entcxy)|[Week 89](https://reddit.com/gu5t0d)|[Week 99](https://reddit.com/jqjgo2)|[Week 109](https://reddit.com/mf8m6u)|[Week 119](https://reddit.com/p50knh)|||[Week 10](https://www.reddit.com/56s2oa)|[Week 20](https://www.reddit.com/5wh2wb)|[Week 30](https://www.reddit.com/6p3ha7)|[Week 40](https://www.reddit.com/7qel9p)|[Week 50](https://reddit.com/9cf158)|[Week 60](https://reddit.com/bakew0)|[Week 70](https://reddit.com/d1g1k9)|[Week 80](https://reddit.com/euctyw)|[Week 90](https://reddit.com/hddf7j)|[Week 100](https://reddit.com/jz3evt)|[Week 110](https://reddit.com/moy40m)|[Week 120](https://reddit.com/pe2idh)||Most upvoted papers two weeks ago:/u/DeepSquare_io: [paper](https://www.deepsquare.io/)/u/CatalyzeX_code_bot: [Paper link](https://arxiv.org/abs/2012.09841)/u/Different_Fig4002: [Advances in MetaDL](http://proceedings.mlr.press/v140/el-baz21a/el-baz21a.pdf)Besides that, there are no rules, have fun.",POS
1,q59nxu,[D] Simple Questions Thread,https://www.reddit.com/r/MachineLearning/comments/q59nxu/d_simple_questions_thread/,AutoModerator,MachineLearning,1970-01-01 00:00:01.633878019,Discussion,3,25,Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!Thread will stay alive until next one so keep posting after the date in the title.Thanks to everyone for answering questions in the previous thread!,POS
2,q7w9i2,[P] An illustrated tour of wav2vec 2.0,https://www.reddit.com/r/MachineLearning/comments/q7w9i2/p_an_illustrated_tour_of_wav2vec_20/,jonathanbgn,MachineLearning,1970-01-01 00:00:01.634204121,Project,98,3,"When Transformers started getting popular for NLP, we saw great visualizations to understand better the internals of these models like The Illustrated BERT, GPT...I haven't seen much like that for speech processing, so I wrote this quick post to illustrate the architecture and pre-training process of wav2vec 2.0 (now part of the HuggingFace library).[https://jonathanbgn.com/2021/09/30/illustrated-wav2vec-2.html](https://jonathanbgn.com/2021/09/30/illustrated-wav2vec-2.html)Hope this is useful : )",POS
3,q7wy37,[P] List of ICLR 2022 submissions that People are Talking About.,https://www.reddit.com/r/MachineLearning/comments/q7wy37/p_list_of_iclr_2022_submissions_that_people_are/,hnipun,MachineLearning,1970-01-01 00:00:01.634207468,Project,43,10,"We made a list of ICLR 2022 submissions (with around 3400 submissions).**Link:** [https://papers.labml.ai/papers/iclr\_2022](https://papers.labml.ai/papers/iclr_2022)&#x200B;1. **Two lines summaries**The paper list shows a short summary along with the title. This lets you quickly scan the papers. We are working on improving the quality of the summaries.2. **Similar papers**Show papers that are similar to the given paper. These aren't perfect but quite useful.3. **Source codes, Videos, and Comments.**Links Github repositories, Youtube videos, Reddit, Hacker News and Twitter discussions.&#x200B;***PS:*** *This is a side project that I started in May and launched on this sub-Reddit (*[*previous post*](https://www.reddit.com/r/MachineLearning/comments/nig3h7/p_find_trending_machine_learning_research_papers/)*). Your encouragement and support helped us bring it to the current state.*",POS
4,q86n8b,[D] Chelsea Finn on Meta Learning & Model Based Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/q86n8b/d_chelsea_finn_on_meta_learning_model_based/,regalalgorithm,MachineLearning,1970-01-01 00:00:01.634238977,Discussion,6,0,"Hi there! Think lots of you might enjoy this [new interview with professor Chelsea Finn](https://thegradientpub.substack.com/p/chelsea-finn-on-meta-learning-and). In case you don't know, her research deals with creating the capability of robots and other agents to develop broadly intelligent behavior through learning and interaction.We mostly talk about her research, including these works:* [Learning to Learn with Gradients](https://www2.eecs.berkeley.edu/Pubs/TechRpts/2018/EECS-2018-105.html)* [Visual Model-Based Reinforcement Learning as a Path towards Generalist Robots](https://bair.berkeley.edu/blog/2018/11/30/visual-rl/)* [RoboNet: A Dataset for Large-Scale Multi-Robot Learning](https://arxiv.org/abs/1910.11215)* [Greedy Hierarchical Variational Autoencoders for Large-Scale Video](https://arxiv.org/abs/2103.04174)* [Example-Driven Model-Based Reinforcement Learning for Solving Long-Horizon Visuomotor TasksÂ Â ](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=vfPE6hgAAAAJ&sortby=pubdate&citation_for_view=vfPE6hgAAAAJ:-_dYPAW6P2MC)Curious what you think about the interview style, not sure if we get too technical for people outside of RL. Seems like fun to me, but perhaps inaccessible for people who don't work on this stuff.",POS
5,q7zgzc,[P] Demo: Chatting with Gandalf using GPT-J-6B,https://www.reddit.com/r/MachineLearning/comments/q7zgzc/p_demo_chatting_with_gandalf_using_gptj6b/,cranthir_,MachineLearning,1970-01-01 00:00:01.634217262,Project,15,6,"Hello everyone,I've build a bot (with GPT-J-6B) on HuggingFace where you can chat and ask questions to Gandalf.For instance, he explained why he didn't use the great eagles to fly Frodo to Mordor.https://i.redd.it/twvpsgz21ft71.gifThe idea is that by using gpt we can develop meaningful conversations with the player. **If you want to try it** ðŸ‘‰  [https://huggingface.co/spaces/ThomasSimonini/Chat-with-Gandalf-GPT-J6B](https://huggingface.co/spaces/ThomasSimonini/Chat-with-Gandalf-GPT-J6B)**I would love about what the ML community think about that and your feedbacks.**If you want to see **my other demos** ðŸ‘‰ [https://twitter.com/ThomasSimonini](https://twitter.com/ThomasSimonini)",POS
6,q83ac9,"[D] Guys who set up infra for ML training, How do I debug my setup?",https://www.reddit.com/r/MachineLearning/comments/q83ac9/d_guys_who_set_up_infra_for_ml_training_how_do_i/,PaganPasta,MachineLearning,1970-01-01 00:00:01.634229042,Discussion,6,3,"Hi,I am talking about setting up GPU clusters, connecting with a storage and running a manager like RunAI or SLURM on top. &#x200B;At the moment, I am struggling to get 2x[DGX](https://www.nvidia.com/en-gb/data-center/dgx-a100/)\-A100 with 2TB of main memory and [excelero](https://www.excelero.com/product/pny-3s-2400-ai-optimized-storage-server-nvmesh/) pny-3s-2400(8TB) storage with [Infiniband](https://en.wikipedia.org/wiki/InfiniBand) connectivity to run more than 3 training job simultaneously without causing massive delay in training. Training is standard Imagenet-1k image classification. One possible solution is to use Webdatasets(Pytorch) for storing data which does make the setup perform 'better' with increase in training jobs. But, I haven't seen this being done from work that I see on github from official sources to train on clusters. So I am not sure if data format is the **only** problem. &#x200B;My questions in no particular order are:1. How do you compute IOPS(i/o operations per second) supported by the setup?2. Is the current incapable of handling more than 3 simultaneous training jobs? 3. Any upgrades that might be required?4. Where do I start to narrow down on the problem?5. How do people at Google/FB etc have the setup behind the scene?",NEG
7,q86kqn,[D] What are some ideas that are hyped up in machine learning research but don't actually get used in industry (and vice versa)?,https://www.reddit.com/r/MachineLearning/comments/q86kqn/d_what_are_some_ideas_that_are_hyped_up_in/,NedML,MachineLearning,1970-01-01 00:00:01.634238773,Discussion,6,12,"I was attending a talk by someone who was studying differential privacy and at some point, he revealed that people who he talked within the industry do not appear to be quite excited about the idea, either complaining that it is too hard to understand or provides too little incentive for them to implement. Are there some other ideas that are extremely popular or heavily looked at in academia but don't get their time of the day in the industry (or vice versa)?",POS
8,q7ypjd,[D] ONNX ecosystem,https://www.reddit.com/r/MachineLearning/comments/q7ypjd/d_onnx_ecosystem/,davidmezzetti,MachineLearning,1970-01-01 00:00:01.634214636,Discussion,11,11,"The [ONNX](https://github.com/microsoft/onnxruntime) runtime provides a common serialization format for machine learning models. ONNX supports a number of different platforms/languages and has features built in to help reduce inference time.PyTorch has robust support for exporting Torch models to ONNX. This enables exporting Hugging Face Transformers and/or other downstream models directly to ONNX. With the [onnxmltools](https://github.com/onnx/onnxmltools) library, traditional models from scikit-learn, XGBoost and others can be exported to ONNX.ONNX opens an avenue for direct inference using a number of languages and platforms. For example, a model could be run directly on Android to limit data sent to a third party service. ONNX is an exciting development with a lot of promise.Interested in hearing if anyone is using ONNX in their work. I've extensively started to use ONNX with [txtai](https://github.com/neuml/txtai) ([example](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/18_Export_and_run_models_with_ONNX.ipynb), [example](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/21_Export_and_run_other_machine_learning_models.ipynb)).",POS
9,q832od,"[R]: Bert for Embeddings, different layer capture different meaning?",https://www.reddit.com/r/MachineLearning/comments/q832od/r_bert_for_embeddings_different_layer_capture/,randy_wales_qq,MachineLearning,1970-01-01 00:00:01.634228410,Research,6,1,"HelloI wanna use Bert as embeddings. I heard that different layer of BERT captures different semantic meaning, is that true? Can I read more about this?",POS
10,q7od7f,[P] StyleGAN3 + CLIP,https://www.reddit.com/r/MachineLearning/comments/q7od7f/p_stylegan3_clip/,Ouhenio,MachineLearning,1970-01-01 00:00:01.634171140,Project,70,27,"Today [nshepperd](https://twitter.com/nshepperd1) published [this notebook](https://colab.research.google.com/drive/1eYlenR1GHPZXt-YuvXabzO9wfh9CWY36#scrollTo=V\_rq-N2m0Tlb) to use StyleGAN3 with CLIP.If you want to use a version with a friendlier interface, I made [this notebook](https://colab.research.google.com/github/ouhenio/StyleGAN3-CLIP-notebook/blob/main/StyleGAN3%2BCLIP.ipynb) based on the one created by nshepperd.Since it's a work in progress, I'll also share [this repo](https://github.com/ouhenio/StyleGAN3-CLIP-notebook) where I've been updating the notebook.PS: As you can see, most of the code was made by nshepperd, I just formatted it and added the video generation capabilities, so all the credits go to her.PS 2: If someone can help me figure out the correct license for this I'd be very thankful.",POS
11,q80wgg,[P] Transformers from Spin Models: Approximate Free Energy Minimization,https://www.reddit.com/r/MachineLearning/comments/q80wgg/p_transformers_from_spin_models_approximate_free/,mcbal70,MachineLearning,1970-01-01 00:00:01.634221824,Project,4,1,"In a previous project on [Deep Implicit Attention: A Mean-Field Theory Perspective on Attention Mechanisms](https://mcbal.github.io/post/deep-implicit-attention-a-mean-field-theory-perspective-on-attention-mechanisms/), we introduced a mean-field theory perspective on transformer modules. We showed how their outputs can be understood as mean-field spin expectation values of simple Ising-like vector-spin systems. Physically, the process of training a transformer module can be interpreted as a classical many-body system modulating its behavior by learning how to respond to being probed by incoming data.In this follow-up project, we flesh out the idea of looking at transformer modules as spin systems. Having identified vector-spin systems as plausible physical models underlying transformers, we turn to the 1960s statistical-mechanics literature to look for inspiration on how to deal with their partition functions. We rediscover that the partition function of a particular class of vector-spin models can be approximated in the limit of large local spin dimension using steepest descent, leading to approximate yet tractable expressions for the free energy and other derived quantities.Combining these canonical results from statistical mechanics with modern differentiable programming, we implement a differentiable vector-spin model. Internally, the model uses an implicit layer to solve for the stationary point of the partition function in a differentiable way. We then construct a transformer-like attention module which encapsulates the spin model by routing inputs to applied magnetic fields and spin expectation values to outputs. The latter are obtained by following the familiar recipe of statistical mechanics: differentiating the spin modelâ€™s (approximate) log partition function with respect to its conjugate input variables.**Blog post:** https://mcbal.github.io/post/transformers-from-spin-models-approximate-free-energy-minimization/**Code:** https://github.com/mcbal/afem**TL;DR:** We interpret transformer modules as vector-spin systems familiar from statistical mechanics. We implement a differentiable vector-spin model whose couplings act as learnable parameters and probe it with data to find a transformer-like attention response. Check out the [final section of the post for a visual summary and architecture comparison with vanilla transformers and deep equilibrium transformers](https://mcbal.github.io/post/transformers-from-spin-models-approximate-free-energy-minimization/#323-comparison-with-vanilla-transformers). Comments welcome!",POS
12,q7ynw9,[D] Experiment Tracking Today: What do you use? Pros and cons.,https://www.reddit.com/r/MachineLearning/comments/q7ynw9/d_experiment_tracking_today_what_do_you_use_pros/,roma-glushko,MachineLearning,1970-01-01 00:00:01.634214466,Discussion,5,3,What do you use for experiment tracking in your ML/DL projects? What is good about your approach? And what would you like to change?I personally tried W&B and [Neptune.ai](https://Neptune.ai). I think they don't focus enough on experiment tracking and compensate that with collaboration features and plotting.I'm thinking about building a little open source project to  reimagine how tracking process may look like. I hope that your experience will help to find more things that the current tools are lucking. Thanks!,POS
13,q7upuj,[R] Federated Learning - A decentralised form of Machine Learning,https://www.reddit.com/r/MachineLearning/comments/q7upuj/r_federated_learning_a_decentralised_form_of/,ifcarscouldspeak,MachineLearning,1970-01-01 00:00:01.634196333,Research,12,10,"Introduced  a few years ago by Google, Federated learning is an approach that  downloads the current model and computes an updated model on the device  itself (a little like edge computing) using local data. These locally  trained  models are then sent from the devices back to the central  server where  they are aggregated. Essentially, weights are averaged and  then a single consolidated and improved global model is sent back to  the devices.This allows multiple  organizations to collaborate on the development of models, exposing the  model to a significantly wider range of data than what any single  organization possesses in-house, while preserving data security - as  only updates are shared with devices - not the actual data.Original Article - [https://blog.mindkosh.com/what-is-federated-learning/](https://blog.mindkosh.com/what-is-federated-learning/)",POS
14,q7ey06,[D] Tired of writing mundane data wrangling code.,https://www.reddit.com/r/MachineLearning/comments/q7ey06/d_tired_of_writing_mundane_data_wrangling_code/,ydennisy,MachineLearning,1970-01-01 00:00:01.634142695,Discussion,172,77,"I find the field of ML and DS to be fascinating, there is so much to learn, read & experiment.However recently in my day to day I am under pressure to try and deliver product, I find myself writing lots of hacky little notebooks trying various pipelines, libraries, algos etc95% of the code is very bad and repetitive, concat of numpy arrays, split etc etcI feel that I am doing something wrong, we cannot all be spending this much time wrangling DFs.Reaching out to fellow practitioners who maybe had been in this stage and managed to somehow break free of the shackles of data wrangling and get their minds into advanced techniques and papers.",NEG
15,q82epz,"[D] Does groups like DeepMind, FAIR, OpenAI etc. offers master's theses?",https://www.reddit.com/r/MachineLearning/comments/q82epz/d_does_groups_like_deepmind_fair_openai_etc/,Dear-Vehicle-3215,MachineLearning,1970-01-01 00:00:01.634226443,Discussion,3,2,"I am a Master's Degree student in Computer Science Engineering and during this year most likely I will start to work on my thesis so I start looking around me. I tried to reach someone in DeepMind to ask about it, but no one reply to my request. I see from their website internship opportunities, but anything related to this.Do you have any useful information about it?Thank you so much!",POS
16,q86o44,[D] We Need to Talk About Data: The Importance of Data Readiness in Natural Language Processing,https://www.reddit.com/r/MachineLearning/comments/q86o44/d_we_need_to_talk_about_data_the_importance_of/,frippeo,MachineLearning,1970-01-01 00:00:01.634239048,Discussion,1,0,"Hey there,We've collected our experiences on teasing out the data readiness of organizations in relation to ML/NLP projects. We describe a method comprised of 15 questions that help stakeholders gauge their data readiness, along with a way to visualize the outcome of applying the method.arXiv: [https://arxiv.org/abs/2110.05464](https://arxiv.org/abs/2110.05464)Abstract: In this paper, we identify the state of data as being an important reason for failure in applied Natural Language Processing (NLP) projects. We argue that there is a gap between academic research in NLP and its application to problems outside academia, and that this gap is rooted in poor mutual understanding between academic researchers and their non-academic peers who seek to apply research results to their operations. To foster transfer of research results from academia to non-academic settings, and the corresponding influx of requirements back to academia, we propose a method for improving the communication between researchers and external stakeholders regarding the accessibility, validity, and utility of data based on Data Readiness Levels. While still in its infancy, the method has been iterated on and applied in multiple innovation and research projects carried out with stakeholders in both the private and public sectors. Finally, we invite researchers and practitioners to share their experiences, and thus contributing to a body of work aimed at raising awareness of the importance of data readiness for NLP.And the code for the visualizations is here:GitHub: [https://github.com/fredriko/draviz](https://github.com/fredriko/draviz)I'll be happy to hear any feedback! :)",POS
17,q81lle,[D] Absolute depth information using monocular depth estimation?,https://www.reddit.com/r/MachineLearning/comments/q81lle/d_absolute_depth_information_using_monocular/,floriv1999,MachineLearning,1970-01-01 00:00:01.634223985,Discussion,2,1,"In recent years machine learning based methods for monocular depth estimation such as ""Vision Transformers for Dense Prediction"" became more and more popular.While the generated heat maps look promising I was wondering whether or not this data is usable in an absolute frame of reference, meaning to determine that an object is about n meters away. For common scenes like a  standard kitchen or street scenes these scales might be learnable, but others might be harder.So my questions are:  How is the output of these techniques interpreted at the moment? Is it more like a relative guideline to compare what object is closer or can it be used to make rough estimates in e.g. meters? And how do these ""zero-shot"" techniques behave in environments where a sense of scale might be harder to get e.g. in the scene includes miniatures of some kind? Are there papers on this topic and what is your experience with it? What's your experience with fine-tuning the models on your specific scenery?Thank you very much and I would be happy to hear your opinions on this topic.",POS
18,q7r6e3,[R] The Role of Permutation Invariance in Linear Mode Connectivity of Neural Networks,https://arxiv.org/abs/2110.06296,hardmaru,MachineLearning,1970-01-01 00:00:01.634181220,Research,16,5,,NEU
19,q84pud,[P] Ideas for physics based modelling of solids using machine learning,https://www.reddit.com/r/MachineLearning/comments/q84pud/p_ideas_for_physics_based_modelling_of_solids/,Poohger,MachineLearning,1970-01-01 00:00:01.634233229,Project,1,0,"Hello r/MachineLearning!To give you a bit of context, I'm in my final year studying aerospace engineering and require a few ideas from you experts here.Unfortunately we aren't allowed to pick our own dissertation topics at this university (makes me regret going to one) and I've been given the topic exactly as mentioned in the title above: ""physics based modelling of solids using machine learning"". Here's the real kicker though, we've learnt nothing about this topic and my supervisor has rejected 15 ish ideas that i presented to him today.I come to you as my last resort.Can you think of a detailed topic that combines these fields?Some of mine that were flat out rejected were:1) structural integrity of satellites over time on exposure to space weather (rejected for being astrophysics and not aerospace which i think is bull)2) orbital impact analysis of space debris along with it's fracture mechanics (rejected for not actually requiring machine learning)3) resonance effects in engine bells of rockets during launch (also rejected for not requiring machine learning)Please advise.If you can't think of topics, please give me some sources to learn from.Thank you",NEG
20,q81eax,[R] Google Researchers Explore the Limits of Large-Scale Model Pretraining,https://www.reddit.com/r/MachineLearning/comments/q81eax/r_google_researchers_explore_the_limits_of/,Yuqing7,MachineLearning,1970-01-01 00:00:01.634223403,Research,1,0,"A Google Research team conducts a systematic exploration comprising more than 4800 experiments on Vision Transformers, MLP-Mixers and ResNets with parameters ranging from 10 million to 10 billion, evaluated on more than 20 downstream image recognition tasks, aiming to capture the nonlinear relationships between performance on upstream and downstream tasks. Here is a quick read: [Google Researchers Explore the Limits of Large-Scale Model Pretraining.](https://syncedreview.com/2021/10/14/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-123/)The paper *Exploring the Limits of Large Scale Pre-training* is on [arXiv](https://arxiv.org/abs/2110.02095).",POS
21,q81bkg,[D] Compensating for variances in different data distributions when training.,https://www.reddit.com/r/MachineLearning/comments/q81bkg/d_compensating_for_variances_in_different_data/,G-Force369,MachineLearning,1970-01-01 00:00:01.634223154,Discussion,1,2,"Hi there,I'm looking to have a discussion on data transformation, or perhaps normalization techniques, that one can do to help an ML model generalize better across different data. Let's say you have a dataset to train on, but the model you produce has to be capable of operating on a slightly different dataset, than the one used for training. Specifically their distributions and features all differ slightly. I was wondering what advanced normalization techniques are popular to add to training pipelines that could transform the distributions to a sort of common ground feature space? Please understand, I know about traditional normalization and standardization. Let's assume in this case, those ""best practices"" don't work in this case, and also we don't have training data available from the other dataset. In this situation all that is available from the other dataset is a tiny subset.Keen to hear some opinions on this ðŸ˜‰.",POS
22,q7zw6d,[D] Putting ML models in production,https://www.reddit.com/r/MachineLearning/comments/q7zw6d/d_putting_ml_models_in_production/,Proletarian_Tear,MachineLearning,1970-01-01 00:00:01.634218667,Discussion,1,5,What to consider when putting ML models in production using cloud services like Google Cloud or AWS? Are there dedicated production-ready cloud services for ML models?,POS
23,q7xz2j,[P] Mosec: deploy your machine learning model in an easy and efficient way,https://www.reddit.com/r/MachineLearning/comments/q7xz2j/p_mosec_deploy_your_machine_learning_model_in_an/,craftyoung,MachineLearning,1970-01-01 00:00:01.634211807,Project,1,4," [Mosec](https://github.com/mosecorg/mosec) is a high-performance and flexible model serving framework for building ML model-enabled backend and microservices. It bridges the gap between any machine learning models you just trained and the efficient online service API. * **Highly performant**: web layer and task coordination built with Rust ðŸ¦€, which offers blazing speed in addition to efficient CPU utilization powered by async I/O* **Ease of use**: user interface purely in Python ðŸ, by which users can serve their models in an ML framework-agnostic manner using the same code as they do for offline testing* **Dynamic batching**: aggregate requests from different users for batched inference and distribute results back* **Pipelined stages**: spawn multiple processes for pipelined stages to handle CPU/GPU/IO mixed workloads* **Cloud friendly**: designed to run in the cloud, with  the model warmup, graceful shutdown, and Prometheus monitoring metrics,  easily managed by Kubernetes or any container orchestration systems* **Do one thing well**: focus on the online serving part, users can pay attention to the model performance and business logic",POS
24,q7leg1,[N] Cambridge Quantum (CQ) Open-Sources â€˜lambeqâ€™: A Python Library For Experimental Quantum Natural Language Processing (QNLP),https://www.reddit.com/r/MachineLearning/comments/q7leg1/n_cambridge_quantum_cq_opensources_lambeq_a/,techsucker,MachineLearning,1970-01-01 00:00:01.634161424,News,10,1,"[Cambridge Quantum (â€œCQâ€)](https://cambridgequantum.com/) announced the release of the worldâ€™s first toolkit and an [open-source library ](https://github.com/CQCL/lambeq)for Quantum Natural Language Processing (QNLP), called [â€˜lambeqâ€™](https://arxiv.org/abs/2110.04236).Speaking in simple words, â€˜lambeqâ€™ is the toolkit for QNLP (Quantum Natural Language Processing) to convert sentences into a quantum circuit. It can be used to accelerate development in practical, real-world applications such as automated dialogue systems and text mining, among other things.â€˜lambeqâ€™ has been released on a fully [open-sourced basis](https://github.com/CQCL/lambeq) for the benefit of all quantum computing researchers and developers. Lambeq seamlessly integrates with CQâ€™s (Cambridge Quantum) TKET, the worldâ€™s leading and fastest-growing quantum software development platform that is also fully open-sourced. The open-sourcing of this technology provides QNLP developers with an even broader range for their work.# [Quick 3 Min Read](https://www.marktechpost.com/2021/10/13/cambridge-quantum-cq-open-sources-lambeq-a-python-library-for-experimental-quantum-natural-language-processing-qnlp/) | [Paper](https://arxiv.org/abs/2110.04236) | [Github](https://github.com/CQCL/lambeq) | [Documentation](https://cqcl.github.io/lambeq/) |[CQ Blog](https://medium.com/cambridge-quantum-computing/quantum-natural-language-processing-ii-6b6a44b319b2)&#x200B;https://preview.redd.it/j93x5n8efat71.png?width=1212&format=png&auto=webp&s=38c70032d1d8fbae68431570df10015dbffb7c87",POS
25,q76js4,"Schmidhuber pays tribute to Kunihiko Fukushima for winning the 2021 The Bower Award for Achievement in Science, for his pioneering research that applied principles of neuroscience to AI through his invention of the first deep convolutional neural network.",https://www.reddit.com/r/MachineLearning/comments/q76js4/schmidhuber_pays_tribute_to_kunihiko_fukushima/,hardmaru,MachineLearning,1970-01-01 00:00:01.634112789,News,127,19,"Schmidhuber congratulated Fukushima for winning the 2021 Bower Award and Prize for Achievement in Science (from The Franklin Institute) in a [tweet](https://twitter.com/SchmidhuberAI/status/1448168005469413376): *â€œKunihiko Fukushima was awarded the 2021 Bower Award for his enormous contributions to deep learning, particularly his highly influential convolutional neural network architecture. My laudation of Kunihiko at the 2021 award ceremony is on [YouTube](https://youtu.be/ysOw6lNWx2o).â€*(Schmidhuber set up a YouTube channel recently, and posted the video there: https://youtu.be/ysOw6lNWx2o)Background on [Kunihiko Fukushima](https://en.wikipedia.org/wiki/Kunihiko_Fukushima) (Japanese: [ç¦å³¶ é‚¦å½¦](https://ja.wikipedia.org/wiki/%E7%A6%8F%E5%B3%B6%E9%82%A6%E5%BD%A6)) and his award this year: https://www.fi.edu/laureates/kunihiko-fukushimaAlso an article from NHK about his time at NHK's Science and Technology Research Labs (yes, the lab was part of a public broadcaster in Japan) about his award: https://www.nhk.or.jp/strl/english/publica/giken_dayori/194/1.html",POS
26,q7jf4g,[D] Paper explained - StyleNeRF: ICLR 2022 submission (5-minute summary),https://www.reddit.com/r/MachineLearning/comments/q7jf4g/d_paper_explained_stylenerf_iclr_2022_submission/,KirillTheMunchKing,MachineLearning,1970-01-01 00:00:01.634155386,Discussion,12,1,"Itâ€™s a NeRF, itâ€™s a GAN itâ€™s ~~Superman~~  StyleNeRF. But no for real, it happened, two of the biggest (probably)  breakthroughs of the last couple of years are joining forces. StyleGAN  is great at generating structured  2D images but it has zero knowledge  about the 3D world. NeRF, on the other hand, is great at understanding complex 3D scenes but struggles to generate view-consistent scenes when trained on unposed images.  StyleNeRF fuses the two into a  style-conditioned radiance field  generator with explicit camera pose control. Seems like a perfect match!  Letâ€™s find out if it really lives up to the hype.Fresh out of the oven! Full summary: [https://www.casualganpapers.com/unsupervised-discovery-nonlinear-latent-editing-directions-generator/StyleNeRF-explained.html](https://www.casualganpapers.com/unsupervised-discovery-nonlinear-latent-editing-directions-generator/StyleNeRF-explained.html)[Can't wait to see the gifs](https://preview.redd.it/ebrt7gu3x9t71.png?width=862&format=png&auto=webp&s=51156e4d0831acd8fe6d402541b4a7ca8774f05d)arxiv: [https://arxiv.org/pdf/2109.13357v1.pdf](https://arxiv.org/pdf/2109.13357v1.pdf)  code: Coming soon",POS
27,q7kpi0,[D] Production Model Monitoring - What are you using?,https://www.reddit.com/r/MachineLearning/comments/q7kpi0/d_production_model_monitoring_what_are_you_using/,bfeeny,MachineLearning,1970-01-01 00:00:01.634159239,Discussion,4,0,"We are mostly an AWS Shop and can use SageMaker Model Monitor, but we also have some production models outside of AWS.What types of solutions are you all using to just look at things like precision/recall and other metrics, or even more advanced stuff like detection of bias/drift?We wrote an in house dashboard a while back that we just naively send metrics too, and that works ok, but I am wanting to see if anyone is using any slick tools that would be worth looking at.",POS
28,q7tca8,[R] Dynamic Execution/Dynamic Inference Repos?,https://www.reddit.com/r/MachineLearning/comments/q7tca8/r_dynamic_executiondynamic_inference_repos/,bang-em-boi,MachineLearning,1970-01-01 00:00:01.634189827,Research,1,0,I am currently looking into real-time machine learning research and came across the idea of dynamic execution/dynamic inference. Are there any open library/code repo's that I can play with to see how it works? Thank you!,POS
29,q7hsps,[R] Open-Set Recognition: A Good Closed-Set Classifier is All You Need,https://arxiv.org/abs/2110.06207,downtownslim,MachineLearning,1970-01-01 00:00:01.634150804,Research,7,1,,NEU
30,q739y5,[R] A Few More Examples May Be Worth Billions of Parameters,https://arxiv.org/abs/2110.04374,hardmaru,MachineLearning,1970-01-01 00:00:01.634098398,Research,71,23,,NEU
31,q7gisc,[R] ICCV2021 oral paper -- Unsupervised Depth Completion with Calibrated Backprojection Layers improves generalization across sensor platforms,https://www.reddit.com/r/MachineLearning/comments/q7gisc/r_iccv2021_oral_paper_unsupervised_depth/,alexk_wong,MachineLearning,1970-01-01 00:00:01.634147184,Research,6,1,"Our work ""Unsupervised Depth Completion with Calibrated Backprojection Layers"" has been accepted as an oral paper at ICCV 2021! We will be giving our talk during Session 10 (10/13 2-3 pm PST / 5-6 pm EST and 10/15 7-8 am PST / 10-11 am EST, https://www.eventscribe.net/2021/ICCV/fsPopup.asp?efp=WlJFS0tHTEMxNTgzMA%20&PosterID=428697%20&rnd=0.4100732&mode=posterinfo). This is joint work with Stefano Soatto at the UCLA Vision Lab.In a nutshell: we propose a method for point cloud densification (from camera, IMU, range sensor) that can generalize well across different sensor platforms. The figure in this link  illustrates our improvement over existing works: https://github.com/alexklwong/calibrated-backprojection-network/blob/master/figures/overview_teaser.gifThe slightly longer version: previous methods, when trained on one sensor platform, have problem generalizing to different ones when deployed to the wild. This is because they are overfitted to the sensors used to collect the training set. Our method takes image, sparse point cloud and camera calibration as input, which allows us to use a different calibration at test time. This significantly improves generalization to novel scenes captured by sensors different than those used during training. Amongst our innovations is a ""calibrated backprojection layer"" that imposes strong inductive bias on the network (as opposed trying to learn everything from the data). This design allows our method to achieve the state of the art on both indoor and outdoor scenarios while using a smaller model size and boasting a faster inference time.For those interested, here are the links topaper: https://arxiv.org/pdf/2108.10531.pdfcode (pytorch): https://github.com/alexklwong/calibrated-backprojection-network",POS
32,q7e9d8,[D] Video coloring SOTA,https://www.reddit.com/r/MachineLearning/comments/q7e9d8/d_video_coloring_sota/,escapevelocitylabs,MachineLearning,1970-01-01 00:00:01.634140795,Discussion,2,0,"A while back I took a look at Deoldify for video coloring, but it had a few issues (colors were not stable from one frame to the next). Since then, I was wondering if there are newer techniques that do a better job at this task.&#x200B;Do you know of any?",POS
33,q7usfu,[D] The four fundamental interactions of transformers,https://www.reddit.com/r/MachineLearning/comments/q7usfu/d_the_four_fundamental_interactions_of/,wangyi_fudan,MachineLearning,1970-01-01 00:00:01.634196712,Discussion,0,3,Some strange thoughts on transformers' analogue to physics:MLP layer=Strong interaction: it works within an object.CNN layer=Weak interaction: it has limited range between objects. newly added to the Primer architecture.Attention=Electromagnetism: it requires positive/negative matches between query and key and has long distance interaction.Relu/GELU=Mass/Gravity: it twists the time and space.Weight Matrix=Time and space,NEG
34,q78l51,[D] What exactly is the context representation in wav2vec 2.0?,https://www.reddit.com/r/MachineLearning/comments/q78l51/d_what_exactly_is_the_context_representation_in/,satoshibitchcoin,MachineLearning,1970-01-01 00:00:01.634122436,Discussion,2,10,"the paper doesn't really explain and all of the blog posts gloss over it. what exactly is the transformer producing in real terms? i dont understand the handwavy 'relativ vs fixed positional embeddings' jargon without knowing exactly what it means in this context? i've gone through every result on google and youtube and am a loss. also the contrastive task has to do a cosine similarity with this output and the quantized representation of the latent representation - so does that mean the transformer is producing something similar to the quantized representation? Or am i off completely?edit. this is the closest explanation ive found but it still doesn't fill in the details for me: > The model first processes the raw waveform of the speech audio with a multilayer convolutional neural network to get latent audio representations of 25ms each. These representations are then fed into a quantizer as well as a transformer. The quantizer chooses a speech unit for the latent audio representation from an inventory of learned units. About half the audio representations are masked before being fed into the transformer. **The transformer adds information from the entire audio sequence. Finally, the output of the transformer is used to solve a contrastive task.** This task requires the model to identify the correct quantized speech units for the masked positions.",POS
35,q79ocw,[D] How Do you Evaluate your Rendered Synthetic Data?,https://www.reddit.com/r/MachineLearning/comments/q79ocw/d_how_do_you_evaluate_your_rendered_synthetic_data/,DisWastingMyTime,MachineLearning,1970-01-01 00:00:01.634126671,Discussion,2,16,"This question is probably specific to people in the vision field who use synthetic data to train their models.In short, I'm running into an issue where our synthesizied data doesn't actually  represent the real data although it very much looks close enough, and I wondered if people already ran into this problem and have some process/benchmark to evaulate their synth.",NEG
36,q7ga5b,[discussion] OS and Infrastructure for MultiModel Production,https://www.reddit.com/r/MachineLearning/comments/q7ga5b/discussion_os_and_infrastructure_for_multimodel/,Simusid,MachineLearning,1970-01-01 00:00:01.634146509,Discussion,1,0,"Single purpose ML would be something like a security camera.   If it detects a face or a car (or some condition) it sends some kind of alert.   That's very straightforward.  A self driving car (for example) probably has dozens of specialized models.   One to detect traffic signs, one to detect roadways, one to detect people, one for other vehicles and emergency vehicles, and I'm sure others.   What sort of architecture would you use today to orchestrate these models?   I've done some googling and it tells me that other people are using combinations of Celery, Rabbitmq, Redis, Elastic beanstalk, Flask, Containers, and kubernetes.    This seems kind of excessive.  This is all in userspace too, at the application level.   Would there be any performance gains by integrating models deeper in the OS?   Could a model be effectively implemented as a device driver?",POS
37,q73mb3,[R] Chaos as an interpretable benchmark for forecasting and data-driven modelling,https://arxiv.org/abs/2110.05266,hardmaru,MachineLearning,1970-01-01 00:00:01.634099713,Research,11,2,,NEU
38,q6rbzm,"[R] Facebook AI Introduces â€˜3DETRâ€™ That Increases 3D Comprehension and â€˜DepthContrastâ€™, A self-supervised Learning Mechanism That Doesnâ€™t Rely On Labels",https://www.reddit.com/r/MachineLearning/comments/q6rbzm/r_facebook_ai_introduces_3detr_that_increases_3d/,techsucker,MachineLearning,1970-01-01 00:00:01.634059902,Research,75,9,"In todayâ€™s world, itâ€™s critical to develop systems that can understand 3D data about the world. For example, autonomous automobiles require 3D understanding to move and avoid colliding with objects. In contrast, AR/VR applications can assist people with everyday activities like imagining whether a couch will fit in a living room.Computer vision is an AI domain that employs Machine Learning (ML) and Deep Learning (DL) that enable computers to observe, recognise, and interpret objects in images and videos in the same way that humans do. Recent advances in DL methodologies and technological breakthroughs have greatly enhanced the capabilities of computer vision systems.Â High-performance computer vision (CV) models use large labelled data sets for pretraining. However, it hasnâ€™t been widely used for 3D recognition tasks like detecting and locating a piece of furniture in a 3D scan of a living room. This is because of the scarcity of annotated data and the time-consuming nature of labelling 3D data sets. Furthermore, 3D understanding models frequently rely on a handcrafted architecture design strongly associated with the specific 3D data set used for training.Facebook AI introduces [3DETR ](https://ai.facebook.com/blog/simplifying-3d-understanding-using-self-supervised-learning-and-transformers/)and[ DepthContrast](https://ai.facebook.com/blog/simplifying-3d-understanding-using-self-supervised-learning-and-transformers/), two complimentary new models that increase 3D comprehension. These models make it easier to get started by providing a general 3D architecture that makes 3D understanding easier and a self-supervised learning mechanism that doesnâ€™t rely on labels.# [Quick Read](https://www.marktechpost.com/2021/10/12/facebook-ai-introduces-3detr-that-increases-3d-comprehension-and-depthcontrast-a-self-supervised-learning-mechanism-that-doesnt-rely-on-labels/)| [Paper 3DETR](https://arxiv.org/abs/2109.08141?) | [Code 3DETR](https://github.com/facebookresearch/3detr) | [Paper DepthContrast](https://arxiv.org/abs/2101.02691?) | [Code DepthContrast](https://github.com/facebookresearch/DepthContrast?) | [FB Blog](https://ai.facebook.com/blog/simplifying-3d-understanding-using-self-supervised-learning-and-transformers/)&#x200B;https://i.redd.it/vg75hfz912t71.gif",POS
39,q71alj,"[P] Our lab released a unified prompt-learning toolkit #OpenPrompt, which aims to make it easy to deploy prompt-learning framework to solve various NLP problems based on pre-trained models. Welcome to use and contribute! Github: https://github.com/thunlp/OpenPrompt",https://www.reddit.com/r/MachineLearning/comments/q71alj/p_our_lab_released_a_unified_promptlearning/,TsinghuaNLP,MachineLearning,1970-01-01 00:00:01.634091189,Project,15,0,&#x200B;https://i.redd.it/edaqw8hjm4t71.gif,NEU
40,q7c3bc,[N] Webinar announcement: First Open-Source Data Discovery and Observability Platform for ML Engineers,https://www.reddit.com/r/MachineLearning/comments/q7c3bc/n_webinar_announcement_first_opensource_data/,Haarolean,MachineLearning,1970-01-01 00:00:01.634134515,News,0,0,"Hi,I'd like to share some news: There's an upcoming virtual event, during which problems related to data discovery and observability will be addressed and discussed.There'll be presented a free open source solution for managing metadata.The webinar will happen on 20th of October (11 PM PT, 2 PM ET, 8 PM CEST).Details and registration [here](https://provectus.com/data-discovery-and-observability-platforms/)",POS
41,q7bn8k,[D] Improving picture focus/noise in very constrained dataset,https://www.reddit.com/r/MachineLearning/comments/q7bn8k/d_improving_picture_focusnoise_in_very/,dofphoto,MachineLearning,1970-01-01 00:00:01.634133238,Discussion,1,4,"Hi all,I'm wondering where we are with ease-of-implementation for an idea like this with very limited scope and very specific training data.Data: Say I have a ton of pictures of a \*specific\* pet, or a person, etc Just one specific subject.Problem: I would like to \*attempt\* to make the pictures that are grainy/out-of-focus/artifact-y to look like the good quality ones. I am happy to select out the good quality ones and make a ""training set"". I can probably make a dataset on the order of 1000 easily. Maybe more if needed.Strategy: I am thinking of a couple of strategies:(1) Take the good quality ones (sharp, low noise, etc), degrade them in some arbitrary way, and train a simple architecture (unet, densenet, etc) to reconstruct to good quality. Maybe using a perceptual loss like LPIPS + MSE?(2) Use an adversary on the loss. But I'm not sure of the state of how challenging it is to make these work nowadays.Any opinions?&#x200B;edits: \- perhaps both \[D\] and \[P\] apply to the tag...\- This is for a personal project -- I actually want to have this tool to work on my images. I'm not trying to publish a paper or something.",POS
42,q6jvd6,[D] Arxiv submissions of double-blind review papers,https://www.reddit.com/r/MachineLearning/comments/q6jvd6/d_arxiv_submissions_of_doubleblind_review_papers/,trow-away-420,MachineLearning,1970-01-01 00:00:01.634037439,Discussion,115,38,"There seem to be many papers currently under double-blind review for ICLR 2022 on Arxiv with full author information. Most have been submitted to Arxiv after submitting ICLR 2022. Here are some examples,* [Exploring the Limits of Large Scale Pre-training](https://papers.labml.ai/paper/48b5fef4264011ec9e9dcba33be64600) ([openreview](https://openreview.net/forum?id=V3C8p78sDa))* [Causal ImageNet](https://papers.labml.ai/paper/434da7a62b1b11ec9e9dcba33be64600) ([openreview](https://openreview.net/forum?id=XVPqLyNxSyh))* [Learning Sampling Policy for Faster Derivative Free Optimization](https://papers.labml.ai/paper/nUoI0DKg_Ti)* [Bootstrapped Meta-Learning](https://papers.labml.ai/paper/b-ny3x071E5) ([authors tweet](https://twitter.com/flennerhag/status/1437453527916425220), [Discussion on /r/MachineLearning](https://www.reddit.com/r/MachineLearning/comments/prw2ly/r_deepminds_bootstrapped_metalearning_enables/))When I first saw an ICLR 2022 submission on Arxiv it surprised me, and thought it was an exception. Then I found the above four in a few minutes. The reviewers can find the authors with a google search or an Arxiv lookup.Among these there are papers like [Patches Are All You Need?](https://papers.labml.ai/paper/dd638a442a9e11ec9e9dcba33be64600) that even anonymizes the Github repos, honoring the double-blind review process.",POS
43,q70akn,[R] Vectorization of Raster Manga by Deep Reinforcement Learning,https://arxiv.org/abs/2110.04830,hardmaru,MachineLearning,1970-01-01 00:00:01.634087734,Research,8,3,,NEU
44,q6scne,[P] Creating a new ML algorithm or improving an existing one,https://www.reddit.com/r/MachineLearning/comments/q6scne/p_creating_a_new_ml_algorithm_or_improving_an/,MlecznyHotS,MachineLearning,1970-01-01 00:00:01.634062911,Project,20,11,"This semester I'm attending a ML class. I have 2.5 years of ML epxerience profesionally. To graduate I have to create a project, the most difficult kind of project is to design a new algorithm or somehow improve an existing one. It feels like creating a new ML algorithm is very difficult and something a group of professors or PhD might accomplish.If I were to try and create this sort of project with a group of similarly experienced friends where would you reccomend that we start?",POS
45,q785fp,[D] Conditional Generative Models,https://www.reddit.com/r/MachineLearning/comments/q785fp/d_conditional_generative_models/,alkaway,MachineLearning,1970-01-01 00:00:01.634120456,Discussion,0,1,"I understand that for a GAN, you input a random z vector for variation. However, if you have a conditional generative model, and you have a feature vector you want to condition on, then you don't need this random z vector, do you?I was reading [Where2Act: From Pixels to Actions for Articulated 3D Objects](https://arxiv.org/pdf/2101.02692.pdf) and they have a conditional generative model which takes in as input both a feature vector and a random vector (see Action Proposal Module at the end of page 3)...Additionally, they use the Min-of-N strategy to train the conditional generative model. Is this common to train conditional generative models?",POS
46,q6nkdz,[D] Virtual MLOps Roundtable,https://www.reddit.com/r/MachineLearning/comments/q6nkdz/d_virtual_mlops_roundtable/,simbakdev,MachineLearning,1970-01-01 00:00:01.634049307,Discussion,36,10,"I'm putting together an MLOps roundtable focused on peer learning. ML and MLOps practitioners can share how they utilize MLOps to automate and scale their ML processes and learn about how other teams structure theirs.It works like this: we break people up into small groups of 5-7 people based on their team size and what they are working on. The overarching topic for this month is ""Applying MLOps at Scale"".There is absolutely no selling or pitching. The focus is pure peer learning.You can sign up here if you're interested: [https://www.eventbrite.com/e/mlops-round-table-tickets-186267700707](https://www.eventbrite.com/e/mlops-round-table-tickets-186267700707)Let me know if you have any ideas, thoughts, or feedback.",POS
47,q6obcr,[R] Are Patches All You Need? New Study Proposes Patches Are Behind Vision Transformersâ€™ Strong Performance,https://www.reddit.com/r/MachineLearning/comments/q6obcr/r_are_patches_all_you_need_new_study_proposes/,Yuqing7,MachineLearning,1970-01-01 00:00:01.634051404,Research,32,5,"A research team proposes ConvMixer, an extremely simple model designed to support the argument that the impressive performance of vision transformers (ViTs) is mainly attributable to their use of patches as the input representation. The study shows that ConvMixer can outperform ViTs, MLP-Mixers and classical vision models. Here is a quick read: [Are Patches All You Need? New Study Proposes Patches Are Behind Vision Transformersâ€™ Strong Performance.](https://syncedreview.com/2021/10/12/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-121/)The ConvMixer code is available on the projectâ€™s [Github](https://github.com/tmp-iclr/convmixer). The paper *Patches Are All You Need?* is on [OpenReview](https://openreview.net/forum?id=TVHS5Y4dNvM).",POS
48,q70cyn,[R] Relative Molecule Self-Attention Transformer,https://arxiv.org/abs/2110.05841,hardmaru,MachineLearning,1970-01-01 00:00:01.634087969,Research,4,1,,NEU
49,q70cb0,"ABO: Dataset and Benchmarks for Real-World 3D Object Understanding. The ""Amazon Berkeley Objects"" dataset is a large-scale dataset of product images and 3D models corresponding to real household objects.",https://arxiv.org/abs/2110.06199,hardmaru,MachineLearning,1970-01-01 00:00:01.634087905,Research,3,1,,NEU
50,q77km8,[Discussion] What are the problems you face as ML Practitioner and what tools do you use to solve for those problems ?,https://www.reddit.com/r/MachineLearning/comments/q77km8/discussion_what_are_the_problems_you_face_as_ml/,MLEngineerOnRoll,MachineLearning,1970-01-01 00:00:01.634117875,Discussion,0,0,Some problems that I see are:1. Feature Discovery is hard and cumbersome.2. Difficult to identify models that exists in production within the organization3. Identifying the right technique for the ML problem being solved4. Tracking the notes and experiments during the ML experiments5. Tracking the progression of data science projects over period of timePlease let me know if these problems resonate with you or if there are any other pressing problems that you face. Thanks a lot.,NEG
51,q6ark8,[R] StyleGAN3: Alias-Free Generative Adversarial Networks,https://www.reddit.com/r/MachineLearning/comments/q6ark8/r_stylegan3_aliasfree_generative_adversarial/,hardmaru,MachineLearning,1970-01-01 00:00:01.634001632,Research,175,41,The team at NVIDIA released the latest StyleGAN3 model (to appear at NeurIPS 2021).- Paper: https://arxiv.org/abs/2106.12423- Project website: https://nvlabs.github.io/stylegan3/- PyTorch implementation: https://github.com/NVlabs/stylegan3,NEU
52,q73wvt,[R] Go-Blend behavior and affect,https://arxiv.org/abs/2109.13388,hardmaru,MachineLearning,1970-01-01 00:00:01.634100937,Research,1,1,,NEU
53,q6vzz8,[D] Can an algorithm that query and update a knowledge graph be enough for a high level AI?,https://www.reddit.com/r/MachineLearning/comments/q6vzz8/d_can_an_algorithm_that_query_and_update_a/,agoramancy,MachineLearning,1970-01-01 00:00:01.634073668,Discussion,0,12,"What if all that we need is the correct algorithm applied to a knowledge graph to have the correctly answer every query?\[Edit\] Considering the assumption that a lot of the knowledge used to produce the answer comes from the question, and related previous knowledge.",NEU
54,q60wlh,[N] DeepSpeed and Megatron train the largest dense language model at 530B parameters,https://www.reddit.com/r/MachineLearning/comments/q60wlh/n_deepspeed_and_megatron_train_the_largest_dense/,p1nh3ad,MachineLearning,1970-01-01 00:00:01.633973105,News,152,59,[https://www.microsoft.com/en-us/research/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/](https://www.microsoft.com/en-us/research/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/),NEU
55,q6uhs9,[R] Has GRAD-CAM been superseded?,https://www.reddit.com/r/MachineLearning/comments/q6uhs9/r_has_gradcam_been_superseded/,MDC1721,MachineLearning,1970-01-01 00:00:01.634069178,Research,1,3,"I'm training a graph neural network, and my objective is to find out which nodes are being used the most for classification. GRAD-CAM requires changing the model architecture, so it got me thinking if there's a better alternative to it now.I researched a bit but didn't find anything. Does anyone here know of any new research which deals with this?Thanks a ton!",POS
56,q6ilpd,[D] Adaptive loss weight in VQGAN paper.,https://www.reddit.com/r/MachineLearning/comments/q6ilpd/d_adaptive_loss_weight_in_vqgan_paper/,chasep255,MachineLearning,1970-01-01 00:00:01.634032184,Discussion,5,3,"If you look at page 4 of this paper ([https://arxiv.org/pdf/2012.09841.pdf](https://arxiv.org/pdf/2012.09841.pdf)) the authors compute an adaptive weight lambda to balance the perceptual and adversarial loss.  Basically they compute the gradients of the last layer of the generator with respect to the adversarial loss and the reconstruction loss, and use this ratio the balance the weight of the adversarial loss.  They don't offer any explanation of why they do this, and its the only paper where I have seen this.  It kind of makes sense to me in that they want half the update to come from the discriminator and half to come from the perceptual loss.  Has anyone experimented with this?  All the papers I have seen where there is an auxiliary loss function to a GAN just assign some sort of fixed weight to the discriminator.",NEG
57,q6sb9b,Reinforcement learning for recommendation systems [D][P],https://www.reddit.com/r/MachineLearning/comments/q6sb9b/reinforcement_learning_for_recommendation_systems/,Killer_queen2020,MachineLearning,1970-01-01 00:00:01.634062796,Discussion,0,2,"Hi All, I'm exploring the use of reinforcement learning for a recommendation system. It's not for e-commerce but my reward would be measured using Clicks. Any suggestions are welcome as I am pretty new to applying RL for solving real world problems.",POS
58,q6f5o0,"[R] Unsupervised Neural Machine Translation with Generative Language Models Only. They use few-shot learning to amplify GPT-3's zero-shot translations and create fine-tuning datasets for machine translation with no supervision. New SOTA in unsupervised translation on WMT14, BLEU score of 42 on En-Fr",https://arxiv.org/abs/2110.05448,hardmaru,MachineLearning,1970-01-01 00:00:01.634016988,Research,11,2,,NEU
59,q6mwqu,[D] Are there any oral papers at ICCV'21?,https://www.reddit.com/r/MachineLearning/comments/q6mwqu/d_are_there_any_oral_papers_at_iccv21/,BananaCode,MachineLearning,1970-01-01 00:00:01.634047438,Discussion,2,2,I cannot seem to find a way to filter for oral papers at ICCV'21. Were there any oral papers?,NEU
60,q6851m,"[P] glum: High performance Python generalized linear modeling, a glmnet alternative!",https://www.reddit.com/r/MachineLearning/comments/q6851m/p_glum_high_performance_python_generalized_linear/,tbenthompson,MachineLearning,1970-01-01 00:00:01.633993190,Project,30,8,"I'm super excited to announce the release of glum ([https://github.com/Quantco/glum/](https://github.com/Quantco/glum/)),  a Python-first generalized linear modeling library. We focused a lot on  correctness, performance and a powerful range of features. glum gets a  3-10x (or more) speed up over glmnet. It's also fun to use!https://preview.redd.it/pn1gtr2yiws71.png?width=1800&format=png&auto=webp&s=82f5ef7bbb16e7024db2f5d1a232da7f21574891We're also releasing tabmat ([https://github.com/Quantco/tabmat/](https://github.com/Quantco/tabmat/)),  a tabular matrix backend for glum. It supports mixes of dense, sparse  and categorical matrices. On some operations, tabmat is 50x faster than  scipy.sparse! And it's memory-efficient.https://preview.redd.it/iq6snquyiws71.png?width=2100&format=png&auto=webp&s=bfe9772127c41a029dcfeeffb7f79f7d113e3491I'm  personally excited about the ability to set penalties for individual  elastic net parameters. And to have variable matrix-valued Tikhonov  matrix penalties. This lets you do cool things like imposing smoothness  between adjacent zip-code fixed effects.https://preview.redd.it/ep2sngxziws71.png?width=1175&format=png&auto=webp&s=88b1d5f947dd5bfc112a647d26d0a3b68ecdfb4cBecause  tabmat handles categorical features very efficiently, we've dealt with  real-world problems that have millions of categories. This cool figure  shows the sparsity pattern of some categorical matrix multiplications.https://preview.redd.it/psdz27w0jws71.png?width=448&format=png&auto=webp&s=c688e10076d3e2e95388220a82f5bf25c713273dWe  also support lots of other stuff like linear inequality constraints,  arbitrary distributional and link function choices, regularization  paths, and automatic cross validation.There's a lot more in the documentation for glum, including tutorials and a discussion of the algorithm: [https://glum.readthedocs.io/en/latest/](https://glum.readthedocs.io/en/latest/)",POS
61,q6r8ss,"COVID-19 Medical Data You Can Understand, Trust, And Gain Actionable Insights From [R]",https://www.reddit.com/r/MachineLearning/comments/q6r8ss/covid19_medical_data_you_can_understand_trust_and/,CovidKnowledgeGraph,MachineLearning,1970-01-01 00:00:01.634059642,Research,0,0,Released our first public version of the COVID-19 Knowledge Graph - [http://www.covidkg.org/graph](http://www.covidkg.org/graph). Make sure you're up to date on COVID-19 medical facts![http://www.covidkg.org/graph](http://www.covidkg.org/graph),POS
62,q6qr0l,[D] Voice modification ML: state of the art and resources,https://www.reddit.com/r/MachineLearning/comments/q6qr0l/d_voice_modification_ml_state_of_the_art_and/,NightRail3748,MachineLearning,1970-01-01 00:00:01.634058220,Discussion,2,2," Hey Hey! Recently I have developed interest in ML applications in audio. I want to do a few personal projects, keeping in mind potential business use cases, but before I dive in, I'd like to understand where the field is currently at. I come from Data Science in Finance, so I am comfortable with the general ML concepts, but have rather limited understanding of the audio-specific stuff.One of the apps I am looking to develop: **modify a recorded voice (reading) to sound like a different person**. I know there are plenty of 'Change your voice to sound like a celebrity' apps, but I haven't seen any good ones, and I am also not interested in using celebrity voices. I have also seen multiple applications that change your voice to sound like fantasy characters, but again, not what I am after.**What is currently considered the state of the art in this field? Any good examples?****How much data (hours of recording) one would need to train a decent model for one voice?** By decent I mean the modified voice does not sound like a robot. I have roughly 20h recorded of person A (the target voice) and roughly 10h of person B (the voice that will be modified to sound like person A). Is this workable, or would I need much, much more data than that?**Any good resources, repos, book articles etc. on the topic, that you would suggest?** (Yes, I know Google exists, but it is also full of BS)Many thanks in advance",POS
63,q6kb8x,[R] SimJEB: A dataset of crowdsourced 3D engineering models and structural simulations from MIT,https://www.reddit.com/r/MachineLearning/comments/q6kb8x/r_simjeb_a_dataset_of_crowdsourced_3d_engineering/,rodeoClown2,MachineLearning,1970-01-01 00:00:01.634039121,Research,2,0,"&#x200B;https://preview.redd.it/wz4wkc3280t71.png?width=5489&format=png&auto=webp&s=64e10be67d605b108fbd9dd59918ec4332126dd5&#x200B;The Simulated Jet Engine Bracket Dataset (SimJEB) is a collection of 381 mechanical bracket designs with accompanying structural simulations.The bracket designs originate from GrabCAD.comâ€™s ""GE Jet Engine Bracket Challenge"": an open engineering design competition with over 700 hand-designed CAD entries from 320 designers representing 56 countries.This open dataset has several potential uses, including testing geometric deep learning, geometry processing, meshing, and surrogate modeling methods, as well as those studying ML applications for engineering design.The designs have been cleaned, oriented, scaled, meshed, and simulated. Each entry has a corresponding CAD file, tetrahedral mesh, triangular surface mesh, and structural simulation results from four load cases. The designs have also been hand-labeled as pertaining to one of six general design categories. [https://simjeb.github.io/](https://simjeb.github.io/)",POS
64,q6nj2f,[D] question about Semi-Supervised Learning,https://www.reddit.com/r/MachineLearning/comments/q6nj2f/d_question_about_semisupervised_learning/,Snoo-62877,MachineLearning,1970-01-01 00:00:01.634049201,Discussion,1,2,"Dataset: \- I have dataset of 10 classes (classified by me)\- they are 2d-represented mechanical materials&#x200B;Problem I want to solve:\- Labeling unlabeled data based on labeled data\- I already have all of them labeled but I want to go back and make use of Semi Supervised Learning so that the process of labeling data seems more reasonable than the way done manually by me.&#x200B;specifically, there are 2,000 labeled data, each classes. and 100,000 unlabeled data. since the data are easily represented by image, I thought of using ResNet to train and predict on unlabeled data, and use the result as class.&#x200B;is it a naive way? hope to hear about a reasonable way to do semi supervised learning to label unlabeled data. thanks",POS
65,q6ncmb,[Discussion] How do neural nets generalize vs overfit ?,https://www.reddit.com/r/MachineLearning/comments/q6ncmb/discussion_how_do_neural_nets_generalize_vs/,harharveryfunny,MachineLearning,1970-01-01 00:00:01.634048697,Discussion,0,18,"There's an article just published by Quanta magazine addressing the perennial issue of why over-parametized neural nets can generalize vs overfit. The explanation suggested here, if I'm understanding it correctly, is that the effective dimensionality of the parameter space is reduced since neural nets, like support vector machines, may be deriving optimal hyperplane decision boundaries, rather than being able to use all their parameters to define more gerrymandered overfitting boundaries.[https://www.quantamagazine.org/a-new-link-to-an-old-model-could-crack-the-mystery-of-deep-learning-20211011/](https://www.quantamagazine.org/a-new-link-to-an-old-model-could-crack-the-mystery-of-deep-learning-20211011/)What frustrates me every time this comes up is that I really don't understand why multi-layer neural nets would NOT be expected to generalize if fed the right type of training data. I guess either there's something wrong with my mental model of what neural nets are doing,  or perhaps my mental model is right in that it predicts the observed behavior of generalization, and similar to this new explanation it's based on what hyperplane decision boundaries are being learnt.Here's my mental model of what's going on. Please excuse if this all seems obvious, because it does seem obvious!1. Each layer (or node in more general case) of a neural net can be considered as mapping an input space to an output space (who's dimensions are the number of inputs and outputs respectively).2. The way each layer performs this input space to output space mapping is by learning hyperplane decision boundaries that carve up the input space into chunks corresponding to the output space.Not that it's important to this conceptual model, but presumably it's the activation functions that are really defining the position of these decision boundaries, with the layer's weights having learnt to warp/scale the input space coordinates such that the following fixed activation functions can then do their job.3) The neural net as a whole, just like an individual layer, can be regarded as mapping an input space (samples) to an output space (labels), and does so via a (multi-layer) hierarchy of decision boundaries which carve up the evolving (layer output) feature spaces such that at the output layer each label is defined by a set of decision boundaries which isolate samples having a given label.4) In the overfitting case (including the extreme case where the training set had random labels uncorrelated to the samples) each label is defined by decision boundaries isolating a chunk of input/feature space containing a single sample, or small set of samples (not all of those with the same label).5) In the generalization case, the training set needs to have consisted of consistently labelled samples such that the labels correspond to features learnable by the neural net (e.g. hierarchical visual patterns). In this case, if it has learned to generalize, what will have happened is that the decision boundaries will have isolated chunks of input/feature space each containing many samples with the same label. Generalization occurs since unseen samples with the same features will therefore fall into this same chunk of output space - i.e. receive the same label.6) What makes the difference between overfitting and generalization is mostly the training data - whether the training samples have features in common (that can be learnt by the net in question) that correspond to the given labels, and whether there is sufficient variety of samples (after augmentation) to provide coverage of the features space such that there is no ambiguity in which sets of features correspond to each label.The key to the ""mystery of generalization"" seems to be that if the above training data conditions are met, then the error feedback encountered during training will be consistent across samples with the same label and will be moving the decision boundaries in a common direction to isolate the region(s) of feature space corresponding to the common label. Therefore, rather than the ""highly gerrymandered"" regions of output space corresponding to the overfitting case, this consistent error feedback will nudge the decision boundaries in a common direction for all samples having the same label-to-feature-set correspondence, leading to a single (or maybe few) large region(s) of feature space having been isolated corresponding to the common label, therefore providing good generalization.So, what is wrong with the above mental model of what is going on in a neural net to achieve generalization, and if nothing (other than lack of detail) then can we be more specific about what step of this training process is surprising rather than inevitable?I suppose this explanation could also be cast in terms of optimal decision boundaries, the same as an SVM, since the boundaries are only going to move as far as they need to minimize the errors. Once all samples are correctly predicted the errors will go to zero and the boundaries will stop moving.",NEG
66,q5y0rp,[P] @SummarizedML: A Twitter bot that summarizes the latest ML papers,https://www.reddit.com/r/MachineLearning/comments/q5y0rp/p_summarizedml_a_twitter_bot_that_summarizes_the/,JaviFuentes94,MachineLearning,1970-01-01 00:00:01.633965217,Project,61,8,"Hey r/MachineLearning!&#x200B;I wanted to share my latest project, [SummarizedML](https://twitter.com/summarizedml).[SummarizedML](https://twitter.com/summarizedml) is a Twitter bot that uses machine learning to summarize the latest ML papers.I have always been frustrated with the existing Twitter bots that post ArXiV papers, and after reading this [paper](https://arxiv.org/abs/2004.15011) I decided to build something better using those models.&#x200B;We all know it is difficult to keep up with current research in the field, and this is my small contribution to help people with it.&#x200B;You can read more about it in the [launch Twitter thread](https://twitter.com/JavierFnts/status/1447578497191317511)And follow the SummarizedML bot [here](https://twitter.com/summarizedml)Looking forward to hearing your thoughts on it! And I hope you find it useful!",POS
67,q6drlo,"[D] A place to discuss papers and ask questions, and a question about a paper on connected sets in neural networks.",https://www.reddit.com/r/MachineLearning/comments/q6drlo/d_a_place_to_discuss_papers_and_ask_questions_and/,carlml,MachineLearning,1970-01-01 00:00:01.634011797,Discussion,5,0,"This is a two-fold post: 1) a question about where to go discuss papers with peers (if there are already groups that do that). 2) a specific question about a particular papersThere are some initiatives to 1) such as [https://artigopapers.io/home/trending](https://artigopapers.io/home/trending) and [https://web.hypothes.is/](https://web.hypothes.is/) , which unfortunately do not seem very popular? Is there a place more like math stackexchange or math overflow where one can use Mathjax and ask/discuss papers related to ML? Also, are there groups that do this? I've seen here sometimes someone sends an invitation to a talk about a paper, not about a group of individuals studying paper. On that same note, many times I've emailed the authors of the paper using my school email and I've never gotten a reply; is this normal? is there some protocol as to what to say in those emails to get the author to reply? I doubt my email goes to their spam bin because I always make sure to not have generic messages.With respect to 2), it actually came about from a [post](https://www.reddit.com/r/MachineLearning/comments/q072ov/d_what_nice_mathematical_results_there_are_about/) I made some time ago. The paper in question is [On Connected Sublevel Sets in Deep Learning](https://arxiv.org/pdf/1901.07417.pdf) I am studying it. They show that there are unbounded connected sets on the weight space where the loss is non-increasing; they do that by finding two different points that have the same loss and showing the are connected by a continuous path along which the loss does not increase. If someone happens to have read/studied this paper, I'd appreciate some help. Right before the proof of Theorem 3.2, they show that there are two points U\* and U\^h that give the same output and hence same loss; I am struggling to see why these two points are different, in fact, it seems to me they are going to be the same, in which case proceeding with the rest of the proofs is futile. I know I am probably wrong about something, so that's why I think I need some guidance.",NEG
68,q66pl7,[D] Paper explained - WarpedGANSpace: Finding non-linear RBF paths in GAN latent space (5-minute summary),https://www.reddit.com/r/MachineLearning/comments/q66pl7/d_paper_explained_warpedganspace_finding/,KirillTheMunchKing,MachineLearning,1970-01-01 00:00:01.633988932,Discussion,8,1,"Linear  directions are great for GAN-based image editing, but who is to  say  that going straight across the latent space is the best option?  Well,  according to Christos Tzelepis and his colleagues from the Queen  Mary  University of London non-linear paths in the latent space lead to  more  disentangled and interpretable changes in the synthesized images   compared to existing SOTA methods! Their method, which is based on   optimizing a set of RBF warp functions, works without supervision and   learns a set of easily distinguishable image editing directions such as   pose and facial expressions.Full summary: [https://www.casualganpapers.com/unsupervised-discovery-nonlinear-latent-editing-directions-generator/WarpedGANSpace-explained.html](https://www.casualganpapers.com/unsupervised-discovery-nonlinear-latent-editing-directions-generator/WarpedGANSpace-explained.html)https://preview.redd.it/ix041l8f6ws71.png?width=996&format=png&auto=webp&s=3017807c58e5636e0dd131f8588bf1b2969000d3arxiv: [https://arxiv.org/pdf/2109.13357v1.pdf](https://arxiv.org/pdf/2109.13357v1.pdf)  code: [https://github.com/chi0tzp/WarpedGANSpace](https://github.com/chi0tzp/WarpedGANSpace)",POS
69,q64760,[D] are there non neural network-based connectionist systems?,https://www.reddit.com/r/MachineLearning/comments/q64760/d_are_there_non_neural_networkbased_connectionist/,paarulakan,MachineLearning,1970-01-01 00:00:01.633982001,Discussion,7,6,"Most  literature uses connectionist systems synonymous with neural networks.  Are there any other connectionists systems that do not employ neural  networks? By neural network, I mean all its variants like Spiking neural  networks, networks produced by neuroevolution, and of course  backpropagation networks.",POS
70,q5ez02,[R] Pairwise shape studies in 3D deep learning - Link to a free online lecture by the author in comments,https://i.redd.it/9c7pugbtcos71.png,pinter69,MachineLearning,1970-01-01 00:00:01.633894380,Research,226,1,,NEU
71,q5sprs,[D] What is a summary of the development and SOTA of image classification?,https://www.reddit.com/r/MachineLearning/comments/q5sprs/d_what_is_a_summary_of_the_development_and_sota/,Gere1,MachineLearning,1970-01-01 00:00:01.633947233,Discussion,16,17,"Does someone know a resources which \*succinctly\* summarizes the advances of image classification? Or maybe someone can write a good summary?Here is roughly what I've seen so far and maybe someone can fill the gaps:Neural networks / Perceptron and backpropagation have been known for many decades, but only with powerful computers they could be made useful. At first there was AlexNet. (What was the breakthrough there?). There was unsupervised pretraining. At some point BatchNorm was key. There was ResNet (it only introduce skip connections to enable more layers?). At some point EfficientNet suggested nicely working sizes for layers and how to scale. Unsupervised pretraining is not recommended anymore. Is normalization still recommended? Nowadays, we are at 90% top-1 accuracy on ImageNet which is still notably below human error(?) (even taking into account mistakes in the dataset) [https://paperswithcode.com/sota/image-classification-on-imagenet](https://paperswithcode.com/sota/image-classification-on-imagenet) .  So SOTA for image classification are still the neural networks from decades ago, but with powerful computers, skip connections and an idea for how to choose layer sizes(?)It would be brilliant to see a TLDR of image classification along these lines, which still captures all important milestones. Anyone?",POS
72,q5xky8,[R] Independent performance benchmarks (training) of Nvidia A10 and A30 impossible to find?,https://www.reddit.com/r/MachineLearning/comments/q5xky8/r_independent_performance_benchmarks_training_of/,longboard2020,MachineLearning,1970-01-01 00:00:01.633963996,Research,3,4,"Hi, has anyone come across comparison benchmarks of these two cards? I feel like I've looked everywhere but I cant' seem to find anything except for the official [nvidia numbers.](https://developer.nvidia.com/deep-learning-performance-training-inference)In particular I'm interested in their training performance (single gpu) on 2D/3D images when compared to the 3090 and the A6000/A40.Any info would be greatly appreciated!",POS
73,q5rw2n,[D] Is using GAN for data augmentation a chicken or the egg causality dilemma,https://www.reddit.com/r/MachineLearning/comments/q5rw2n/d_is_using_gan_for_data_augmentation_a_chicken_or/,Ok_Mind_4267,MachineLearning,1970-01-01 00:00:01.633943549,Discussion,13,11,"If I wanna use GAN for generating enough data for training a model, but i do not have enough data for GAN to train.how do you see this ,how to solve this.",POS
74,q5xy8a,[P] Open Domain Long Form Question Answering Applicability,https://www.reddit.com/r/MachineLearning/comments/q5xy8a/p_open_domain_long_form_question_answering/,LeoVS010,MachineLearning,1970-01-01 00:00:01.633965016,Project,2,3,"My friend and I want to apply current researches in QA networks in order to make a real [AI, which can answer most of the people's questions](https://t.me/AsqaBot). Founding researches and networks wasn't difficult, but deployment to production was challenging. Especially in our case, we are both software engineers, that only recently have started to learn about ML.In our opinion, the most promising setup is to combine the [BART](https://arxiv.org/abs/1910.13461) generator with the [DPR](https://arxiv.org/abs/2004.04906) searcher. [The implementation from Huggingface](https://github.com/huggingface/notebooks/blob/master/longform-qa/Long_Form_Question_Answering_with_ELI5_and_Wikipedia.ipynb) helped us a lot to get started. BART has been trained on the ELI5 dataset, a dataset of answers from [Explain Like I'm Five](https://www.reddit.com/r/explainlikeimfive/) subreddit. It is really taught for BART to answer complex questions in a simple way (and sometimes toxic way ðŸ˜‚). DPR is using a small subset of Wikipedia for the knowledge database. Surprisingly, it can answer most of the questions correctly.We rapidly deployed low resources [prototype](https://t.me/AsqaBot) in order to personally observe how does it work in reality. The main goal is to allow anyone to receive simple answers to complex questions. Most people are searching for answers on Wikipedia where they probably get confused by the complexity and the length of the articles. Who wants to read this many pages in order to get an answer.Here is the link to the bot [https://t.me/AsqaBot](https://t.me/AsqaBot) (It currently only in Telegram)So we want to get some advice on how to make the [bot](https://t.me/AsqaBot) better? We currently struggle with many possible ways to improve and have no idea which one of them can bring greater results. On the other hand, we would like to add the ability to rate answers, the only thing we don't know yet is how to train BART to learn from bad answers (I think that they are just as important as the good ones)[Asqa Example](https://preview.redd.it/ymbxg6877us71.jpg?width=582&format=pjpg&auto=webp&s=157b2508e2c1cae102e7e6b08c9be640f0ee8623)",POS
75,q6b22i,[D] Do you use any general CS in ML/DL?,https://www.reddit.com/r/MachineLearning/comments/q6b22i/d_do_you_use_any_general_cs_in_mldl/,ice_shadow,MachineLearning,1970-01-01 00:00:01.634002643,Discussion,0,27,"It just seems like penalized maximum likelihood, signal processing, nonlinear regression via numerical computing to me. Which all falls under applied math/stat. Where exactly does the â€œgeneral CSâ€ come in, outside of any infrastructure related aspects? Or is that the main place? Based on ISLR/ESLR and books like Probabilistic ML, it seems like in theory it can be done with very little general CS knowledge as long as you know probability+linear algebra+calc and stats behind regression. So how is it that CS departments got into ML? Is it mostly just in the computational efficiency/scalability aspects? The coding required to do optimization of DL isnâ€™t too much and mostly is like using PyTorch as a calculator. PT needs OOP, but its largely formulaic from what I have seen. Even more so with PT-lightning. Of course I guess inventing PT itself had general CS but in terms of developing new DL algorithms and optimization, does one ever need to go to extremely low level GPU programming and how often? When I go on arxiv, the majority of the papers donâ€™t really go to that level. What general CS concepts are you using and should you get familiar with when doing ML research?",POS
76,q5rhu8,[D] Multihead attention dimensionalities,https://www.reddit.com/r/MachineLearning/comments/q5rhu8/d_multihead_attention_dimensionalities/,WISEWENZEL,MachineLearning,1970-01-01 00:00:01.633941632,Discussion,5,10,"Hey guys,I'm trying to reproduce results from a [paper](https://link.springer.com/chapter/10.1007/978-3-030-30493-5_78) which uses the transformers architecture, and I want to use the [PyTorch implementation](https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html). However, there is one issue... In the paper I'm looking at, a transformer with model dimensionality d\_model=64 is used, yet each of the n\_h=8 separate attention heads uses d\_k=d\_v=64. Note that this is different from the description in the [original paper](https://arxiv.org/abs/1706.03762) by Vaswani et. al., where the model dimensionality is split across the different heads s.t. d\_k=d\_v=d\_model/n\_h. Since the PyTorch Transformer implementation is not flexible in this regard, I would have to re-implement quite a lot of stuff to reproduce the exact same results.If I would use d\_model=64, n\_h=8 s.t. d\_k=d\_v=8 with the PyTorch implementation, should I expect any significant drops in model performance, why/why not? Have any of you dealt with this issue before, and if so, found a simple solution to the problem?Any answers would be appreciated!&#x200B;Edit: Fixed hyperlinks, unclear text",POS
77,q5q0zv,[D] Zero-Shot Crosslingual Sentence Simplification (Paper Walkthrough),https://www.reddit.com/r/MachineLearning/comments/q5q0zv/d_zeroshot_crosslingual_sentence_simplification/,prakhar21,MachineLearning,1970-01-01 00:00:01.633934526,Discussion,2,0,Sentence simplification aims to make sentences easier to read and understand.This research proposes a Zero-Shot modeling framework that transfers simplification knowledge from English to another language (Crosslingual). ðŸ”¥Achieves better results than pivot-based and unsupervised methods when evaluated with humans and automated metrics. ðŸ”¥Paper summary: https://youtu.be/JOB7gwufvAwPaper Link: https://aclanthology.org/2020.emnlp-main.415.pdf,POS
78,q574t3,[D] Supporting Hacktoberfest for ML Datasets,https://www.reddit.com/r/MachineLearning/comments/q574t3/d_supporting_hacktoberfest_for_ml_datasets/,PhYsIcS-GUY227,MachineLearning,1970-01-01 00:00:01.633869026,Discussion,79,0,"Hey r/ML! I'm one of the creators of [DagsHub](https://dagshub.com/). [Hacktoberfest](https://hacktoberfest.digitalocean.com/) is well underway, but there's still a lot of time left, and we were missing some opportunities to contribute to the community on the ML/DS fronts.If you're not familiar, this is the 8th year Hacktoberfest is running. As its name suggests â€“ it's a month-long virtual hackathon for open source. You can give back to the community by creating pull requests! This provides a chance for you to:* Take part in the open-source ML community* Contribute to datasets that will be used by other practitioners, using open source tools* Create cool projects that can be a part of your portfolio, and **get some swag** on the way :DWe're supporting Hacktoberfest by creating an open-source catalog of datasets in the **audio** domain. The idea is to have a bunch of audio datasets, which will be completely open-source, with the ability to view, visualize (waveform, spectrograms, etc), and download for your projects. Check out this dataset that I created for an example: [https://dagshub.com/DagsHub/Librispeech-ASR-corpus/src/master/dev-clean/84/121123/84-121123-0000.flac](https://dagshub.com/DagsHub/Librispeech-ASR-corpus/src/master/dev-clean/84/121123/84-121123-0000.flac).https://preview.redd.it/c1rhgs6v9ms71.png?width=1128&format=png&auto=webp&s=7eedc7ab1e50951cef4ee0bcbf74d349dbb7a073You can read the full guidelines here: [https://dagshub.com/blog/hacktoberfest-x-dagshub-2/](https://dagshub.com/blog/hacktoberfest-x-dagshub-2/)Would be happy to answer questions, but I think if you're passionate about open-source ML, this is a great opportunity to contribute.",POS
79,q5pb2b,"[R] Researchers From Imperial College London Introduces â€˜HeadGANâ€™, A Novel One-Shot GAN-Based Method For Talking Head Animation And Editing",https://www.reddit.com/r/MachineLearning/comments/q5pb2b/r_researchers_from_imperial_college_london/,techsucker,MachineLearning,1970-01-01 00:00:01.633931373,Research,3,1,"While recent attempts to solve the problem of head reenactment using a single reference image have shown promising results, most of them perform poorly in terms of photo-realism and fail at preserving identity. Researchers from Imperial College London, Huawei Technologies (UK), and the University of Sussex propose â€˜[HeadGAN](https://arxiv.org/pdf/2012.08261.pdf)â€˜, a novel one-shot GAN-based method for talking head animation and editing.The research group took a different approach from most existing few-shot methods and used 3D face representations to condition synthesis. They benefit from prior knowledge of expression and identity disentanglement, enclosed within 3D Morphable Models (3DMMs).Â # [5 Min Quick](https://www.marktechpost.com/2021/10/10/researchers-from-imperial-college-london-introduces-headgan-a-novel-one-shot-gan-based-method-for-talking-head-animation-and-editing/) [Read](https://www.marktechpost.com/2021/10/10/researchers-from-imperial-college-london-introduces-headgan-a-novel-one-shot-gan-based-method-for-talking-head-animation-and-editing/) | [Paper](https://arxiv.org/pdf/2012.08261.pdf) | [Project](https://michaildoukas.github.io/HeadGAN/) | [Video](https://www.youtube.com/watch?v=5eg85fi7Y5g)&#x200B;https://reddit.com/link/q5pb2b/video/dawja626frs71/player",POS
80,q5ldpu,[P][D] Ubiquity of Quality ML datasets,https://www.reddit.com/r/MachineLearning/comments/q5ldpu/pd_ubiquity_of_quality_ml_datasets/,gordosuperfly,MachineLearning,1970-01-01 00:00:01.633916110,Discussion,5,1,"One issue I have in the ML world is the access to high quality scientific datasets. Sure, there are sites and hackathons available (e.g., kaggle) but it seems like there is no ""GitHub"" for quality datasets. Do any of you agree with this notion? What are your pain points when it comes to getting access to datasets for machine learning projects? This seems to be the biggest hurdle in industry/acedmia-- just getting the data! Would love to hear your thoughts on what could be done to resolve this. In addition, if any of you as working professionals in industry might have the time, I would love to discuss with you one on one for a school project. Please PM me! Much appreciate it.",POS
81,q57eqs,[P] cudnnxx: cuDNN C++ wrapper.,https://www.reddit.com/r/MachineLearning/comments/q57eqs/p_cudnnxx_cudnn_c_wrapper/,t-ubukata,MachineLearning,1970-01-01 00:00:01.633870108,Project,42,0,"cudnnxx is a cuDNN C++ wrapper. It's header only library, and cuDNN descriptors are RAII-wrapped, so you don't need to suffer from resource control.Any feedback would be greatly appreciated.[https://github.com/t-ubukata/cudnnxx](https://github.com/t-ubukata/cudnnxx)",POS
82,q58wnd,Vision Transformers in Medical Domain [D],https://www.reddit.com/r/MachineLearning/comments/q58wnd/vision_transformers_in_medical_domain_d/,MushiML,MachineLearning,1970-01-01 00:00:01.633875529,Discussion,11,12,"Hi,I am planning to use vision transformers in one of my medical images projects. I am looking for some comparison in terms of performance between CNN and transformers, especially for the medical domain. Please can you share or direct me to some resources that you read recently and talks about this topic. Also, if you can share some resources which explains the vision transformers in the best possible way :)Thank you",POS
83,q5g7m9,"[N] Sagemaker Experiments vs Comet, Neptune, WandB etc",https://www.reddit.com/r/MachineLearning/comments/q5g7m9/n_sagemaker_experiments_vs_comet_neptune_wandb_etc/,bfeeny,MachineLearning,1970-01-01 00:00:01.633898226,News,4,7,"In regard to working in a purely AWS environment, does anyone see the benefit of using Comet, Neptune, WandB, etc over just using Sagemaker Experiments and other Sagemaker MLOps tools?  Sagemaker experiments is not additional charge, neither is pipelines, hyperparameter optimization, etc.  for comparison, Comet is about $250,000 per year or more for my team, depending on the details.  I have looked into it myself and I donâ€™t see the benefit but I wondering if I am missing something.  Again this is for a purely AWS environment.",POS
84,q5lgxg,[D] Can text-generated pictures from the GANs be used for data augmentation?,https://www.reddit.com/r/MachineLearning/comments/q5lgxg/d_can_textgenerated_pictures_from_the_gans_be/,Ok_Mind_4267,MachineLearning,1970-01-01 00:00:01.633916402,Discussion,0,6,"the feasibility, i cannot find any paper about this",NEU
85,q4p3lf,[P] Bayesian optimization book,https://www.reddit.com/r/MachineLearning/comments/q4p3lf/p_bayesian_optimization_book/,romangarnett,MachineLearning,1970-01-01 00:00:01.633798227,Project,332,21,"I am in the process of finalizing a monograph on Bayesian optimization to be published next year by Cambridge University Press. The target audience is graduate students in machine learning, statistics, and related fields, but I hope practitioners will find it useful as well.A major goal of the book is to build up modern Bayesian optimization algorithms â€œfrom scratch,â€ revealing unifying themes in their design.I am making a draft available for initial commentary and erratum squashing:https://bayesoptbook.com/Once published, the book will remain freely available on the companion webpage.I welcome feedback via creating an issue on an associated GitHub repository:https://github.com/bayesoptbook/bayesoptbook.github.ioI hope the community will find this resource useful!-Roman Garnett",POS
86,q5lkxh,[D] AI Background Removal: a quick comparison between RVM & BGMv2,https://youtu.be/2GcegtA95pY,cloud_weather,MachineLearning,1970-01-01 00:00:01.633916848,Discussion,0,1,,NEU
87,q54gta,Paper with no public implementation [D],https://www.reddit.com/r/MachineLearning/comments/q54gta/paper_with_no_public_implementation_d/,RaivoK,MachineLearning,1970-01-01 00:00:01.633856543,Discussion,9,14,"For an upcoming publication (I am very inexperienced with academic writing) , I am studying several model blocks, one of which is this one called W3 [https://arxiv.org/abs/2004.01278](https://arxiv.org/abs/2004.01278). It is highly relevant. However, even though it is a year and a half old, there are no implementations of it anywhere and the authors have not published any code either. I will not be able to implement it myself either in my time constraint.&#x200B;Now, what should I do.1. Talk about W3 and introduce it in my paper (aka say its relevant and should be in my experiments) and compare to it in the ""related work"" section, but completely leave it out from any experiments. This way I will have to somehow justify why I am not including it in my experiments. For example, ""In spite of its relevance, we do not include W3 in our experiments due to missing public implementations"".2. leave it out of my paper completely.&#x200B;Mainly, I am wondering whether doing (1) would be a bad idea? It will help the completeness of my ""related work"" section if I can talk about W3.",NEG
88,q5c8ff,[D] Best papers to read for video understanding/synthesis,https://www.reddit.com/r/MachineLearning/comments/q5c8ff/d_best_papers_to_read_for_video/,scraper01,MachineLearning,1970-01-01 00:00:01.633885925,Discussion,2,4,Any good resources?,POS
89,q54tef,[D] Has anyone compared XVFI's interpolation quality with RIFE?,https://www.reddit.com/r/MachineLearning/comments/q54tef/d_has_anyone_compared_xvfis_interpolation_quality/,naweed__,MachineLearning,1970-01-01 00:00:01.633858265,Discussion,9,0," [GitHub - JihyongOh/XVFI: Official repository of XVFI (accepted in ICCV2021, Oral)](https://github.com/JihyongOh/XVFI)   It has been trained on 4k 1000fps clips, very interesting",POS
90,q4qrzi,[D] AI research: the unreasonably narrow path and how not to be miserable,https://www.reddit.com/r/MachineLearning/comments/q4qrzi/d_ai_research_the_unreasonably_narrow_path_and/,konasj,MachineLearning,1970-01-01 00:00:01.633803308,Discussion,89,6,"I just watched this excellent talk by Rosanne Liu about the difficulties many struggle with when doing ML research and how you have to decide whether you let the system change yourself or try to change the system.  As I have seen a bunch of discussions within this subreddit which left the impression that many struggle with the same questions, I thought it is worth to share. [https://www.youtube.com/watch?v=0blQp0\_9NwY](https://www.youtube.com/watch?v=0blQp0_9NwY)",POS
91,q4z9vb,[P] Generate READMEs Using AI,https://www.reddit.com/r/MachineLearning/comments/q4z9vb/p_generate_readmes_using_ai/,tomd_96,MachineLearning,1970-01-01 00:00:01.633832652,Project,11,0,"&#x200B;https://i.redd.it/cd44cn8q9js71.gifYou can use this program I wrote to generate readmes: [https://github.com/tom-doerr/codex-readme](https://github.com/tom-doerr/codex-readme)It's far from perfect, but I'm still surprised how well it works if you give it a few tries. It often generates interesting usage examples and explains the available command line options.You probably won't use this for larger projects, but I think this can make sense for small projects or single scripts. Many small scripts are very useful but might never be published because of the work that is required to document and explain it. Using this AI might assist you with that.Be aware that you need to get access to OpenAI's Codex API to use it.What do you think?",POS
92,q56f2c,[P] The fastest deep learning inference software for Arm Cortex-M microcontrollers,https://www.reddit.com/r/MachineLearning/comments/q56f2c/p_the_fastest_deep_learning_inference_software/,roelandbpn,MachineLearning,1970-01-01 00:00:01.633865889,Project,1,0,we at Plumerai built the fastest and most memory-efficient deep learning inference software for Arm Cortex-M microcontrollers. It has **40% lower latency** and requires **42% less RAM** than TensorFlow Lite for Microcontrollers with Armâ€™s CMSIS-NN kernels while retaining the same accuracy. It also greatly outperforms any other deep learning inference software for Arm Cortex-M.  We explain several of our optimizations here: [https://blog.plumerai.com/2021/10/cortex-m-inference-software/](https://blog.plumerai.com/2021/10/cortex-m-inference-software/),NEG
93,q4xdfs,Need advice regarding situation where my work is plagiarised [D],/r/computervision/comments/q4uz1l/just_saw_that_the_research_paper_i_have_been/,AhmedZubairGCU,MachineLearning,1970-01-01 00:00:01.633825081,Discussion,11,1,,NEU
94,q56r1y,[Discussion] ML Serving Framework for Real time predictions on tabular data,https://www.reddit.com/r/MachineLearning/comments/q56r1y/discussion_ml_serving_framework_for_real_time/,pythondeveloper77,MachineLearning,1970-01-01 00:00:01.633867410,Discussion,1,10,"HiWondering what people are using to serve ML models for real time predictions.We are working with gunicorn + flask at the moment which hard to scale and doesn't meet our response times needs (over 500 RPS  and going upwards)Our requirements are : using custom models ( with predict function that we load from pickle) , custom logging , API with SSL ( or if you have a better suggestion than HTTPS I'd like to know).Thanks",POS
95,q5e7e0,"[D] How to train your Yahtzee? What would be your approach plan for an optimal ML learned strategy starting from scratch? I have added mine, let me know what you would do different/better :)",https://www.reddit.com/r/MachineLearning/comments/q5e7e0/d_how_to_train_your_yahtzee_what_would_be_your/,Fun-Engineering2112,MachineLearning,1970-01-01 00:00:01.633892033,Discussion,0,3,"Hi all,Glad to see that this title intrigued you!This is my first Reddit post, hoping to find like minded people.How could the ""Perfect Yahtzee algorithm"" be created? Assuming you don't have to do all the work yourself (which would be a life's work and possibly a post-mortem Nobel prize if you achieve a framework that is applicable to many other games and problems?). Why are modern successful game  algo's not generally applicable to other problems?This rant is an ant step in the quest for a general ai framework. Been thinking the last 2 hours about that question as if I'm OCD.This would be my approach: Focus on the AI experts / programmers collaboration platform/framework and try to build a community. Identify which parts still need to be built and visualize / numerify that shit: - ML algorithms - Yahtzee simulator (Uber fast for Millions of runs - Input interfaces (website, OCR from picture, etc.) - Statistics framework: (Algorithm A beats Algorithm B with 2.3 points average over 165.765 simulations. R-value=xx.x) - Collaboration platform: I'm a noob in this, but I'm imagining something very visual at first: like a math problem written outside the university classroom. Some kind of forum for any science / tech enthousiast worldwide to run through topics. Click one: opens a visual graph with tasks already completed and still open. (All that fancy filtering on difficulty / estimated effort etc. Included of course). Does anything like that exists already? - ...Setting the first goal of having a website where you can enter (or scan, printscreen, webservice, whatever) any current configuration of a Yahtzee game. And it will return your best option at that point (""keep the 2 3's""). An easy website for Noobs and Pro's alike. If this is taking off, then of course beat the best human players as well with the optimal algorithm/ per situation optimal. All though, Yahtzee is not nearly as complex as chess, backgammon nor Go; so I guess this has been done before. The underlying motivation for myself being to learn more about how to achieve bigger goals in AI using collaboration platforms. Are abstract project management / frameworks possible over different games and problems? In an abstract framework, can a new algorithm quickly measure it's fitness over multiple disciplines / compare with it's peers?I'm sure I'm not the first nor the one with the most ML context to think of this. Sooo, share your knowledge please :)Apologies for the OCD like rant, your turn now ;)",POS
96,q4n2u1,"[R] Microsoft Researchers Introduce â€˜Mesh Graphormerâ€™, A Graph-Convolution-Reinforced Transformer (Codes Released)",https://www.reddit.com/r/MachineLearning/comments/q4n2u1/r_microsoft_researchers_introduce_mesh_graphormer/,techsucker,MachineLearning,1970-01-01 00:00:01.633791852,Research,37,2,"While 3D human pose and mesh reconstruction from a single image is a trending area of research because of its applications for human-computer interactions, it is also a very challenging process due to the complex body articulation.The current progress tools include transformers and graph convolutional neural networks in human mesh reconstruction. Transformer-based approaches are more effective in modeling nonlocal interactions among 3D mesh vertices and body joints. In contrast, GCNNs are good at exploiting neighborhood vertex interactions based on a pre-specified mesh topology.Researchers from Microsoft introduced a graph-convolution-reinforced transformer, named [Mesh Graphormer](https://arxiv.org/pdf/2104.00272.pdf), for reconstructing human pose and mesh from a single image. The researchers inject graph convolutions into transformer blocks to improve the local interactions among neighboring vertices and joints. The proposed Graphormer is free to join and attend all image grid features to leverage the strength of graph convolutions. Therefore, Graphormer and image grid features are utilized and enforced to improve human pose and mesh reconstruction performance.# [4 Min Read](https://www.marktechpost.com/2021/10/09/microsoft-researchers-introduce-mesh-graphormer-a-graph-convolution-reinforced-transformer/) | [Paper](https://arxiv.org/pdf/2104.00272.pdf) | [Code](https://github.com/microsoft/MeshGraphormer)&#x200B;https://preview.redd.it/c4d654lgwfs71.png?width=1392&format=png&auto=webp&s=0b80e22889e210af5d566b21248d8e9abf8f0545",POS
97,q4eicp,[R] Keypoint Communities,https://v.redd.it/3r58jvsyvcs71,Illustrious_Row_9971,MachineLearning,1970-01-01 00:00:01.633755400,Research,213,13,,NEU
98,q4hkix,"[R] Local plasticity rules can learn deep representations using self-supervised contrastive predictions. No need to backprop error signals between layers. Networks trained with this self-supervised and local rule build deep hierarchical representations of images, speech and video.",https://arxiv.org/abs/2010.08262,hardmaru,MachineLearning,1970-01-01 00:00:01.633770016,Research,51,3,,NEU
99,q4teff,[P] Speech to emotion,https://www.reddit.com/r/MachineLearning/comments/q4teff/p_speech_to_emotion/,Ingvariuss,MachineLearning,1970-01-01 00:00:01.633811472,Project,5,12," Hi,I'm building out a Speech to Emotion model and have got it to be \~85% accurate. For its proper use, it should be at least above 92% accuracy. The current dataset that I'm using has about 120k total examples split across 8 emotions. I'm working on obtaining more data but are there any tips and tricks that I could use to improve the model performance? Anything that comes to mind is welcome. Thanks in advance.",POS
100,q51fei,[R] Batch size-invariance for policy optimization,https://arxiv.org/abs/2110.00641,hardmaru,MachineLearning,1970-01-01 00:00:01.633841690,Research,3,1,,NEU
101,q4lvl5,[P] Denoising Diffusion Probabilistic Models implementation with annotations,https://www.reddit.com/r/MachineLearning/comments/q4lvl5/p_denoising_diffusion_probabilistic_models/,mlvpj,MachineLearning,1970-01-01 00:00:01.633787920,Project,6,0,We implemented Denoising Diffusion Probabilistic Models (DDPM) paper.Code with annotations: [https://nn.labml.ai/diffusion/ddpm/index.html](https://nn.labml.ai/diffusion/ddpm/index.html)* [Github](https://github.com/labmlai/annotated_deep_learning_paper_implementations/tree/master/labml_nn/diffusion/ddpm)* [Paper](https://papers.labml.ai/paper/2006.11239)* [Conference video](https://slideslive.com/38936172/denoising-diffusion-probabilistic-models),NEU
102,q4l2hl,[D] Lower learning rate vs gradient accumulation.,https://www.reddit.com/r/MachineLearning/comments/q4l2hl/d_lower_learning_rate_vs_gradient_accumulation/,chasep255,MachineLearning,1970-01-01 00:00:01.633785117,Discussion,5,6,I am trying to optimize a model similar to VQGAN.  I am away from my RTX 3090 and stuck using an RTX 3070 right now.  Normally I have used a batch size of 16 but with 8GB of memory I can only fit batches of size 4.  This seems too low to me so I implemented some simple gradient accumulation  to get an effective batch size of 16.  What I want to know is would I be better off just using batches of size 4 with 1/4 the learning rate?  Also could we take this to the extreme and use batches of size 1 with 1/16th the learning rate?  Anyone experiment with this?,POS
103,q4p05f,[P] Clone a voice in 5 seconds to generate arbitrary speech in real-time,https://github.com/babysor/MockingBird,binaryfor,MachineLearning,1970-01-01 00:00:01.633797932,Project,1,1,,NEU
104,q4psyx,[D] Security feature labeled dataset for code2vec,https://www.reddit.com/r/MachineLearning/comments/q4psyx/d_security_feature_labeled_dataset_for_code2vec/,maazamalik,MachineLearning,1970-01-01 00:00:01.633800344,Discussion,0,0,"I am looking for a dataset that would contain code snippets (or vector representing it) and labels that are security specific features such as authentication, encryption, logging etc. I need to apply techniques like code2vec [https://github.com/tech-srl/code2vec](https://github.com/tech-srl/code2vec) but with security-specific labels. Any leads where can I find this kind of dataset?",POS
105,q4goiu,[Discussion] Question about Variational Autoencoder,https://www.reddit.com/r/MachineLearning/comments/q4goiu/discussion_question_about_variational_autoencoder/,Blasphemer666,MachineLearning,1970-01-01 00:00:01.633765796,Discussion,5,5,"Why in Variational Autoencoder they use centered isotropic multivariate Gaussian N(z;0,1) for calculating KL divergence?Could we alternatively use the distribution of existing dataset to get the mean and s.d. in the latent space?[VAE paper](https://arxiv.org/pdf/1312.6114.pdf)",NEU
106,q43unx,[N] Python Polars release v0.10.0,https://www.reddit.com/r/MachineLearning/comments/q43unx/n_python_polars_release_v0100/,ritchie46,MachineLearning,1970-01-01 00:00:01.633718085,News,12,0,Happy to announce a new release of [polars](https://github.com/pola-rs/polars). Polars is one of the fastest dataframe libraries that help you scale an a single machine. Its runtime performance is typically 2-20x times faster than pandas with less memory consumption. It comes with a powerful expression API that allows to compose complex queries without the need of running (slow) python lambdas/functions.See the complete [changelog](https://github.com/pola-rs/polars/blob/master/py-polars/CHANGELOG.md).,POS
107,q3xon8,[P] Fast and Simple Image Captioning model using CLIP and GPT-2,https://www.reddit.com/r/MachineLearning/comments/q3xon8/p_fast_and_simple_image_captioning_model_using/,RonMokady,MachineLearning,1970-01-01 00:00:01.633699421,Project,30,14,"Hi All,Image Captioning used to be a very complicated task, but now all you need is some pretrained CLIP and GPT-2. Check out  my [project repo](https://github.com/rmokady/CLIP_prefix_caption) for code and inference notebook, including our pretrained models. You can easily try on arbitrary images, please share your results :).We present a new approach that does not requires additional supervision (such as object annotations), thus can be applied to any data. In addition, our model's training time is much faster than similar methods while achieving close to state-of-the-art results. We trained  our model for the huge Conceptual Captions dataset contains over 3M images using a single 1080 GPU!We use the [CLIP](https://github.com/openai/CLIP) model, which was already trained over an extremely large number of images, so is capable of generating semantic encodings for arbitrary images without additional supervision. To produce meaningful sentences we fine-tune a pretrained GPT-2. The key idea is to use the CLIP encoding as a prefix to the textual captions by employing a simple MLP over the raw encoding, and then fine-tune our language model to generate a valid caption.&#x200B;What's your thoughts about it?[Conceptual Captions dataset](https://preview.redd.it/0ueotedi98s71.png?width=1714&format=png&auto=webp&s=1cdf8dae57770380337c718fbabf6f58ab45a26d)[COCO dataset](https://preview.redd.it/9u29mfdi98s71.png?width=1720&format=png&auto=webp&s=b68b83ab7ea42f507af01d70ede3a70cea82826e)",POS
108,q4c9vt,[D] clip - masked self attention for pretrained language models?,https://www.reddit.com/r/MachineLearning/comments/q4c9vt/d_clip_masked_self_attention_for_pretrained/,Cold-Writing2826,MachineLearning,1970-01-01 00:00:01.633745999,Discussion,3,6,"In the original CLIP paper there is a statement:> Masked self-attention was used in the text encoder to preserve the ability to initialize with a pretrained language model or add language modelling as an auxillary objectiveI'm not really following what this means. Does anyone have a reference I can follow to get the required background to grokk it?(Sorry repost, deleted old one to add the required tag)",POS
109,q42c95,[R] Learning high-speed flight in the wild,https://www.reddit.com/r/MachineLearning/comments/q42c95/r_learning_highspeed_flight_in_the_wild/,Sirisian,MachineLearning,1970-01-01 00:00:01.633713681,Research,9,3,"[Press Release](https://www.media.uzh.ch/en/Press-Releases/2021/Drone-in-the-wild.html)[Paper](https://www.science.org/doi/10.1126/scirobotics.abg5810)[Video from the above press release is on Youtube also](https://www.youtube.com/watch?v=m89bNn6RFoQ)> Quadrotors are agile. Unlike most other machines, they can traverse extremely complex environments at high speeds. To date, only expert human pilots have been able to fully exploit their capabilities. Autonomous operation with onboard sensing and computation has been limited to low speeds. State-of-the-art methods generally separate the navigation problem into subtasks: sensing, mapping, and planning. Although this approach has proven successful at low speeds, the separation it builds upon can be problematic for high-speed navigation in cluttered environments. The subtasks are executed sequentially, leading to increased processing latency and a compounding of errors through the pipeline. Here, we propose an end-to-end approach that can autonomously fly quadrotors through complex natural and human-made environments at high speeds with purely onboard sensing and computation. The key principle is to directly map noisy sensory observations to collision-free trajectories in a receding-horizon fashion. This direct mapping drastically reduces processing latency and increases robustness to noisy and incomplete perception. The sensorimotor mapping is performed by a convolutional network that is trained exclusively in simulation via privileged learning: imitating an expert with access to privileged information. By simulating realistic sensor noise, our approach achieves zero-shot transfer from simulation to challenging real-world environments that were never experienced during training: dense forests, snow-covered terrain, derailed trains, and collapsed buildings. Our work demonstrates that end-to-end policies trained in simulation enable high-speed autonomous flight through challenging environments, outperforming traditional obstacle avoidance pipelines.",NEG
110,q4a5df,[D] Sequential decision-making aspect in FAIR's CompilerGym environments?,https://www.reddit.com/r/MachineLearning/comments/q4a5df/d_sequential_decisionmaking_aspect_in_fairs/,definitelyuncertain_,MachineLearning,1970-01-01 00:00:01.633737973,Discussion,2,1,"As you may have heard, Facebook recently released [CompilerGym](https://compilergym.com/index.html), which is claimed to be ""a toolkit for applying reinforcement learning to compiler optimizations"".I think that is a very fascinating idea. However, I am struggling to see in the environments currently available in the suite what sequential aspect to the problem exists, if any. For example, in the GCC Environment, the goal is to choose the appropriate combination of GCC flags that leads to the best machine code according to some criterion. This is clearly a supervised learning or contextual bandit problem (with some additional information provided along with the reward), because the choice of flags for a given program does not affect the reward from future choices or the programs shown in the future.I'm no expert in this area, but I believe this is the case for the LLVM environments also. Could someone enlighten me if I'm missing something here? Otherwise, what sequential decision-making problems do you think might belong in a project like this?",POS
111,q3tvy3,User Evaluation for my Research Project [D],https://www.reddit.com/r/MachineLearning/comments/q3tvy3/user_evaluation_for_my_research_project_d/,prakhar21,MachineLearning,1970-01-01 00:00:01.633683447,Discussion,33,8,"Hey everyone , Hope all of you are doing well.I need a small help from you guys to evaluate the project that I have been doing for quite some time now. It's gonna take less than 5 min. for sure. It would be really great if you can participate in the survey - https://forms.gle/2gkUaFEnSPcRUg9s8The project is about automatically generating Trailer/Preview videos for academic courses. As a part of this user study, you will be asked to answer a couple of questions regarding the quality of our AI-generated Trailers. All the questions are self-explanatory, although we have provided extra information wherever required. IMPORTANT: You will be seeing a total of 2 trailers(both generated from the same underlying corpus). They differ from each other at the level of detail and aesthetics. You will be required to answer 8 questions per trailer.Thank you once again!",POS
112,q3xo2a,[D] Your Go-To Image Dataset Analysis Tools?,https://www.reddit.com/r/MachineLearning/comments/q3xo2a/d_your_goto_image_dataset_analysis_tools/,onyx-zero-software,MachineLearning,1970-01-01 00:00:01.633699363,Discussion,9,11,"Hey all, I'm just curious what tools you have come across for image data exploration? I have a fairly large dataset and had a (surprisingly) hard time finding a generic tool I could use to explore the data without rolling my own in a collection of notebooks. I found a few duplicate-image detectors, but for the most part none of them did what I needed. My gut feeling is that there is a very obvious tool and I just haven't heard of it before, so I wanted to ask the community. Things I would like to have from such a tool:- Duplicate image detection- PCA/LDA (or other methods of unsupervised feature analysis)- Anomaly detection- Clustering by image content and/or Metadata (k means or mean-shift)None of these (other than maybe anomaly detection) seem like they'd be particularly novel and have been around for a long time, which is why I figured there must be something out there.Thanks!",POS
113,q3y41e,[R] Apple Study Reveals the Learned Visual Representation Similarities and Dissimilarities Between Self-Supervised and Supervised Methods,https://www.reddit.com/r/MachineLearning/comments/q3y41e/r_apple_study_reveals_the_learned_visual/,Yuqing7,MachineLearning,1970-01-01 00:00:01.633700856,Research,7,1,"An Apple research team performs a comparative analysis on a contrastive self-supervised learning (SSL) algorithm (SimCLR) and a supervised learning (SL) approach for simple image data in a common architecture, shedding light on the similarities and dissimilarities in their learned visual representation patterns. Here is a quick read: [Apple Study Reveals the Learned Visual Representation Similarities and Dissimilarities Between Self-Supervised and Supervised Methods.](https://syncedreview.com/2021/10/08/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-120/)The paper *Do Self-Supervised and Supervised Methods Learn Similar Visual Representations?* is on [arXiv](https://arxiv.org/abs/2110.00528).",NEU
114,q3sye7,[R] TruthfulQA: Measuring How Models Mimic Human Falsehoods,https://arxiv.org/abs/2109.07958,SubstantialRange,MachineLearning,1970-01-01 00:00:01.633678699,Research,8,7,,NEU
115,q43dhw,[P] Creating a cross-platform automatically updated wiki,https://www.reddit.com/r/MachineLearning/comments/q43dhw/p_creating_a_crossplatform_automatically_updated/,EmilKay,MachineLearning,1970-01-01 00:00:01.633716658,Project,0,2,"Hey all, Not sure if this is the right place, but here goes. Weâ€™d like to create a cross-platform automatically-updating wiki. We got a pretty big budget for this. Think following scenarios: If someone shares a presentation in Microsoft Teams public channel, related to a topic. Throw it in the wiki (Collab. Platform)Someoneâ€™s account manager for a client? Throw it in the wiki (CRM Platform)Someoneâ€™s available to work in 2 weeks? Throw it in the wiki (ERP Platform) Problem right now is I have no idea at all where to begin. Anyone got any inputs? Could be platforms, AI, machine learning, anything really. The more specific the better. All I got is â€˜start with the data people will actually need/should be easily available.",POS
116,q367tz,[Project] Generating cool names for machine learning projects,https://www.reddit.com/r/MachineLearning/comments/q367tz/project_generating_cool_names_for_machine/,joaoperfig,MachineLearning,1970-01-01 00:00:01.633602869,Project,305,14,"Just wanted to share a silly little project of mine for helping with publishing projects  GRaNDPapA: Generator of Rad Names from Decent Paper Acronyms   Trying to publish a new machine learning model and can't write a decent title for your paper?     Are all of your titles just sequences of 10 keywords?     Jealous of the cool kids with their sweet paper names like ""ALBERT"" and ""ELMo""?     Well look no further, GRaNDPapA will take whatever buzzwords you want in the title and make a cool Acronym out of it.    [https://github.com/joaoperfig/GRaNDPapA](https://github.com/joaoperfig/GRaNDPapA)",POS
117,q3m4g6,[D] Would many ephemeral RNN's enable compute efficient world models?,https://www.reddit.com/r/MachineLearning/comments/q3m4g6/d_would_many_ephemeral_rnns_enable_compute/,Douglas_schon,MachineLearning,1970-01-01 00:00:01.633651846,Discussion,25,12,"Recently, I've been reading A Thousand Brains by Jeff Hawkins and I went looking for research that attempts to replicate some of the structure he describes in the neocortex. Basically, many sparse highly parallelizable reference frames that build a predictive world model. I found a lot of hierarchical rnn research papers and a lot of multi agent rnn research papers but nothing that seems to attempt to build something like what I would imagine would be possible. What I'm asking for here is a sanity check. Does this make sense and could it be something worth spending cycles on?My imagined implementation would be a model that has an actor-critic model for itself. When it encounters something new it builds a new actor-critic model and places that model on a latent space where the location in that latent space is determined by the perceived similarity to other models that have been placed in that space. The weights of this new model would be instantiated to be similar to the models in a similar location in the latent space. I'm currently imagining the similarity metric would either be determined by some self-supervised learner based on perceived characteristics and/or would just be updated based on actor weights. If the observed reference frame is sufficiently similar to another one of the models then observations with respect to it are applied to that old reinforcement learner instead of a new one being created.At this point it will be possible to in theory build a representation of what any specific object the agent encounters will do by itself by letting each model predict it's next state. You would update the models you are actively observing when they do something unexpected. The next step is to add the ability to pass references to the other models to each other to allow the reference frames to interact with each other. This could be done by giving reference frame 1 the location in the latent space of reference frame 2 along with reference frame 2â€™s expected action state and world state into the input of the reference frame 1 RNN.Now the agent will be able to develop causal maps of the environment including themselves. To build long chains of these causal maps will still run into catastrophic forgetting but making it so groups of reference frames can be represented as a single reference frame with methods similar to hierarchical reinforcement learning should make it possible to build long complex causal theories about how objects interact.Whatâ€™s more, the vast majority of the time any given reference frame will not be used and it isnâ€™t tightly coupled to a central algorithm so you can save the network state and shut down networks you aren't using. This means you only should ever be working with a small set of highly specialized models at a time that are very good at predicting what the current relevant world state will do next while also being able to store a massive amount of accurate world state without having to constantly be dealing with a monolithic network. My hope is such a system would ultimately be large, but fairly compute efficient and be better at understanding causality which in turn would make one-shot learning more tenable.So what do ya think? Is this insane and overly complex? Any existing research you know of that might shed light on any of pieces of this? Think it might work? Obviously this would be damn hard to pull off even if it would, I'm fine with that.If you think this wouldn't work I would love to know why.Edit: Cool upvotes. Wasn't really expecting that. I'm looking at grad schools so if you have recommendations on who I should talk to about this stuff or where I should apply let me know.",POS
118,q3wkg4,"[D] Using sigmoid activation for input points that have range below 0 to ""normalize"" it ?",https://www.reddit.com/r/MachineLearning/comments/q3wkg4/d_using_sigmoid_activation_for_input_points_that/,Kendolph,MachineLearning,1970-01-01 00:00:01.633695474,Discussion,2,7,"If some points of your data go from like -10 to 10 and some are always above 0 is it a good idea to use Sigmoid activation for the first layer to kind of ""normalize"" the input to 0-1 and then ReLU for the other layers ? Or should I somewhat ""map"" the data to be above 0 and skip the sigmoid layer ?",POS
119,q3ykw0,[D] Did you use AI engines in the cloud? Your feedback?,https://www.reddit.com/r/MachineLearning/comments/q3ykw0/d_did_you_use_ai_engines_in_the_cloud_your/,tah_zem,MachineLearning,1970-01-01 00:00:01.633702377,Discussion,0,6,"Hi there!Have you, if you're a developer, ever used and integrated AI engines in the cloud? By that I mean APIs offered by companies to process your data (image recognition, machine translation, text mining, etc.).If so, in what context and what is your feedback? What are the problems you have encountered?Thanks,Taha",NEG
120,q3ua9o,[P] Masking and embedding in variable length LSTM autoencoder,https://www.reddit.com/r/MachineLearning/comments/q3ua9o/p_masking_and_embedding_in_variable_length_lstm/,Tentamenstress,MachineLearning,1970-01-01 00:00:01.633685439,Project,3,14,"Hi all,I am working on an LSTM autoencoder that I want to use to create dense embeddings of sequences. The sequences consist of encoded values and are of variable length. Since they are encoded I think I have to use an embedding layer. Additionally, since they are of variable length, I think masking is required. The problem is that I am not sure how to combine the two. Is it good practice to first mask and then embed, or does this nullify the effect of masking? I am new to both concepts, so any help would be greatly appreciated.Thanks!EDIT: meant padding instead of masking.",POS
121,q3v2q2,"[R] Ready, Steady, Go AI: A practical tutorial on fundamentals of artificial intelligence and its applications in phenomics image analysis",https://www.reddit.com/r/MachineLearning/comments/q3v2q2/r_ready_steady_go_ai_a_practical_tutorial_on/,aTestCandidate,MachineLearning,1970-01-01 00:00:01.633689285,Research,0,0,"Advances in AI technologies have the potential to significantly increase our ability to turn plant phenomics data into valuable insights. However, performing such analyses requires specialized programming skills commonly reserved for computer scientists. We created an interactive tutorial with free, open-source, and FAIR notebooks that can aid researchers to conduct such analyses without the need for an extensive coding experience. We supplemented it with a practical guide on how to implement AI and explainable AI (X-AI) algorithms that augment and complement human experience in classifying tomato leaf diseases and spider mites. Our tutorial is not only applicable to other stresses but also transferable to other plants and research domains, making it possible for researchers from various scientific fields to generate insights into their data. Check out our paper at https://doi.org/10.1016/j.patter.2021.100323",POS
122,q3tshg,[D] What laptop do you have ?,https://www.reddit.com/r/MachineLearning/comments/q3tshg/d_what_laptop_do_you_have/,strojax,MachineLearning,1970-01-01 00:00:01.633682934,Discussion,0,15,"I am wondering what kind of laptop research scientist at deepmind, openAI and other companies running mainly deep learning models employees have. Is the market still heavily cloud oriented or are companies starting to buy powerful laptop such as the tensorbooks from lambdalabs ?Edit: looks like I have misguided the answers with the above text. In a time where cloud computing has been accepted by a large majority of research scientist/engineers, what make a laptop, a good laptop ? I opened the question on whether computing power could be a criteria for a laptop but I think we kinda all agree that it is not a thing.",POS
123,q38a35,[D] Paper explained - Unsupervised Discovery of Interpretable Directions in the GAN Latent Space (5-minute summary),https://www.reddit.com/r/MachineLearning/comments/q38a35/d_paper_explained_unsupervised_discovery_of/,KirillTheMunchKing,MachineLearning,1970-01-01 00:00:01.633610654,Discussion,18,6,"https://i.redd.it/b7bb0ksmx0s71.gifGAN-based editing is great, we all know that! Do you know what isnâ€™t?  Figuring out what the heck you are supposed to do with a latent vector to edit the corresponding image in a   coherent way. Turns out taking a  small step in a random direction will most likely change more than one aspect of the photo since latent spaces of most well-known generators are rather entangled, meaning that by adding a smile to the generated face you are likely to also unintentionally change the hair color, the eye shape or any number of other wacky things. In this paper by Andrey  Voynov and Artem Babenko from Yandex, a new unsupervised method is introduced that discovers meaningful disentangled editing directions for simple attributes such as gender, age, etc as well as less obvious ones such as background removal, rotation, and background blur.Check out the [full paper summary](https://www.casualganpapers.com/unsupervised-discovery-editing-directions-gan-latent-space/Unsupervised-Directions-Discovery-explained.html) on Casual GAN Papers (Reading time \~5 minutes).\[[arxiv](https://arxiv.org/pdf/2002.03754.pdf)\]\[[github](https://github.com/anvoynov/GanLatentDiscovery)\]&#x200B;Subscribe to [my channel](https://t.me/casual_gan) and follow me on [Twitter](https://twitter.com/KirillDemochkin) for weekly AI paper summaries!",POS
124,q35lex,[R] Patches Are All You Need?,https://openreview.net/forum?id=TVHS5Y4dNvM,hardmaru,MachineLearning,1970-01-01 00:00:01.633600201,Research,25,12,,NEU
125,q3akjb,[D] Anyone used Gradio or Streamlit before?,https://www.reddit.com/r/MachineLearning/comments/q3akjb/d_anyone_used_gradio_or_streamlit_before/,kid_blizzard,MachineLearning,1970-01-01 00:00:01.633617695,Discussion,10,11,Re: [my previous post](https://www.reddit.com/r/MachineLearning/comments/pz8if5/d_do_you_find_it_hard_to_communicate_the_value_of/) on communicating your ML work effectively. Someone directed me to [Gradio](https://www.gradio.app/) and [Streamlit](https://streamlit.io/) and I'm curious to know people's experience with it and how valuable they find it.,POS
126,q3clcy,[D] your favourite iclr 2022 papers,https://www.reddit.com/r/MachineLearning/comments/q3clcy/d_your_favourite_iclr_2022_papers/,gauravc2796,MachineLearning,1970-01-01 00:00:01.633623473,Discussion,6,5,"Iclr 2022 papers is open with anonymous authors ðŸ˜‚ðŸ™ˆI have almost reviewed almost dozen of papers. (Totally biases over titles)ðŸ˜Will share a mega video review with explainer on the selected paper's. Any opinions, thoughts on any paper you can share here.Also would love to know which papersðŸ“„ interests you.I will go firstðŸ™‹ðŸ»â€â™‚ï¸Patches Are All You Need?: https://openreview.net/pdf?id=TVHS5Y4dNvM MEMORIZING TRANSFORMERS: https://openreview.net/pdf?id=TrjbxzRcnf- CoBERL: Contrastive BERT for Reinforcement Learning: https://openreview.net/pdf?id=RNn-2FCIdxa HYPERTRANSFORMER: ATTENTION-BASED CNN MODEL GENERATION FROM FEW SAMPLES: https://openreview.net/pdf?id=E9z2A1-O7e DICTFORMER: TINY TRANSFORMER WITH SHARED DICTIONARY https://openreview.net/pdf?id=GWQWAeE9EpB TRANSFORMERS ARE META-REINFORCEMENT LEARNERS: https://openreview.net/pdf?id=H7Edu1_IZgR NEURAL PARAMETER ALLOCATION SEARCH: https://openreview.net/pdf?id=srtIXtySfT4 PIX2SEQ: A LANGUAGE MODELING FRAMEWORK FOR OBJECT DETECTION: https://openreview.net/pdf?id=e42KbIw6Wb COSFORMER : RETHINKING SOFTMAX IN ATTENTION: https://openreview.net/pdf?id=Bl8CQrx2Up4 REGIONVIT: REGIONAL-TO-LOCAL ATTENTION FOR VISION TRANSFORMERS: https://openreview.net/pdf?id=T__V3uLix7V PROMISSING: PRUNING MISSING VALUES IN NEURAL NETWORKS: https://openreview.net/pdf?id=M_o5E088xO5 TL;DR:TWIN LEARNING FOR DIMENSIONALITY REDUCTION: https://openreview.net/pdf?id=VppWsjXgBY6 AN INVESTIGATION INTO THE ROLE OF AUTHOR DEMOGRAPHICS IN ICLR PARTICIPATION AND REVIEW: https://openreview.net/pdf?id=1DUwCRNAbA SLICED RECURSIVE TRANSFORMER: https://openreview.net/pdf?id=VFDDn-7_NRZ UNCONDITIONAL DIFFUSION GUIDANCE: https://openreview.net/pdf?id=lsQCDXjOl3k List of all papers: ðŸ“‘ https://twitter.com/SergeyI49013776/status/1445816149963251717?t=6hbx1gGCz-hMUwSFTNKz_A&s=19",POS
127,q2u2kx,[D] Paper Explained - Grokking: Generalization beyond Overfitting on small algorithmic datasets (Full Video Analysis),https://www.reddit.com/r/MachineLearning/comments/q2u2kx/d_paper_explained_grokking_generalization_beyond/,ykilcher,MachineLearning,1970-01-01 00:00:01.633556044,Discussion,132,37,"[https://youtu.be/dND-7llwrpw](https://youtu.be/dND-7llwrpw)Grokking is a phenomenon when a neural network suddenly learns a pattern in the dataset and jumps from random chance generalization to perfect generalization very suddenly. This paper demonstrates grokking on small algorithmic datasets where a network has to fill in binary tables. Interestingly, the learned latent spaces show an emergence of the underlying binary operations that the data were created with.&#x200B;OUTLINE:0:00 - Intro & Overview1:40 - The Grokking Phenomenon3:50 - Related: Double Descent7:50 - Binary Operations Datasets11:45 - What quantities influence grokking?15:40 - Learned Emerging Structure17:35 - The role of smoothness21:30 - Simple explanations win24:30 - Why does weight decay encourage simplicity?26:40 - Appendix28:55 - Conclusion & Comments&#x200B;Paper: [https://mathai-iclr.github.io/papers/papers/MATHAI\_29\_paper.pdf](https://mathai-iclr.github.io/papers/papers/MATHAI_29_paper.pdf)",POS
128,q3bgsu,[D] Best way to publish code for a paper,https://www.reddit.com/r/MachineLearning/comments/q3bgsu/d_best_way_to_publish_code_for_a_paper/,mikcnt,MachineLearning,1970-01-01 00:00:01.633620231,Discussion,3,7,"Hello everyone. Title is approximately the question: what is the best way to publish code for a paper? I'll try to explain: after I've finished working on a paper, I've probably got a repository on Github with lots of commits, and probably lots of things in general. Should I create a new repository, with very few things, just to show how the models work? Should I keep things very simple?    Also, what about templates for experiments reproducibility: should I use them for the final code release? Same question goes for code wrappers, as PyTorch Lightning. Is that a good idea to publish code written in Lightning, or should I switch back to vanilla PyTorch for the release?    Thanks in advance.",POS
129,q3avhc,[R] Facebook & CMUâ€™s Zero-Shot VideoCLIP Outperforms Fully-Supervised SOTA Methods for Video-Text Understanding,https://www.reddit.com/r/MachineLearning/comments/q3avhc/r_facebook_cmus_zeroshot_videoclip_outperforms/,Yuqing7,MachineLearning,1970-01-01 00:00:01.633618590,Research,4,1,"A research team from Facebook AI and CMU presents VideoCLIP, a contrastive approach for pretraining a unified model for zero-shot video and text understanding without requiring annotated data for downstream tasks. Here is a quick read: [Facebook & CMUâ€™s Zero-Shot VideoCLIP Outperforms Fully-Supervised SOTA Methods for Video-Text Understanding.](https://syncedreview.com/2021/10/07/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-119/)The researchers have open-sourced the VideoCLIP code on the project [GitHub](https://github.com/pytorch/%20fairseq/tree/main/examples/MMPT). The paper *VideoCLIP: Contrastive Pre-Training for Zero-Shot Video-Text Understanding* is on [arXiv](https://arxiv.org/abs/2109.14084).",POS
130,q331f7,[R] Should You Go Deeper? Optimizing Convolutional Neural Network Architectures without Training by Receptive Field Analysis,https://www.reddit.com/r/MachineLearning/comments/q331f7/r_should_you_go_deeper_optimizing_convolutional/,KrakenInAJar,MachineLearning,1970-01-01 00:00:01.633588173,Research,13,1,"TL;DR Essentially, we can predict how efficient an architecture will utilize their parameters with some napkin math and leverage this to optimize the neural architectures before ever training them. We predict which layers will be unproductive / idle by computing the minimal-receptive field of each layer and comparing it to the input resolution. Modifying the architectures to better match the input resolution can be done before setting up training and improves the predictive performance of the models consistently on the task ad hand.Paper Link: [https://arxiv.org/abs/2106.12307](https://arxiv.org/abs/2106.12307)",POS
131,q2y9n5,[D] Deep Learning Framework Benchmark,https://www.reddit.com/r/MachineLearning/comments/q2y9n5/d_deep_learning_framework_benchmark/,kajika91,MachineLearning,1970-01-01 00:00:01.633569819,Discussion,26,3,"I made a blog article about [benchmarking deep learning framworks](https://ayo.tokyo/blog/2021-10-07/Deep%20Learning%20Framework%20Benchmarks) and the [code is open sourced on github](https://gitlab.com/corentin-pro/dl_bench).I think having some benchmarks on PyTorch/TF/Jax can be interesting, I would gladly accept contribution and remarks.",POS
132,q2kfm4,[R] 6 Key Jobs in Data Industry,https://www.reddit.com/r/MachineLearning/comments/q2kfm4/r_6_key_jobs_in_data_industry/,literallair,MachineLearning,1970-01-01 00:00:01.633526768,Research,305,43,"  Hey, Iâ€™m Alex Miller, Director of Data Science at Neuton.ai. Iâ€™d be happy to share my experience through a series of articles to inspire colleagues to scale new career heights in the DS field. My first article is an overview of the key roles in the data industry and their average salaries. In the upcoming posts, Iâ€™m going to cover topics such as tips for an ideal CV, top books to boost a data-driven outlook, review of best AutoML solutions, common business cases to solve with AutoML, and more. **Follow me to stay tuned!**# Average Salaries Per Year (USA)[Average Salaries Per Year \(USA\)](https://preview.redd.it/u370zcrb5ur71.png?width=4966&format=png&auto=webp&s=97183a2c2c942de49ac9bca4adfa89318438a8b8)# The Most Popular Data Roles to ConsiderThere is absolutely no doubt that the data industry is a promising landscape, offering great flexibility and generous employment opportunities. The market swarms with data-related jobs for all tastes, so sometimes it's really easy to confuse one with another.If the multiplicity of data roles still puzzles you, Iâ€™ll be glad to navigate you among the most popular ones.# Must-have Skillset to Start a Data CareerBefore I start listing the highly demanded data professions, Iâ€™d also like to briefly describe the basic skill set required for these roles, which you should learn before sending your CV to potential employers.To cut a long story short, your checklist to enter the data industry should include Python, SQL, and Microsoft Excel. Trust me, youâ€™ll hear these buzzwords at every interview :)In addition, itâ€™s a good idea to supply your armory with Data Visualization and Data Cleaning skills in order to unlock the door to a greater data career.# Most Popular Data Roles: Whoâ€™s Who* **Data Analyst*****Key Focus***\*: Performs analysis of business data to find beneficial opportunities.\*In fact, no company can do without a Data Analyst, but their job titles usually vary from company to company. Depending on the industry specifics, you may come across such titles as ""Business Analyst"", ""Business Intelligence Analyst"", ""Healthcare Data Analyst"" and so on,  but most of them relate to similar functionality.As a rule, Data Analysts are engaged in collecting and analyzing data, as well as reporting outcomes to the companyâ€™s management in order to prioritize needs and target business strategies.* **Data Engineer*****Key Focus:*** *Optimizes the infrastructure supporting the data analytics workflow.*Data Engineers are responsible for building and testing optimal ecosystems that ensure worry-free data processing and the running of different algorithms. Every piece of technology goes out of date and needs regular upgrades, so Data Engineers make sure that the current version of the system or platform is the most efficient one.Apart from hands-on experience with programming languages such as Java, C++, and NoSQL, this position requires the ability to work with data APIs and ETL tools.* **Database Developer*****Key Focus:*** *Ensures the proper functioning of databases.*Since databases are subject to processing massive datasets and experiencing high loads, Database Developers provide the full cycle of DB maintenance, from modifying to backups and recoveries, in addition to designing and developing new databases.Additionally, the Database Developer workload includes â€‹â€‹ensuring that all new business projects meet the existing database standards, and creating IT documentation.* **Data Architect*****Key Focus:*** *Creates the guidelines for data management within the company.*The main goal of a Data Architect is to identify the end-use of the databases existing in the company. By writing detailed blueprints for all employees, Data Architects help to successfully coordinate the database integration, development, and testing as well as protect them according to the most contemporary security measures.The most in-demand Data Architects should possess in-depth expertise in database structure and requirements, data mining, and segmentation techniques.* **Data Scientist*****Key Focus:*** *Offers actionable business solutions and predictions through leveraging AI.*In fact, the majority of Data Scientists start their careers as Data Analysts. Speaking of the transition, the requirements that allow a Data Analyst to enter the data â€œIvy Leagueâ€ include mastering advanced programming skills and mathematics, as well as learning how to implement Machine Learning solutions.Data Scientists are expected to collect data in order to perform predictive analysis, even on unstructured (unlabeled) datasets. They detect patterns and trends, and provide data-driven insights that can improve the decision-making process within the company.* **Chief Data Officer (CDO)*****Key Focus:*** *Leads the data workflows across the enterprise.*By crafting data strategy and overseeing data management, CDOs ensure data quality and find ways of driving business processes in the right direction. As a CDO, you are engaged in establishing a â€œdata-drivenâ€ culture that streamlines data sharing among the employees and making informed decisions on how to get more satisfying business outcomes.# To Sum UpMore and more businesses nowadays increasingly recognize that they store treasure troves of data that, properly utilized, can be a great competitive advantage and bring value. As such, there has never been a better time to enter the data field because the demand for data specialists is skyrocketing, and organizations are willing to pay those who are able to convert data into a powerful business weapon handsomely.",POS
133,q3bz69,"""[Project]"" How to increase performance of LightGBM for ranking",https://www.reddit.com/r/MachineLearning/comments/q3bz69/project_how_to_increase_performance_of_lightgbm/,Initial_Tea,MachineLearning,1970-01-01 00:00:01.633621712,Project,0,0,"Can anyone share some advice on how to improve NDCG score for a learning to rank project using LightGBM. Currently working on a school project that requires a learning to rank functionality to rank documents per query, I have trained my model with the following parameters:    objective=""lambdarank"", metric=""ndcg"", to be used with LGBMRanker, Initially my NDCG scores were quite high, however by running the predicted ranking against a correct validation set from the teacher the NDCG score drops considerably (0.78 to 0.5). I tweaked my parameters to this to reduce overfitting and I've also run a series of F-score tests, mutual information tests and random forest importance from sklearn to select features. however my NDCG score is still quite low, I'm finding it difficult to predict the correct NDCG without overfitting and also to improve the accuracy of my model. current parameter I am using:    objective=""rank_xendcg"", metric=""ndcg"", max_bin = 63, learning_rate = 0.03, num_iterations = 100, num_leaves = 31, path_smooth = 50, lambda_l1 = 10, min_gain_to_split = 10 thanks in advance.",POS
134,q3bggv,[D] Graph Barlow Twins paper,https://www.reddit.com/r/MachineLearning/comments/q3bggv/d_graph_barlow_twins_paper/,Curious_Jellyfish616,MachineLearning,1970-01-01 00:00:01.633620206,Discussion,0,2,"Recently a paper has been submitted to iclr titled ""Graph Barlow Twins: A self-supervised representation learning framework for graphs""Link: [https://openreview.net/forum?id=MRGFutr0p5e](https://openreview.net/forum?id=MRGFutr0p5e&fbclid=IwAR1bnN8_U4cSizvLcCCV4TaEiU7kc7a09n1Cr1Qrkx1J0kwYYNN9N_mQP68) The authors claimed that they have proposed a SSL framework for graphs using the Barlow twins loss function. However, a similar paradigm has been proposed earlier and also compared with other SSL loss functions like VICReg in the work titled ""Graph Self Supervised Learning: the BT, the HSIC, and the VICReg""Link: [https://arxiv.org/abs/2105.12247](https://arxiv.org/abs/2105.12247)This work was accepted in the ijcai wsrl 2021 workshop earlier this year.Link: [https://wsl-workshop.github.io/ijcai21.html](https://wsl-workshop.github.io/ijcai21.html)The arXiv submission dates also confirms the above fact.  The authors didn't cite the ijcai wsrl paper either.",NEU
135,q3319a,[R] What is SOTA for link prediction on dynamic graphs with no node attributes?,https://www.reddit.com/r/MachineLearning/comments/q3319a/r_what_is_sota_for_link_prediction_on_dynamic/,miladink,MachineLearning,1970-01-01 00:00:01.633588152,Research,5,0,"I have been working on a project for predicting future links in a dynamic graph where nodes and edges have no attributes. While I trained a tgn from this repository [https://github.com/twitter-research/tgn](https://github.com/twitter-research/tgn)  I did not get good performance. I read [this survery(Representation Learning for Dynamic Graphs: A Survey)](https://arxiv.org/abs/1905.11485) but I am  still confused about the GNN models. I have organized my questions as:  1. My understanding is: graph convolutional neural nets are just appropriate when you have node or edge features. Is this correct? 2. I cannot understand what is the SOTA/ famous model on the link prediction task. There is the dyngraph2vec model but it seems outdated (paper is from 2019). Any pointers or any help?3. Finally, my question is if you were to predict links on a dynamic graph (but with no node or edge attributes) what model would you choose right now to iterate on? I thought it should be TGN, but my graph is discrete time and also, I guess it needs node and edge features?Thanks in advance a lot! Any comment is highly appreciated.",POS
136,q31m3z,[R] Geometric Transformers for Protein Interface Contact Prediction,https://www.reddit.com/r/MachineLearning/comments/q31m3z/r_geometric_transformers_for_protein_interface/,alexmorehead,MachineLearning,1970-01-01 00:00:01.633582239,Research,6,0,"I am excited to announce the release of DeepInteract, a geometric deep learning pipeline for predicting protein interface contacts. DeepInteract introduces the Geometric Transformer, a geometry-evolving graph transformer. Training and inference code (Docker and non-Docker versions) as well as pre-trained models available on GitHub.[https://arxiv.org/abs/2110.02423](https://arxiv.org/abs/2110.02423)[https://github.com/BioinfoMachineLearning/DeepInteract](https://github.com/BioinfoMachineLearning/DeepInteract)",POS
137,q2xqfe,[D] Can anyone that has paid for Cloud TPU help explain to me how it works?,https://www.reddit.com/r/MachineLearning/comments/q2xqfe/d_can_anyone_that_has_paid_for_cloud_tpu_help/,jakderrida,MachineLearning,1970-01-01 00:00:01.633568056,Discussion,13,6," I'm terribly sorry if this is the wrong subreddit, but this is just the subreddit I trust for these matters.Their website is here:[https://cloud.google.com/tpu/pricing](https://cloud.google.com/tpu/pricing)I've certainly used free TPU time using Google Colab before and (for the time they allowed it) saw results that far surpassed what I thought was possible for what I was doing. (using BERT to train a binary sentiment indicator for author-tagged articles about stocks)While I'm familiar with how to connect it to gcs and all, I'm always super hesitant to open up a subscription based cloud service ever since I started an Amazon Sagemaker environment and left it running, only to later find out I was being billed for it. (not that it was too expensive, but enough for me not to dive into TPU without knowing the ins and outs)I'm wondering if I'll need to pay close attention to whether I leave my Google Colab running with TPU as the processor to ensure it's both fully utilizing the TPU or whether they'd only bill me for the code which invokes it.Sorry if this is a terribly unstructured question, but any advice would be appreciated. I'm not super-rich, but the results from my free foray into TPU really opens up quite a few possibilities and so I'm definitely willing to spend money if I'm sure I don't screw up and get a huge bill next month.",POS
138,q2rjqb,[D] Understanding Design Principles in CNNs,https://www.reddit.com/r/MachineLearning/comments/q2rjqb/d_understanding_design_principles_in_cnns/,Cizox,MachineLearning,1970-01-01 00:00:01.633548315,Discussion,20,9,"Specifically in the context of image classification or object segmentation, how do DL researchers or data scientists know how to â€œbuildâ€ CNN architectures? Iâ€™m not talking about resource/budget constraints but rather the process on creating an architecture that works, rather than just throwing a bunch of residual blocks and hoping it works. What are some good practices that professionals use to test out various design ideas? How do you know what kernel size or stride to use? In what order to normalize/conv/relu, etc?I ask because in my deep learning class we were tasked with building a CNN for the PASCAL VOC dataset and it just felt like I was throwing stuff like Xception blocks and residual connections, scaling it by depth or width, and hoping it would perform well. I felt like there was a more holistic approach that professionals use to intelligently design an architecture. I guess Iâ€™m asking how do you intelligently hypothesize that some design will work?",POS
139,q2v23p,Efficiently Modeling Long Sequences with Structured State Spaces,https://openreview.net/pdf?id=uYLFoz1vlAC,argosopentech,MachineLearning,1970-01-01 00:00:01.633559131,,7,1,,NEU
140,q344pp,Notebook to Production [D],https://www.reddit.com/r/MachineLearning/comments/q344pp/notebook_to_production_d/,RepresentativeCod613,MachineLearning,1970-01-01 00:00:01.633593094,Discussion,0,15,"Hey [r/MachineLearning](https://www.reddit.com/r/MachineLearning/)! The best way to get data scientists' attention is by using the phrases '**Notebook**', '**Production**', and '**MLOps**'. This post has them **ALL**! ðŸ˜ˆIn recent years, MLOps has become a buzzword that attracted quite a bit of interest among data scientists. One of its practices is to help move from a research state to a 'production-ready' code.There are a lot of angles to this topic, and I'm trying to learn more about how the process of moving from developing code in 'Jupyter notebooks' to 'production-ready' code has changed in the past few years.I'd also love to hear from your experience - what are the best practices you follow, how does your team do it, and what tools are you using in this process?",POS
141,q30wh2,[P]Looking for datasets with both structured data and images for new model proof of concept.,https://www.reddit.com/r/MachineLearning/comments/q30wh2/plooking_for_datasets_with_both_structured_data/,Pseudoabdul,MachineLearning,1970-01-01 00:00:01.633579523,Project,2,0,"I recently worked on a project where I was predicting the likelihood of a car accident at intersections. The dataset contained information about that intersection (number of legs, speed limits, demographical data) as well as GPS coordinates. I fed the GPS coordinates into the Google Maps API to get images of each intersection. So the each row of the dataset contained a list of features, a response variable (number of crashes over 10 years) and the path of the associated image. To process this I built a new model that is able to process the features and images separately, and then combine them to predict a the number of crashes. I wasn't able to get good results on that dataset given my time constraints and the project ended up just using XGBoost. After the project I made a generic implementation of my model for all to use. Before I release it, I would like to validate it on some other datasets to prove it works. I've not been able to find any suitable datasets that have both images and structured features. If you can recommend some, I'll be able to test the model and then I can release it publicly. Thanks",POS
142,q277u6,[D] Feeling overwhelmed because applying machine learning to real life problems is not trivial,https://www.reddit.com/r/MachineLearning/comments/q277u6/d_feeling_overwhelmed_because_applying_machine/,petitponeyrose,MachineLearning,1970-01-01 00:00:01.633472853,Discussion,287,98,"Hello, I'm a young engineer and I was hired in a small company to automate some tasks using image recognition and machine learning. Even though the technology is not new, since I need to adapt it, I face a lot of challenges (gathering the right data, adapting the neural network, solving some other problems that neural nets can't solve on their own with traditional machine learning...) The current results I get are not good enough and I have to admit I feel overwhelmed.Is it normal? How is you experience? Have a nice day.",POS
143,q2vtte,Archive Text Reader [Project],https://www.reddit.com/r/MachineLearning/comments/q2vtte/archive_text_reader_project/,JohnClayborn,MachineLearning,1970-01-01 00:00:01.633561692,Project,3,3,"Hi all, I'm new to Machine Learning and I'm wondering if someone may have already made a tool for this, or if they might be interested in helping to develop one. I run an online military museum. We have partnered with 2 other online museums and are poring over tens of thousands of pages of data.   I thought about how AI might be able to serve two purposes - to be able to read the page and create a transcription of the image, and to examine scanned pages where the text is blurry or faded and provide a translation. Does anyone know if either of these might exist on GitHub or something already? Or would anyone be willing to help create such a tool? Thanks.",POS
144,q2m3v0,[R] Google Significantly Improves Visual Representations by Adding Explicit Information Compression,https://www.reddit.com/r/MachineLearning/comments/q2m3v0/r_google_significantly_improves_visual/,Yuqing7,MachineLearning,1970-01-01 00:00:01.633532053,Research,9,0,A Google Research team presents compressive variants of SimCLR and BYOL that yield better and more robust visual representations. Here is a quick read: [Google Significantly Improves Visual Representations by Adding Explicit Information Compression.](https://syncedreview.com/2021/10/06/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-118/)The paper *Compressive Visual Representations* is on [arXiv](https://arxiv.org/abs/2109.12909).,POS
145,q366z6,[P] Chief Technology Officer job post,https://www.reddit.com/r/MachineLearning/comments/q366z6/p_chief_technology_officer_job_post/,barbarac74,MachineLearning,1970-01-01 00:00:01.633602762,Project,0,3,"Hi everyone,My team is looking for a Chief Technology Officer for our client [Forecasty.AI](http://forecasty.ai/).[Forecasty.AI](http://forecasty.ai/) is a German-based corporate startup that empowers businesses through accurate AI-powered forecasts, and it is looking for a self-driven technology enthusiast who knows how to lead a team through the entire product development cycle successfully.This role requires:* Fluency in German and English* Ability to work at least 2 days a month from Mannheim* 5 years experience in AI/ML integration, API development, database design, DevOps, or other related areasYou can find more information on this opportunity here: [https://www.bundl.com/careers/chief-technology-officer](https://www.bundl.com/careers/chief-technology-officer)If you are interested, please fill in the online form on the job page. If you have any questions or would like further information, please donâ€™t hesitate to contact me. Iâ€™d be happy to help!",POS
146,q2qius,[R] Does anyone know of corpora for English speaking dialogue between humans and voice assistants?,https://www.reddit.com/r/MachineLearning/comments/q2qius/r_does_anyone_know_of_corpora_for_english/,MattyXarope,MachineLearning,1970-01-01 00:00:01.633545277,Research,3,0,I'm doing some research and it would really help! I've found the VACC paper (Voice Assistant Conversation Corpus) but that's in German. Thanks!,POS
147,q2ab5p,[D] How I Got a Job at DeepMind as a Research Engineer (without a Machine Learning Degree!),https://www.reddit.com/r/MachineLearning/comments/q2ab5p/d_how_i_got_a_job_at_deepmind_as_a_research/,hardmaru,MachineLearning,1970-01-01 00:00:01.633483932,Discusssion,65,47,"A blog post by [Aleksa GordiÄ‡](https://twitter.com/gordic_aleksa/status/1445375696298254339) about his progression from machine learning as a side passion to a professional RSWE role at DeepMind:blog: https://gordicaleksa.medium.com/how-i-got-a-job-at-deepmind-as-a-research-engineer-without-a-machine-learning-degree-1a45f2a781deI usually don't post career links since many readers of this sub would view them as ""beginners"" or ""introduction material"" (and complain), but I feel this post has sufficient depth that will add value to this sub, as there is many links to specific materials related to current ML research topics (such as GNNs), discussion about different ways that ML research can be presented, and how it relates to ML-research related career search.",POS
148,q2vk4z,[D] Ensembling: best regime for DNNs?,https://www.reddit.com/r/MachineLearning/comments/q2vk4z/d_ensembling_best_regime_for_dnns/,hypothesenulle,MachineLearning,1970-01-01 00:00:01.633560779,Discussion,0,4,"It used to be that papers on ensembles couldn't get published since they had an unfair theoretical advantage that overshadowed orthogonal contributions. Now they get published commonly.I do not aim to do research. I want to know a bit about the experience of people applying deep ensembles. Specifically:1- Whether ensembling is better started at the learning stage or inference stage (i.e. optimized together or optimized separately)?2- Whether each learner of ensemble should be given the full dataset and only differ on architecture/loss, or it is better to keep the loss the same and differ the data subsets shown during training?3- Best way to aggregate ensemble predictions (i.e. voting vs averaging).If anyone can share their experience or point me toward relevant literature I would really appreciate it.",POS
149,q2u15o,[P] Predicting missing words in consistent length sentences,https://www.reddit.com/r/MachineLearning/comments/q2u15o/p_predicting_missing_words_in_consistent_length/,semanticme,MachineLearning,1970-01-01 00:00:01.633555925,Project,0,4,"I have kind of a strange (and interesting) use case. I have ""sentences"" composed of seven axes. That is, the first ""word"" is a member of a list of values from axis 1, the second ""word"" is a member of the list of values from axis 2 and so on. Only the final axis can be blank (that is not, not every sentence needs a member of the seventh axis in position 7) - the rest are always present. This makes it behave like an NLP problem but in my case it is a \_very\_ regular sentence that is being parsed.What I want to do is predict ""missing"" words in the sentence. For example, if I give it\_\_\_\_ DOG ATE MY HOMEWORK WHICH SUCKEDIt might predict ""MINE"" or ""YOUR"" or anything else which has previously been seen in training data to likely coexist with the other words in the ""sentence"".Likewise, I'd like to give it something sparser likeMy FRIEND \_\_\_ \_\_\_ AND \_\_\_\_ \_\_\_\_and it would predict slots 3, 4, 6, 7 by finding the likeliest word for 3, then using that new input to predict 4, and so on.I know I am not looking for anything cutting edge here but don't know what this problem type is called or what options might exist for implementing an example in Python. I know I'll want embeddings and will need to have the model learn them, but how about the ""predicting missing pieces"" part?Any help would be super appreciated",POS
150,q2t95e,[D] Asymmetric encoder/decoder in VQVAE model.,https://www.reddit.com/r/MachineLearning/comments/q2t95e/d_asymmetric_encoderdecoder_in_vqvae_model/,chasep255,MachineLearning,1970-01-01 00:00:01.633553572,Discussion,0,2,Every example of a VQVAE model I have seen uses a symmetric encoder/decoder architecture.  To me it seems that it might make more sense to have a much more powerful decoder than encoder since much of the encoder information is lost in the quantization.  Is there any research exploring this?,POS
151,q2g2gc,[Discussion] Testing in ML,https://www.reddit.com/r/MachineLearning/comments/q2g2gc/discussion_testing_in_ml/,branislavhesko,MachineLearning,1970-01-01 00:00:01.633508256,Discussion,8,4,"Hello,I am currently working on a set of computer vision models, which should be general enough to be used on a variety of datasets. These models are ideally evolving, but it is problematic to ensure the performance improves. Therefore, I would like to start a discussion or better to ask what are your experience with testing in ML pipelines. I think that this field is somehow omitted in ML because of the random character of training and modeling.How do you test your training scripts?How do you test your models?Are unit-tests enough?Do you use some form of active learning or gradual improvement in production?",POS
152,q21hxi,[R] Incoming distinguished seminar on statistical machine learning - Emmanuel CandÃ¨s (Stanford),https://www.reddit.com/r/MachineLearning/comments/q21hxi/r_incoming_distinguished_seminar_on_statistical/,remymess,MachineLearning,1970-01-01 00:00:01.633455587,Research,76,13,"Hey there!I would like to bring the following event to the attention of those of you interested in what machine learning researchers and academic experts are currently talking about and working on. This week, the **Harvard IACS Seminar Series** is hosting [Emmanuel CandÃ¨s](https://statweb.stanford.edu/~candes/) (Statistics Professor at Stanford) who will present several statistical ideas to quantify accuracy, fairness, and reliability when using machine learning models.Curious to hear more about it? Visit the following event page and see you this Wednesday 6th of October at 18:00CET! [https://mora.stream/event/1055](https://mora.stream/event/1055)NB: For a full list of upcoming talks (as well as the recording of the previous ones), visit the following agora page! [https://mora.stream/Harvard%20IACS%20Seminar%20Series](https://mora.stream/Harvard%20IACS%20Seminar%20Series)",POS
153,q2p0f6,[R] How do you find a Research Topic that interests you?,https://www.reddit.com/r/MachineLearning/comments/q2p0f6/r_how_do_you_find_a_research_topic_that_interests/,jaygshah22,MachineLearning,1970-01-01 00:00:01.633540690,Research,0,0,"Tips from leading researchers at **Google**, **Facebook**, and **Harvard** on deciding and finding a good research topic that interests you and is also relevant to work on as a researcher/PhD student in Machine Learning research. Link: [https://youtu.be/Z93WdX5XbAo](https://youtu.be/Z93WdX5XbAo)",POS
154,q2ozs2,"[N] DeepMind UNSTOPPABLE: 1 day after protein complexes, attacks gene expression from DNA [R]",https://www.reddit.com/r/MachineLearning/comments/q2ozs2/n_deepmind_unstoppable_1_day_after_protein/,abbumm,MachineLearning,1970-01-01 00:00:01.633540638,News,0,0,https://deepmind.com/blog/article/enformer,NEU
155,q29u6o,[R] Mining for strong gravitational lenses with self-supervised learning,https://arxiv.org/abs/2110.00023,hardmaru,MachineLearning,1970-01-01 00:00:01.633481951,Research,9,2,,NEU
156,q29tlt,[R] Autoregressive Diffusion Models,https://arxiv.org/abs/2110.02037,hardmaru,MachineLearning,1970-01-01 00:00:01.633481893,Research,7,1,,NEU
157,q2p3qn,[D] Best Data Cleaning Model or Process,https://www.reddit.com/r/MachineLearning/comments/q2p3qn/d_best_data_cleaning_model_or_process/,Gyllenspetz,MachineLearning,1970-01-01 00:00:01.633540952,Discussion,0,1," Hello everyone,Happy wonderful Wednesday! I wanted to quickly ask the community about their favorite cleaning model or process. Prior to running analysis, as we all know very well the data gathering phase will always result in a ton of noise, how do you reduce this in the quickest and most accurate fashion?\- Do you build a pipe of specific cleaning stages (dedup, irrelevant language, terms used, normalize, remove stop words, lemmatize etc.)\- Have you built a model to remove posts and clean the data? How did you trained said model? How big was your training dataset? What steps did you take to validate or verify it's quality?\- Other processes?I appreciate any and all comments, have an awesome day!All the best,N",POS
158,q29ths,Using adversarial attacks to refine molecular energy predictions,https://news.mit.edu/2021/using-adversarial-attacks-refine-molecular-energy-predictions-0901,FindLight2017,MachineLearning,1970-01-01 00:00:01.633481881,,6,0,,NEU
159,q2cvr3,[D] The second part of the guide to optimization of complex variables is out!,https://www.reddit.com/r/MachineLearning/comments/q2cvr3/d_the_second_part_of_the_guide_to_optimization_of/,Megixist,MachineLearning,1970-01-01 00:00:01.633493738,Discussion,2,0,"This part of the two-article series featured on Weights and Biases explores the convergence constraints for complex-valued neural networks while distinguishing the optimization factors and circularity constraints for strictly linear and widely linear networks. The report delves into the significance of phase, the impact of weight initialization methods, and the holomorphicity requirement for activations while demonstrating image denoising using custom complex-valued convolutions with Tensorflow.The principal objective of this series of articles is to display the versatility of complex representations and to encourage their use for breakthrough research.Link: [https://wandb.ai/darshandeshpande/complex-optimization/reports/The-Reality-Behind-Optimization-of-Imaginary-Variables-II--Vmlldzo5OTM5NTA](https://wandb.ai/darshandeshpande/complex-optimization/reports/The-Reality-Behind-Optimization-of-Imaginary-Variables-II--Vmlldzo5OTM5NTA)Any questions or comments are most welcome! :)",POS
160,q2176g,[P] Albumentations 1.1 is released (a Python library for image augmentation),https://www.reddit.com/r/MachineLearning/comments/q2176g/p_albumentations_11_is_released_a_python_library/,alexparinov,MachineLearning,1970-01-01 00:00:01.633454708,Project,25,4,"The new release of a fast and flexible library for image augmentation includes:# New augmentations* **TemplateTransform** allows the blending of an input image with specified templates.&#x200B;https://preview.redd.it/9qzcv15h1or71.png?width=1500&format=png&auto=webp&s=c460dc6a0d5ef147306cc6596c5ab5ad8e416083* **PixelDistributionAdaptation**. A domain adaptation augmentation. An example of this augmentation is available on [GitHub](https://github.com/arsenyinfo/qudida#example).# Improvements and bug fixesFour augmentations got new parameters for better tunning. ElasticTransform got a performance boost. Also, we fixed bugs for augmentations that work with bounding boxes and keypoints.# Release notesFull release notes are available at [https://github.com/albumentations-team/albumentations/releases/tag/1.1.0](https://github.com/albumentations-team/albumentations/releases/tag/1.1.0)# InstallationAs always, you can install the latest version of the library by running:    pip install -U albumentations",POS
161,q1xne7,[R] DeepMind's FIRE PBT: Automated Hyperparameter Tuning With Faster Model Training and Better Final Performance,https://www.reddit.com/r/MachineLearning/comments/q1xne7/r_deepminds_fire_pbt_automated_hyperparameter/,Yuqing7,MachineLearning,1970-01-01 00:00:01.633444476,Research,28,4,"A DeepMind research team proposes Faster Improvement Rate PBT (FIRE PBT) for Population Based Training (PBT), an automated hyperparameter tuning method for neural network training. The novel approach achieves faster improvement rates and better long-term performance. Here is a quick read: [DeepMind's FIRE PBT: Automated Hyperparameter Tuning With Faster Model Training and Better Final Performance.](https://syncedreview.com/2021/10/05/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-117/)The paper *Faster Improvement Rate Population Based Training* is on [arXiv](https://arxiv.org/abs/2109.13800).",POS
162,q263ki,[D] Artificial Intelligence: Why the Thoughts and Ideas framed in the early stages are often the most Inspiring,https://www.reddit.com/r/MachineLearning/comments/q263ki/d_artificial_intelligence_why_the_thoughts_and/,Philo167,MachineLearning,1970-01-01 00:00:01.633469441,Discussion,6,3,"Many people think that if an article or paper in AI or machine learning is older than three years, it is not worth reading. But this assumption is wrong, and you run the risk of increasing your knowledge with essential gaps. Of course, some papers refer to the first early works, but often this is done superficially to more or less justify the authorâ€™s point of view. I have listed in this article some thoughts & resources (a mixture of historical, philosophical, and technical) to consider as you begin your â€˜careerâ€™ for a basic understanding of AI:  [Why the Thoughts and Ideas framed in the early stages are often the most Inspiring](https://medium.com/nerd-for-tech/artificial-intelligence-why-the-thoughts-and-ideas-framed-in-the-early-stages-are-often-the-most-561b047bc99c)",NEG
163,q254qp,[R] Google DeepMind attacks protein complexes folding in view of CASP-15 [N],https://www.reddit.com/r/MachineLearning/comments/q254qp/r_google_deepmind_attacks_protein_complexes/,abbumm,MachineLearning,1970-01-01 00:00:01.633466547,News,8,0,https://deepmind.com/research/publications/2021/protein-complex-prediction-with-alphafold-multimer,NEU
164,q28a27,[P] New version of CogView (text-to-image) is available in online demo,https://www.reddit.com/r/MachineLearning/comments/q28a27/p_new_version_of_cogview_texttoimage_is_available/,Wiskkey,MachineLearning,1970-01-01 00:00:01.633476390,Project,5,0,[Online demo of new version of CogView](https://agc.platform.baai.ac.cn/CogView/index.html).[Examples #1](https://www.reddit.com/r/bigsleep/comments/q238w2/fox_at_night_2_images_made_using_the_new_cogview/). [Examples #2](https://www.reddit.com/r/bigsleep/comments/q25rj5/beautiful_woman_4_images_using_the_new_cogview/).[GitHub repo for older version of CogView](https://github.com/THUDM/CogView). [GitHub mention of new version](https://github.com/THUDM/CogView/issues/34).[18 examples using older version of CogView](https://www.reddit.com/r/bigsleep/comments/oq5nwh/18_images_generated_by_free_cogview_website_using/).,NEU
165,q1os79,"[P] KotlinDL 0.3 Is Out With ONNX Integration, Object Detection API, 20+ New Models in ModelHub, and Many New Layers",https://www.reddit.com/r/MachineLearning/comments/q1os79/p_kotlindl_03_is_out_with_onnx_integration_object/,NetHairy4282,MachineLearning,1970-01-01 00:00:01.633414233,Project,70,2," Introducing version 0.3 of our deep learning library, [KotlinDL](https://github.com/JetBrains/KotlinDL).Read the release post on [Medium](https://zaleslaw.medium.com/kotlindl-0-3-43fffd3cb94) or in [JetBrains Blog](https://blog.jetbrains.com/kotlin/2021/09/kotlindl-0-3-is-out-with-onnx-integration-object-detection-api-20-new-models-in-modelhub-and-many-new-layers/).KotlinDL 0.3 is available now on [Maven Central](https://search.maven.org/artifact/org.jetbrains.kotlinx/kotlin-deeplearning-api) with a variety of new features â€” check out all the [changes](https://github.com/JetBrains/KotlinDL/blob/master/CHANGELOG.md) that are coming to the new release!We're currently introducing a lot of new models in ModelHub (including the first Object Detection and Face Alignment models), the ability to fine-tune the Image Recognition models saved in ONNX format from Keras and PyTorch, the experimental high-level Kotlin API for image recognition, a lot of new layers contributed by the community members and many other changes.https://i.redd.it/bvnlemtkpkr71.gif If you have any feature requests, please create an issue or write a question in the Discussion chapter.KotlinDL is built on top of the TensorFlow Java API and ONNX Runtime Java API and has an API close to Keras and other high-level frameworks like Sonnet, PyTorch Lighting, and Catalyst.Give a star on Github to KotlinDL if you support this project, run tutorials, taste the Kotlin with Deep Learning!",POS
166,q2a75v,[R] Exploring the Limits of Large Scale Pre-training,https://arxiv.org/abs/2110.02095,koolaidman123,MachineLearning,1970-01-01 00:00:01.633483404,Research,2,1,,NEU
167,q1z4hg,[D] SOTA GAN-based Image Editing - ISF-GAN: An Implicit Style Function for High-Resolution Image-to-Image Translation (5-minute explanation),https://www.reddit.com/r/MachineLearning/comments/q1z4hg/d_sota_ganbased_image_editing_isfgan_an_implicit/,KirillTheMunchKing,MachineLearning,1970-01-01 00:00:01.633448661,Discussion,11,11,I   often find myself wishing I knew how to edit images in photoshop but I    remember that I already have a full-time job without attempting to learn photoshop. This is where ISF-GAN by Yahui Liu et al. comes in.    This new model performs cost-effective multi-modal unsupervised image-to-image translations at high resolution using pre-trained unconditional GANs. ISF-GAN does this by modeling the latent style vector update with an MLP conditioned on a random vector and an attribute code.Check out the [full paper summary](https://www.casualganpapers.com/unsupervised_latent_space_exploration_image_to_image_gan_editing/ISF-GAN-explained.html) on Casual GAN Papers (Reading time \~5 minutes).\[[arxiv](https://arxiv.org/pdf/2109.12492.pdf)\]\[[github](https://github.com/yhlleo/stylegan-mmuit)\][ISF-GAN](https://preview.redd.it/d35r1duwjnr71.png?width=1436&format=png&auto=webp&s=99beaf2e2e11f1d447b62a3ddcb4cf9273fe2693)Subscribe to [my channel](https://t.me/casual_gan) and follow me on [Twitter](https://twitter.com/KirillDemochkin) for weekly AI paper summaries!,POS
168,q27jvs,[D] Difference between meta learning and few-shot learning,https://www.reddit.com/r/MachineLearning/comments/q27jvs/d_difference_between_meta_learning_and_fewshot/,human_treadstone,MachineLearning,1970-01-01 00:00:01.633473938,Discussion,2,4,"I have came across meta and few shot learning. It looks like they are same, I might be wrong. So what is the main differentiating factor between these two. In case, few-shot learning is a subset of meta-learning then  which part of meta-learning does not concern few shot learning.",NEG
169,q2ad6c,"[D] DNN options for multivariate, ragged tensor, time series forecast?",https://www.reddit.com/r/MachineLearning/comments/q2ad6c/d_dnn_options_for_multivariate_ragged_tensor_time/,e1gord0,MachineLearning,1970-01-01 00:00:01.633484203,Discussion,0,0,"Hypothetically, if you had a tabular dataset structured similar to this:Â [table example](https://i.stack.imgur.com/rNkjB.png)Where you had peoples information spread out over different time periods, different addresses, and different stores, as well as about 10-15 other columns (INPUT...), and are trying to predict 3 different outcomes atÂ a future time pointÂ (e.g., Q1, 2022)Â for each individual person,How would you strucuture a Deep Neural Network (e.g., tensorflow [but I'm open to better solutions!]) to handle that future prediction without pivoting time out into a bunch of new columns?(When I pivot the data to make new year/quarter columns it makes for terrible model explainability (e.g., why isÂ INPUT12Â a positive effect in one year and negative in another.))",POS
170,q20g5i,"[R] Google AI 0pen Sources â€˜FedJAXâ€™, A JAX-based Python Library for Federated Learning Simulations",https://www.reddit.com/r/MachineLearning/comments/q20g5i/r_google_ai_0pen_sources_fedjax_a_jaxbased_python/,techsucker,MachineLearning,1970-01-01 00:00:01.633452495,Research,5,3,"Federated learning is a machine learning environment in which multiple clients (such as mobile devices or entire enterprises, depending on the task at hand) collaborate to train a model under the supervision of a central server. The training data, however, remains decentralized.Â Because of the increased attention on privacy and security, federated learning has become a particularly important research topic. In such a fast-paced sector, itâ€™s critical to swiftly translate ideas into code, iterate quickly, and compare and duplicate existing baselines.A new google study introduces [FedJAX](https://github.com/google/fedjax), a [JAX](https://github.com/google/jax)\-based open-source library for federated learning simulations that emphasizes ease-of-use in research. FedJAX intends to construct and assess federated algorithms faster and easier for academics by providing basic building blocks for implementing federated algorithms, preloaded datasets, models, and algorithms, and fast simulation speed.# [Quick 3 Min Read](https://www.marktechpost.com/2021/10/05/google-ai-0pen-sources-fedjax-a-jax-based-python-library-for-federated-learning-simulations/) | [Research Paper](https://arxiv.org/pdf/2108.02117.pdf) | [Github](https://github.com/google/fedjax) | [Google Blog](https://ai.googleblog.com/2021/10/fedjax-federated-learning-simulation.html)&#x200B;https://preview.redd.it/g33gv1idvnr71.png?width=654&format=png&auto=webp&s=c721db252806096af190acf71fe84969f150336f",POS
171,q293wz,[D]Looking for Reviews and analysis of ML vendors for a cloud DAM,https://www.reddit.com/r/MachineLearning/comments/q293wz/dlooking_for_reviews_and_analysis_of_ml_vendors/,icurate,MachineLearning,1970-01-01 00:00:01.633479298,Discussion,1,0,"We have the opportunity to customize a DAM and choose which ML engine we want to use with it for images and video. The main component we need is facial recognition. This would be people we identify once and then it finds the other instances of that face and auto tags them. We are also wanting more basic things like identifying a tie and what color it is. I found an in-depth comparison on [kdnuggets.com](https://kdnuggets.com), but it is 4 years old and I am looking for something more current. I know very little about this field and would like to be able to make the best decision for our needs.",POS
172,q1yrbx,[D] Multilingual Parallel dataset for Machine Translation,https://www.reddit.com/r/MachineLearning/comments/q1yrbx/d_multilingual_parallel_dataset_for_machine/,amruh,MachineLearning,1970-01-01 00:00:01.633447600,Discussion,7,3,"Hi, I've been searching a multilingual parallel MT dataset for English, German and French. Any suggestions? The only multilingual parallel data I could find was IWSLT but it doesn't contain the triplet I'm looking for.",NEU
173,q27h6w,[D] Implement speaker identification module,https://www.reddit.com/r/MachineLearning/comments/q27h6w/d_implement_speaker_identification_module/,hpk_platinium,MachineLearning,1970-01-01 00:00:01.633473699,Discussion,0,0,"Hi everyone,I am seeking to create a speaker identification module, which should work on settings of companies with 30+ employees. The idea is that most of the users will be registered beforehand (by using having at least 20 seconds effective speech for each user) and then during a meeting with multiple people, I want to be able to recognize who is speaking in a given utterance.My questions are the following:* What is the state of the art paper with best results so far? As far as I know it was UIS-RNN for google, but did something change there?* What approach would you use for implementing this? e.g. libraries, prototype notebooks?* Relating to the question above: I came across to speechbrain ([https://github.com/speechbrain/speechbrain](https://github.com/speechbrain/speechbrain)). Has any of you used the speaker recognition module from them? What about the usage in production environment in terms of latency?&#x200B;Thank you very much in advance",POS
174,q26dxr,German dialogue dataset [Research],https://www.reddit.com/r/MachineLearning/comments/q26dxr/german_dialogue_dataset_research/,HiImph1l,MachineLearning,1970-01-01 00:00:01.633470340,Research,1,0,"Hi, I'm desperatly searching for a german dialogue dataset like Cornell Movie Dialogue-Corpus. But I can't find anything in the internet (even visited google page 3). So you guys are pretty much my last hope.",POS
175,q1wf44,[R] DOLG: Single-Stage Image Retrieval with Deep Orthogonal Fusion of Local and Global Features,https://www.reddit.com/r/MachineLearning/comments/q1wf44/r_dolg_singlestage_image_retrieval_with_deep/,Accomplished-Ad-2156,MachineLearning,1970-01-01 00:00:01.633440591,Research,4,2,Local information with multi-atrous convolutions and self-attention with orthogonal components concatenated and aggregated with the global representation to generate the final representation. The paper shows state-of-the-art image retrieval performance on the Revisited Oxford and Paris datasets.Paper Link : [https://arxiv.org/abs/2108.02927](https://arxiv.org/abs/2108.02927)Unofficial Code : [https://github.com/dongkyuk/DOLG-pytorch](https://github.com/dongkyuk/DOLG-pytorch),NEU
176,q1wham,[D] How does your data infrastructure looks like?,https://www.reddit.com/r/MachineLearning/comments/q1wham/d_how_does_your_data_infrastructure_looks_like/,latentlatent,MachineLearning,1970-01-01 00:00:01.633440793,Discussion,3,5,"In my startup company we are struggling to find good ways for storing data. I would like to learn more about how it is done usually/in general so we can start to build our own system.Right now we just put everything in MySQL databases, which is not optimal at all. We have tabular data and text data. The volume is super big, tens of millions of records every day.Now is the time to brag if you've got a great system!(I also google around, but have not found any good resources. If I missed something don't hesitate and send it to me.)EDIT: **I meant ""architecture"" not ""infrastructure""**",POS
177,q23ns6,[P] L1-Penalty vs RFECV,https://www.reddit.com/r/MachineLearning/comments/q23ns6/p_l1penalty_vs_rfecv/,viniltummala,MachineLearning,1970-01-01 00:00:01.633462176,Project,0,0,"I have a problem statement, in which i have to perform feature selection. It has included that i need to use L1-penalty with logistic regression and perform cross validation as well.When i do only L1-penalty a few features coefficients are zero, however, when i use RFECV with L1-Logistic regression many more features are turning out to be pruned. Which is better and why?",POS
178,q1y9pm,"[D] Why would this loss function for contrastive learning have better classifier accuracy, but worse embeddings compared to cross entropy?",https://www.reddit.com/r/MachineLearning/comments/q1y9pm/d_why_would_this_loss_function_for_contrastive/,jmbrown777,MachineLearning,1970-01-01 00:00:01.633446512,Discussion,2,5,"Supervised Contrastive Loss:https://github.com/HobbitLong/SupContrastGoogle blog on it: https://ai.googleblog.com/2021/06/extending-contrastive-learning-to.htmlAs shown in the github, they claim it gets higher accuracy compared to Cross Entropy. But if you look at the t-sne visualizations from the github, the embeddings of SupCon look noticeably worse. Why would this be?",NEG
179,q1ndgp,[P] Examples of ML cloud environments,https://www.reddit.com/r/MachineLearning/comments/q1ndgp/p_examples_of_ml_cloud_environments/,fella85,MachineLearning,1970-01-01 00:00:01.633408124,Project,13,7,"Hi Everyone,Would anyone know of examples of cloud environments (AWS or Azure) for development of ML algorithms and performing statistical analysis (ie. running Stan, etc) ?For example, is there something more sophisticated than just creating a docker image of your favourite tools and spinning a VM with it?I would be interested in different architectures, etc.Cheers,Fella85Edit 1: I'm just going to add a bit more context. I'm trying to see whether our current team of 6 can swap their laptops that they use for ML work for a cloud VM or service.  They are an eclectic bunch of people.. some use RStudio, Python Jupiter notebooks or PyCharm.   Perhaps it is worthwhile to give Azure ML studio or AWS SageMaker studio a try and see whether the team can use them. Most probably there will be a learning curve that will need to get over it. But the most important thing is that the environment does not limit what they can do. ",POS
180,q1wi5g,[P] Jupiter Notebook templating for PyCharm,https://www.reddit.com/r/MachineLearning/comments/q1wi5g/p_jupiter_notebook_templating_for_pycharm/,forforf,MachineLearning,1970-01-01 00:00:01.633440873,Project,2,0,"[Templating Source Code](https://github.com/forforf/pycharm-jupyter-templates)  Feel free to jump to the **tldr** at the bottom.I was in need of a Jupyter Notebook templating engine and I couldnâ€™t find anything that met my particular needs. So I threw one together and I'm posting it here in case anyone else might find it useful. Usage and examples are in the source code link above.Since I use PyCharm to edit and run my notebooks, the engine is set up to work with how PyCharm uses cell metadata. It shouldnâ€™t be too hard to extend this for non-PyCharm notebooks, editing the cell metadata directly might be enough (standard Jupyter editor allows this, albeit its a bit clunky), or feel free to fork the project to create your own custom parser.Even though itâ€™s Â pretty basic (the core logic is only \~100 lines of code), it worked well for my use case which included chaining templates (i.e. templates generating other templates).  A few final notes. This is not an announcement of a new package/library, itâ€™s just code I put together for a problem that I couldnâ€™t ""google"" myself out of. As such, the code isnâ€™t what I consider high quality, but it does the job for me.Other things I looked into that didnâ€™t quite do what I wanted:* [jupytemplate](https://github.com/xtreamsrl/jupytemplate): This required installing an jupyter notebook extension and didnâ€™t support chaining templates (as far as I could tell).* [jupytext](https://jupytext.readthedocs.io/en/latest/index.html): Although having the notebook paired to a normal python file has advantages, and I could probably change the templating into the regular python domain, it would create more overhead than benefit for my use case.If there's any other templating solutions out there I'd be interested in taking a look as well.**tldr**: Github repo of code I threw together to solve a templating need that might be useful to others facing a similar need.Â ",POS
181,q1zrqp,"Understanding Output from Glow, what exactly do the outputs mean ? [D]",https://www.reddit.com/r/MachineLearning/comments/q1zrqp/understanding_output_from_glow_what_exactly_do/,SuitDistinct,MachineLearning,1970-01-01 00:00:01.633450491,Discussion,0,3,"I've been going through [https://github.com/rosinality/glow-pytorch](https://github.com/rosinality/glow-pytorch) to learn more about normalising flows. Even after reading through the code, I am still a little unsure of what exactly what is the output of the Glow class. This is in the [model.py](https://model.py/) file. When going through forward, it returns (log\_p\_sum,logdet,z\_outs). I get that logdet is the log determinant which shows the scaling factor for the bijective function. What I don't get is what does log\_p\_sum and z\_outs stand for ?",NEG
182,q1v2lt,[D] What do you think is the SOTA counterfactual explanation method and the evaluation metric of it?,https://www.reddit.com/r/MachineLearning/comments/q1v2lt/d_what_do_you_think_is_the_sota_counterfactual/,bono_kr,MachineLearning,1970-01-01 00:00:01.633435835,Discussion,2,4,"* I visited the following site and found that there is no evaluation metric, therefore it seemed there is no way to evaluate which is the best method to generate a counterfactual explanation.* [https://paperswithcode.com/task/counterfactual-explanation](https://paperswithcode.com/task/counterfactual-explanation)* I would appreciate it if you share your opinion on this. :)",POS
183,q13ymu,[P] Open-source python library for assessing Data Quality,https://www.reddit.com/r/MachineLearning/comments/q13ymu/p_opensource_python_library_for_assessing_data/,YData_umb,MachineLearning,1970-01-01 00:00:01.633348651,Project,125,15,"Hi r/MachineLearning community! We at YData created an open-source project regarding data quality ( [https://github.com/ydataai/ydata-quality](https://github.com/ydataai/ydata-quality) ) and wanted to share it with you all!As cleaning data is time-consuming and kind of boring we built a Data Quality engine that identifies data quality issues and flags them based on expected impact in a few lines of code. This let us start fixing the data issues much faster and earlier in the machine learning pipelines. Again, this is open-sourced so if you want to contribute and give some feedback it is very much appreciated :)Check out some of the tutorial notebooks to get started: [https://github.com/ydataai/ydata-quality/tree/master/tutorials](https://github.com/ydataai/ydata-quality/tree/master/tutorials)If you've got any ideas or you want to discuss the implementations, feel free to hangout in our friendly slack community at [https://slack.ydata.ai](https://slack.ydata.ai/) !",POS
184,q1vpud,[D] Precisely count the number of operations in Keras model,https://www.reddit.com/r/MachineLearning/comments/q1vpud/d_precisely_count_the_number_of_operations_in/,Hot-Painter4924,MachineLearning,1970-01-01 00:00:01.633438249,Discussion,0,2,"I'm working in a tool to predict the inference speed of a model, based on input shape and model definition, i want to give the inference speed in different platforms. To this, i already measured the inference speed of a wide range of models in different platforms.I believe that this speed is a direct product of number of operations inside model (not only FLOPS), and i'm skeptical that even tf.profiler gives a very precise count of these operations.&#x200B;Eg: sigmoid is defined by:1/(1+exp(-x))&#x200B;I dive deep into tensorflow to find this implementation and found out that actually it uses a wrapper of Eigen. At the end of the day, as there is no built in implementation of exponential function in c++, it is done by a Taylor series approximation, defined as:&#x200B;e(x) = 1 + x + (x\*x)/2 + (x\*x\*x)/6 + (x\*x\*x\*x)/24 ...&#x200B;There is a number of other multiplications and sums that are not even implemented in tensorflow, so how could it count them?So, how could i count all operations that a keras model do? can i rely on tf.profiler.ProfileOptionBuilder.float\_operation() ?",NEG
185,q155fd,[P] Comparison for all Sklearn Classifiers,https://www.reddit.com/r/MachineLearning/comments/q155fd/p_comparison_for_all_sklearn_classifiers/,mingaflo,MachineLearning,1970-01-01 00:00:01.633353093,Project,38,35,Hi everyone! Today I was trying out different classifiers for a multi-label task and wanted to get a baseline using different sklearn classifiers. While I was programming and waiting for the program to finish I wondered if there is a GitHub repo that has implemented all the classifiers with evaluation metrics / graphics and so on. I searched for a bit but I have not found any yet. I know some classifiers are task-dependent and the input varies as well but generally I would say that it is possible to implement a script that tests all the classifiers and returns the best 3 with their evaluations. One would get a good baseline for the task at hand and could fine-tune the classifiers if he wanted to. If you know of any implementations I would appreciate it. I might implement it myself for future projects because it would be really helpful to have a comparison and baseline  between a custom neural network / transformer and the best sklearn algorithms for the tasks. (I will share the repository here when I am done.),POS
186,q125xp,[R] LIVECell - A large-scale dataset for label-free live cell segmentation,https://www.reddit.com/r/MachineLearning/comments/q125xp/r_livecell_a_largescale_dataset_for_labelfree/,rickardsjogren,MachineLearning,1970-01-01 00:00:01.633340541,Research,52,0,"We are a research team at [Sartorius](https://www.sartorius.com/en) and [DFKI](https://www.dfki.de/en/web/) Kaiserslautern who has recently open-sourced [LIVECell](https://sartorius-research.github.io/LIVECell/) ([paper](https://www.nature.com/articles/s41592-021-01249-6)), the largest manually annotated dataset for cell segmentation of microscopic images available to date.[Example image with annotations of a neuronal cell type included in LIVECell.](https://preview.redd.it/zvrkhfv24er71.png?width=1111&format=png&auto=webp&s=78885a28b024e47993b8788f20ebb3cb645a5f4e)**LIVECell (Label-free In Vitro image Examples of Cells)** consists of 5,239 manually annotated, expert-validated, Incucyte HD phase-contrast microscopy images with a total of 1,686,352 individual cells annotated from eight different morphologically distinct cell types (average 313 objects per image). Our hopes are that LIVECell will provide a valuable resource to the cell biology-community as well as an interesting benchmark for the machine learning-community. LIVECell is published under the CC-BY-NC 4.0-license making it freely available for non-commercial purposes.**Why should we care about cell segmentation?** Microscopic images of two-dimensional cell cultures like the ones in LIVECell are extremely commonly used in cell biology-research making them important tools during development of new medicines. Microscopic imaging is cheap and accessible and provides high detail in both space and time, even at the individual cell level. Cell instance segmentation can however be difficult due to low contrast, weird object shapes and high object density. So, we need high-performing instance segmentation-models and datasets to train them to help researchers use microscopy to find new medicines in a faster and cheaper way.**Some things we learned from a machine learning-perspective** (although we have not done any systematic ML-study yet):* The two models we trained on LIVECell achieved similar mask AP to what they achieved on MS COCO, which indicates that the two datasets are somewhat similar in overall difficulty.* Our two models achieve similar mask AP but differ in terms of average false negative ratio (Fig 3). We have not yet investigated the cause of this difference.* The different cell types morphologies vary in difficulty but neuronal-like cell types with their highly non-convex morphologies (image in post) are really tricky to segment well using standard instance segmentation models.* Due to the large number of objects per image (average \~300, up to 3000+), LIVECell poses an interesting challenge for computational efficient instance segmentation models (Fig S11).* We also stress test our models on even higher object densities, finding that our best model in terms of mask AP collapses whereas the other model extrapolates a fair bit (Fig 4). Our hypothesis is that this difference may be due to the different prediction mechanisms (anchor-based/free).* Although all images in LIVECell are from the same instrument, models trained on it transfer very well to others given appropriate image preprocessing (Fig S12-13). We even establish new state-of-the-art on a previously published cell segmentation-dataset without training on its images (Fig S12). **Links*** Link to dataset: [https://sartorius-research.github.io/LIVECell/](https://sartorius-research.github.io/LIVECell/)* Open access paper: [https://www.nature.com/articles/s41592-021-01249-6](https://www.nature.com/articles/s41592-021-01249-6)* Papers with code: [https://paperswithcode.com/dataset/livecell](https://paperswithcode.com/dataset/livecell)* Github: [https://github.com/sartorius-research/LIVECell](https://github.com/sartorius-research/LIVECell)",POS
187,q0vt2b,"[R] ResNet strikes back: An improved training procedure in timm. There has been significant progress on best practices for training neural nets since ResNet's introduction in 2015. With such advances, a vanilla ResNet-50 reaches 80.4% top-1 accuracy on ImageNet without extra data or distillation.",https://arxiv.org/abs/2110.00476,hardmaru,MachineLearning,1970-01-01 00:00:01.633312251,Research,204,46,,NEU
188,q1s1f3,[D] At what point does overparameterization hurts a model?,https://www.reddit.com/r/MachineLearning/comments/q1s1f3/d_at_what_point_does_overparameterization_hurts_a/,l34df4rm3r,MachineLearning,1970-01-01 00:00:01.633421986,Discussion,0,6,"Overparameterized NNs can generalize better.  [sciChina20over.pdf (nju.edu.cn)](https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/sciChina20over.pdf) ,  [Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers (neurips.cc)](https://proceedings.neurips.cc/paper/2019/file/62dad6e273d32235ae02b7d321578ee8-Paper.pdf)  that overparameterized models do not overfit and generalize better. There are also a lot of other works that show how overparameterized models are better at generalization. However, all that comes with increased computational cost, right?I am doing a simple node classification task using graph neural networks. Here's a gist: it is a simple binary classification task and say, I have 2 NNs: NN\_A with 200 params, NN\_B with 400 params and if after x training iterations, NN\_A has better classification accuracy than NN\_B, then should I say that NN\_A performs better? Let's say NN\_B has lower training loss than NN\_A. Can I say that in this case, overparameterization is hurting the classification performance on NNN\_B?",POS
189,q0wzzh,[D] The Great AI Reckoning: Deep learning has built a brave new worldâ€”but now the cracks are showing. IEEE Spectrum Magazine's Special Issue devoted to AI.,https://spectrum.ieee.org/special-reports/the-great-ai-reckoning/,hardmaru,MachineLearning,1970-01-01 00:00:01.633316623,Discusssion,76,19,,NEU
190,q1ib9i,[D] To NLP/ML conference chairs: Are the people who are responsible to distribute the anonymized papers between reviewers allowed to see the identity of the paper authors?,https://www.reddit.com/r/MachineLearning/comments/q1ib9i/d_to_nlpml_conference_chairs_are_the_people_who/,sim_inf,MachineLearning,1970-01-01 00:00:01.633390570,Discussion,0,11,"How does the assignment of papers to reviewers work in top tier conferences? Are people who assign papers to reviewers allowed to see the identity of paper authors?If you are an unknown author, what are the factors against you to get your paper accepted?  (let's be realistic here!)I specifically mean these conferences: ACL, EMNLP, NAACL, WSDM, WWW, AAAI, NIPS, ICML.These usually use CMT, SoftConf, or EasyChair websites to manage submissions. Also recently OpenReview website.",POS
191,q0kypk,[P] SpotML - Managed ML Training on cheap AWS/GCP Spot Instances,https://www.reddit.com/r/MachineLearning/comments/q0kypk/p_spotml_managed_ml_training_on_cheap_awsgcp_spot/,enthusiast_bob,MachineLearning,1970-01-01 00:00:01.633277766,Project,205,54,"Hi there, We built a tool called spotML to make training on AWS/GCP cheaper.[https://spotml.io/](https://spotml.io/)Spot Instances are 70% cheaper than On-Demand instances but are prone to interruptions. We mitigate the downside of these interruptions through the use of persistence features, including optional fallback to On-Demand instances. So you can optimize workflows according to your budget and time constraints.History: We were working on a neural rendering startup that needed a lot of GAN training which was getting very expensive. We were blowing roughly $1000, to train a single category class. Training on Spot instances was cheaper, but still a mess. It needed lot of hand holding/devops stuff to make it usable. So we built SpotML to automate a lot of things.PS: While the above startup failed, posting this here to see if the community finds this helpful :)",POS
192,q11wuj,[D]Projects to help people with special needs,https://www.reddit.com/r/MachineLearning/comments/q11wuj/dprojects_to_help_people_with_special_needs/,woah--,MachineLearning,1970-01-01 00:00:01.633339200,Discussion,9,4,"What are some project ideas that help people with special needs (disabilities, learning difficulties, etc.)  that you have seen and liked?",POS
193,q16hc9,"[R] Debiasing Image Datasets: Oxford University Presents PASS, an ImageNet Replacement for Self-Supervised Pretraining",https://www.reddit.com/r/MachineLearning/comments/q16hc9/r_debiasing_image_datasets_oxford_university/,Yuqing7,MachineLearning,1970-01-01 00:00:01.633357570,Research,3,3,"An Oxford University research team presents PASS, a large (1.28M) image collection excluding humans, created as an ImageNet replacement for self-supervised pretraining without technical, ethical or legal issues. Here is a quick read: [Debiasing Image Datasets: Oxford University Presents PASS, an ImageNet Replacement for Self-Supervised Pretraining.](https://syncedreview.com/2021/10/04/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-116/)The paper *PASS: An ImageNet Replacement for Self-Supervised Pretraining Without Humans* is on [arXiv](https://arxiv.org/abs/2109.13228).",NEG
194,q0ywyk,[D] Primer: Searching for Efficient Transformers for Language Modeling,https://www.reddit.com/r/MachineLearning/comments/q0ywyk/d_primer_searching_for_efficient_transformers_for/,gauravc2796,MachineLearning,1970-01-01 00:00:01.633324320,Discussion,7,6,"Google Research team has introduced a new paper called as Primer: Searching for Efficient Transformersfor Language ModelingVideo: https://youtu.be/ze7poOz-5PwThis paper contributes 2 factors. First is it provides a better architecture for neural network search. where the objective is to create the best efficient model architecture.And secondly using their best approach search transformer they have created a better architecture of the original transformer architecture and named it Primer.For Primer, the model architecture modifications are only two things that are 1. Squaring ReLU activation.2. Adding depthwise convolution layer after each key, value queries projection in self-attention.Timestamps:00:00 - Introduction02:25 - Explainer for Neural Network search07:50 - Neural Network search space15:00 - Neural network search method: evolutionary search23:40 - Dynamic Hurdles overview32:11 - Primer37:22 - Primer changes over the vanilla transformerReferences:Paper: https://arxiv.org/pdf/2109.08668v1.pdfCode: https://github.com/google-research/google-research/tree/master/primer",POS
195,q15tzo,"[D] Free Webinar: How UPS makes Data Driven Decisions for AI Innovation with Laura Patel and Gregory Brown, UPS this Thursday Oct 7 at 11:30 AM ET",https://www.reddit.com/r/MachineLearning/comments/q15tzo/d_free_webinar_how_ups_makes_data_driven/,DataGeek0,MachineLearning,1970-01-01 00:00:01.633355442,Discussion,1,1,"Hey everyone,Wanted to share this webinar I'm planning on attending with you and get your thoughts - I'm interested in how UPS uses AI/ML and their database management systems. Here's the information from the website.**Featured Guest Speakers: Gregory Brown, Vice President of  Strategy and R&D, Advanced Technology Group at UPS and Laura Patel,  Principal Data Analyst at UPS!**Global transportation and logistics leader UPS delivers more than 24  million packages and documents a day for customers in more than 220  countries and territories. To provide reliable service with a high level  of visibility, the company seamlessly collects and crunches petabytes  of information with sophisticated data management systems and highly  specialized data analysts. In this talk we will share insights from the  companyâ€™s deep understanding of how data is collected, engineered, and  analyzed to make scalable, data-driven decisions that support AI for innovation.**About Data for AI:**The Data for AI Community is geared toward innovative companies  pushing the boundaries of whatâ€™s possible with Artificial Intelligence  and cognitive technologies. This community is focused on the data side  of AI including: Data Engineering, Data Preparation, Data Labeling &  Annotation, Sourcing and Generating Data, and All Other Topics  Data-Related for AI. Join us at this monthly event for high-quality  content with compelling & informative speakers and opportunities to  network and connect with fellow like-minded individuals. â€‹**Agenda:*** 11:30-12:30pm: Featured Presentation* 12:30-13:00pm: Your Q&A and interactionHere's the link to the webinar: [https://events.cognilytica.com/CLNTc3MHwyNA](https://events.cognilytica.com/CLNTc3MHwyNA)",POS
196,q0v8eo,[R] Powerpropagation: A sparsity inducing weight reparameterisation,https://arxiv.org/abs/2110.00296,hardmaru,MachineLearning,1970-01-01 00:00:01.633310108,Research,8,1,,NEU
197,q0f6tg,[D] Favorite blogs about a technical (use-case) niche with ML?,https://www.reddit.com/r/MachineLearning/comments/q0f6tg/d_favorite_blogs_about_a_technical_usecase_niche/,paswut,MachineLearning,1970-01-01 00:00:01.633256829,Discussion,42,6,I'm looking for some inspiration. It could be anything from birding to linguistics of dead languages to real-estate. Just something where someone reads/reviews literature pertaining to their interest and does some independent analysis or replication of relevant papers.,POS
198,q0m8dp,[D] Why does the data need to be randomly shuffled?,https://www.reddit.com/r/MachineLearning/comments/q0m8dp/d_why_does_the_data_need_to_be_randomly_shuffled/,uoftsuxalot,MachineLearning,1970-01-01 00:00:01.633281583,Discussion,6,9,"Is there any theory behind why we need to train a model with the data shuffled ? If I recall correctly, if we take MNIST as an example, a model will have trouble learning if I train on 1s first, then 2s and so on. Why is this ? Iâ€™m guessing this probably has to do more with the way GD works but any theories/papers on this ?",NEG
199,q11wz9,Is it possible to train a model in a completely trustless open source way? [D],https://www.reddit.com/r/MachineLearning/comments/q11wz9/is_it_possible_to_train_a_model_in_a_completely/,SensiTemple,MachineLearning,1970-01-01 00:00:01.633339219,Discussion,0,11,"Hey guy's I am building a project and I want to implement some method of training a machine learning mode that would be completely open source and publicly viewable (preferably in real time). Is there a way I can do this, I don't like the idea of people having to trust one party. Is this currently a scalable / feasible solution? Thanks",POS
200,q0by8w,[D] Google/Facebook AI residency application 2022,https://www.reddit.com/r/MachineLearning/comments/q0by8w/d_googlefacebook_ai_residency_application_2022/,healthyhappylucky,MachineLearning,1970-01-01 00:00:01.633241007,Discussion,47,13,Did the applications open up already? I was consistently checking for the last couple of months but didn't find any. Any info on this? Thanks!,POS
201,q0nz9s,[R] Useful structure constraints in indoor SLAM systems - Link to a free online lecture by the author in comments,https://i.redd.it/0y74esek6ar71.png,pinter69,MachineLearning,1970-01-01 00:00:01.633286772,Research,4,1,,NEU
202,q072ov,[D] What nice mathematical results there are about neural networks?,https://www.reddit.com/r/MachineLearning/comments/q072ov/d_what_nice_mathematical_results_there_are_about/,carlml,MachineLearning,1970-01-01 00:00:01.633221376,Discussion,101,70,"I will have the chance to give a presentation to a group of mostly mathematicians. I'd like to present them with some proofs or properties of neural networks. For instance, I remember a paper showed that any two points that are a global minimum are connected by a line where the loss is constant equal to the global minimum; I think that is a neat result. I could not find that paper, I'd appreciate if someone has a link or knows the title. I am open to any other ideas or neat theoretical results/properties about neural networks. I am mostly interested in results that have been settled and are rigorous; for example, generalization is a very interesting problem, but as far as I know, no one has said the last word about that, so I'd rather stay away from those kinds of results. Thanks.&#x200B;Edit: this is the paper I was thinking of [https://arxiv.org/abs/1901.07417](https://arxiv.org/abs/1901.07417) shared by u/vwings",POS
203,q0v0u5,"[D] Computer Vision workshop for teenagers, beginner friendly projects ideas to demonstrate and get their interest",https://www.reddit.com/r/MachineLearning/comments/q0v0u5/d_computer_vision_workshop_for_teenagers_beginner/,randomaier,MachineLearning,1970-01-01 00:00:01.633309350,Discussion,0,3,"Hello everyone, hope you are doing well!Can someone share with me some Deep Learning project ideas to do in a workshop thatâ€™s planned to be for teenagers around 30 minutes.At first I was planning to go with a cat/dog classification, however I would love to show of something more unique since they can easily find it in any tutorial.In a nutshell, I both want something interesting and funny to get their interest in the sphere and something to not burn their mind :)",POS
204,q0ugtp,[P] Bilingual text alignment tools for NMT - help needed,https://www.reddit.com/r/MachineLearning/comments/q0ugtp/p_bilingual_text_alignment_tools_for_nmt_help/,principe2020,MachineLearning,1970-01-01 00:00:01.633307362,Project,0,3,"As per the title, we are beginning a large-scale NMT project and need to reduce reliance on manual alignment work of bilingual texts. Anyone with experience on this? Are any tools available? Help would be greatly appreciated!",POS
205,q0tavx,[D] [Discussion] What does 'cache' mean in XLM-Roberta's code,https://www.reddit.com/r/MachineLearning/comments/q0tavx/d_discussion_what_does_cache_mean_in_xlmrobertas/,kthrn22,MachineLearning,1970-01-01 00:00:01.633303280,Discussion,1,2,"I am reading XLM-Roberta's code: [Link](https://github.com/facebookresearch/XLM/blob/main/xlm/model/transformer.py)What does 'cache' mean in the forward function of the MultiHeadAttention class in the codedef forward(self, input, mask, kv=None, cache=None)",NEU
206,q0ds5x,[R] Baidu Research Introduces PP-LCNet: A Lightweight CPU Convolutional Neural Network With Better Accuracy And Performance,https://www.reddit.com/r/MachineLearning/comments/q0ds5x/r_baidu_research_introduces_pplcnet_a_lightweight/,techsucker,MachineLearning,1970-01-01 00:00:01.633249764,Research,13,2,"Convolutional neural networks (CNNs) have been used to achieve computer vision applications for the past few years. These networks can be trained and applied in many fields, including image classification, object detection, semantic segmentation.Inference speed on mobile devices based ARM architecture or CPU devices based x86 architecture has been challenging to get with the increase of model feature extraction capability and model parameters. Even many good mobile networks have been proposed to resolve this issue, but the speed of these proposed networks is not good enough on the Intel CPU due to various limitations of MKLDNN.Baidu researchers have developed a lightweight CPU network based on the MKLDNN acceleration strategy, named [PP-LCNet](https://arxiv.org/pdf/2109.15099.pdf). This new system improves the performance of models in many tasks making it perfect for future artificial intelligence (AI) systems. The research groupâ€™s rethink the lightweight model elements for a network designed on Intel-CPU. During the research, the research team brings up following three questions to resolve....# [3 Min Quick Read](https://www.marktechpost.com/2021/10/03/baidu-research-introduces-pp-lcnet-a-lightweight-cpu-convolutional-neural-network-with-better-accuracy-and-performance/) | [Paper](https://arxiv.org/pdf/2109.15099.pdf)| [Github PaddleClas](https://github.com/PaddlePaddle/PaddleClas)&#x200B;https://i.redd.it/j0kn7n5k47r71.gif",POS
207,q041zu,"[D] Are there pytorch notebooks for the Aurelien Geron's book ""Hands-on Machine Learning with Scikit-Learn, Keras & TensorFlow""?",https://www.reddit.com/r/MachineLearning/comments/q041zu/d_are_there_pytorch_notebooks_for_the_aurelien/,arsenale,MachineLearning,1970-01-01 00:00:01.633210785,Discussion,66,19,"The second part is about tensorflow. Given that the theory explanations seem good, I wonder if anyone has converted the notebooks to pytorch.",POS
208,q0hdgt,"Approx time for association rule algorithms like ECLAT, Apriori, etc. [D]",https://www.reddit.com/r/MachineLearning/comments/q0hdgt/approx_time_for_association_rule_algorithms_like/,Thanos_nap,MachineLearning,1970-01-01 00:00:01.633266373,Discussion,3,2,"Hi.  I have with me around a months sales data of the company I'm working for and I want to find out what products customers are buying together. Earlier, we were using Excel for doing lift analysis but with a very small dataset. It was a lot of manual work and we have recently moved on to python.  This time around, I have the data in the required format for running these algorithms and was just wondering how much time will it take to run? Should I do it on my local machine (if it won't take a lot of time) or should we use some cloud service which will keep it running at the back end?   The current data which I have has approx 16k rows. Let me know approx time you guys had to run your model!  Format is:  Order 1: item 1, item 2,... Item N  Order 2: item 2, item 2,...item N  .  .  .  Order N: item 1, item 2,.. item NThanks!",POS
209,q0cex6,[D] Help me understand self-supervised learning,https://www.reddit.com/r/MachineLearning/comments/q0cex6/d_help_me_understand_selfsupervised_learning/,metallicapple,MachineLearning,1970-01-01 00:00:01.633243232,Discussion,8,14,"A statistics grad student here. I'm trying to understand the difference between self-supervised learning and unsupervised learning. For example, wikipedia's definition seems to place self-supervised somewhere in between supervised and unsupervised learning, but the blog post from Facebook AI writes that it is about rebranding of unsupervised learning for more clarity. So far, I wasn't able to identify a significant difference between the two. Is it just about renaming or are there more to it?",POS
210,q0lplb,[P] Knowledge Graph Search of 60 Million Vectors with Weaviate,https://link.medium.com/e85q5lyF3jb,thirdtrigger,MachineLearning,1970-01-01 00:00:01.633280011,Project,0,0,,NEU
211,q01pmm,"[R] Learning the internals of Machine Learning systems ft. Maithra Raghu, Google Brain",https://www.reddit.com/r/MachineLearning/comments/q01pmm/r_learning_the_internals_of_machine_learning/,jaygshah22,MachineLearning,1970-01-01 00:00:01.633202945,Research,47,1,"[Vision Transformers vs CNNs](https://youtu.be/htnJxcwJqeA) \- A useful discussion: The first author of a recent paper [""Do Vision Transformers See Like Convolutional Neural Networks?""](https://arxiv.org/abs/2108.08810) explains in this podcast how Vision Transformers (ViTs) differ from traditional CNNs, overall on analyzing the internal workings of deep neural networks so that we can deploy them better keeping humans in the loop and some great tips for PhD students/researchers!",POS
212,q0utbe,"[D] Microsoft, where are C# versions of sklearn, statsmodels, matplotlib, etc?",https://www.reddit.com/r/MachineLearning/comments/q0utbe/d_microsoft_where_are_c_versions_of_sklearn/,a90501,MachineLearning,1970-01-01 00:00:01.633308612,Discussion,0,6,"I'll never understand, MS, 1T$ company, let python eat its lunch in the Data Science area, while they could have created far better DS/ML toolset/libs and environment with C# for Visual Studio, VS Code, and Excel. I guess they are too busy working hard to please JS devs with Typescript and Linux Windows ""fans"" with WSL2.It seems to me that pleasing competitors including fixing their tools and languages to better compete with MS has been MS strategy for a while now. If MS was making Coca Cola, Anders Hejlsberg would be working around the clock improving Pepsi.I've seen some attempts by MS devs to create some libs for DS/ML, but those seemed unsupported hobby attempts - that is why they all appear to be either abandonware or feature poor.ML Studio and ML Workbench are not what I'm looking for, as you may have guessed. I'm looking for pure C# code libs covering most classic algos for DS/ML/Stats needs, that could be incorporated royalty-free in custom closed-source commercial software just like one can do so with pretty much anything MS produces for .net. For example, one can include MS native math .net lib in custom business app and license it to customers without any strings attached unlike with OSS libs.Any ideas/info on when is MS going to create something that uses actual MS technology for DS/ML?I hope I missed it somehow. Thanks in advance for your input.",POS
213,pzo9e1,[R] Vision Transformers for Dense Prediction,https://v.redd.it/nlc5txejryq71,Illustrious_Row_9971,MachineLearning,1970-01-01 00:00:01.633148542,Research,506,6,,NEU
214,pzxcyr,[R] Neural Networks For Chess,https://github.com/asdfjkl/neural_network_chess,Rod_Rigov,MachineLearning,1970-01-01 00:00:01.633188692,Research,38,0,,NEU
215,q0euys,[D] - How to use ML to detect key from a value,https://www.reddit.com/r/MachineLearning/comments/q0euys/d_how_to_use_ml_to_detect_key_from_a_value/,Atul_Su,MachineLearning,1970-01-01 00:00:01.633255290,Discussion,1,5,"I have a CSV file with first row as header & after that each row has set of values for each column. Each row in the CSV correspond to some field in the DB. user has to manually map the header attributes to Business fields (or DB fields). I want to use ML to learn from existing values, when single or set of values are provided then it should detect the key.  Overall I want to avoid manual mapping. One can think going forward there is no more header row in the CSV file. Is any services (preferably Azure) to machine learn from set of key-value pairs. When a value is provided then it should detect the keys? Please note this does not involve any OCR, my data is in CSV form.",POS
216,q01inv,[P] I trained Noam Chomsky TTS,https://www.reddit.com/r/MachineLearning/comments/q01inv/p_i_trained_noam_chomsky_tts/,ASA--NISI--MASA,MachineLearning,1970-01-01 00:00:01.633202315,Project,15,11,"Results: https://www.youtube.com/watch?v=CbIlyyEotBQTrained (fine-tuned) Tacotron 2 + Waveglow on around 1 hour 10 minutes of clean Noam Chomsky speech.I collected the data from YouTube - I specifically looked for videos with captions available. At first I tried an automatic approach to cut the audio by caption timestamps, but the results were terrible (correct timbre, incomprehensible words). So I had to manually go through the waveforms and correctly adjust them. For any 10 seconds of audio I probably had to spend more than 30 seconds of properly aligning and replaying it.I tried Tacotron + WaveRNN - the results were ok but too robotic/artificial, although the training felt easier.I briefly tried Flowtron but couldn't get anything good, and the NVIDIA implementation felt subpar (random code exceptions, didn't have the patience debugging)Both Tacotron 2 and Waveglow were trained for just a few thousand steps (not epochs). At least for tacotron 2, until the validation loss started going up, which happened very early. This was possibly due to my small dataset. I still have ~2 hours of data that I could go over and clean.What do you guys think about the results? Are they any good?",NEG
217,pzy5j1,[D] [R] A Roadmap and Critique of Dynamic Mode Decompositions,https://www.reddit.com/r/MachineLearning/comments/pzy5j1/d_r_a_roadmap_and_critique_of_dynamic_mode/,AcademicOverAnalysis,MachineLearning,1970-01-01 00:00:01.633191272,Discussion,7,0,"Hello everyone!It's been a bit of a while since I've really made a solid post about this, but I have been continuing my exploration of **Dynamic Mode Decompositions**. If you don't know what that is, then it is a machine learning technique for studying unknown dynamical systems from collections of their trajectories. The method intertwines operator theory, data science, and dynamical systems theory, and also intersects a good deal with control theory. **It is a great little place to explore new operator theoretic methods from a pure mathematical standpoint, and to use operator theory tools to answer questions in machine learning.****I have an introductory lecture here**, for those that are interested: [https://youtu.be/\_qjSprLvGS0](https://youtu.be/_qjSprLvGS0)My colleagues and I have been picking apart the field for the past two or three years, and over the last year or so we have made a lot of significant strides. There has been a lot of work surrounding DMD that uses Koopman operators, which play very well with Ergodic theory. That means if you were to restrict yourself to L\^1 or L\^p, then there is some hope of recovering at least one eigenfunction from the Koopman operator through the Birkhoff and Von Neumann Ergodic Theorems. This is largely the approach the Igor Mezic and his colleagues have taken in the exploration of ""Koopmanism.""However, *Koopman operators truly correspond to discrete time systems*, and necessarily, if you are going to study a continuous time system using those operators, then *you need a system that is discretizable*. This means you need a system that is forward invariant or forward complete (where the former is a statement about sets and the latter about dynamics). One approach to verify forward completeness is to have a global Lipschitz condition, which means that your system would need to be bounded globally by a linear system.We just had a paper accepted to the Journal of Nonlinear Science (arXiv: [https://arxiv.org/abs/1910.03977](https://arxiv.org/abs/1910.03977)), which uses occupation kernels and Liouville Operators (a generalization of Koopman generators) to *access the continuous time dynamics directly*, without worrying about discretizations. This means that we can study a wider class of dynamics with our methods, and can leverage observed data that comes from systems that admit finite escape times, such as x' = 1+x\^2 (i.e. tangent functions). I give an outline of the method here: [https://youtu.be/xfZG0mhKd0s](https://youtu.be/xfZG0mhKd0s)But unfortunately, **DMD methods as they are right now are strictly heuristic**. The best convergence guarantees that are out there involve the Strong Operator Topology, which doesn't give spectral convergence. DMD is a spectral method, and so *the obtained models will not necessarily converge to the true underlying dynamics for your data.*The answer to this is fairly clear, *we need compact operator* to get norm convergence of the sequence of finite rank operators that we are approximating with. However, Koopman operator for deterministic systems are rarely compact, where even the simplest dynamics x' = 0 yield Koopman operators that are not compact (the identity operator).Our accepted manuscript also introduces a compact dynamic operator that is ""close"" to the unbounded Liouville operator in some sense, which we call the scaled Liouville operator. It is compact and sort of nudges the data an infinitesimal amount.However, we have better results coming down the line, and we have an arXiv publication that gives a truly compact operator for doing DMD. It's tricky to use, and so I've been expanding my YouTube series to dive into it a bit. This month, I'm going to release another video that will outline the Singular DMD method. To get started, I put this together, which gives an introduction to compact operators and their spectral theory: [https://youtu.be/Bqz73NQ5XaQ](https://youtu.be/Bqz73NQ5XaQ)**I always value the contributions from this community, and you all have helped me catch some errors in my own work. Please, come and have a watch, and I'd love to hear from you.**My DMD Playlist: [https://www.youtube.com/playlist?list=PLldiDnQu2phvYaTSaFes0XdWkhphBHpwt](https://www.youtube.com/playlist?list=PLldiDnQu2phvYaTSaFes0XdWkhphBHpwt)",POS
218,pzqruy,"[R] Microsoft AI Unveils â€˜TrOCRâ€™, An End-To-End Transformer-Based OCR Model For Text Recognition With Pre-Trained Models",https://www.reddit.com/r/MachineLearning/comments/pzqruy/r_microsoft_ai_unveils_trocr_an_endtoend/,techsucker,MachineLearning,1970-01-01 00:00:01.633160667,Research,23,0,"The problem of text recognition is a long-standing issue in document digitalization. Many current approaches for text recognition are usually built on top of existing convolutional neural network (CNN) models for image understanding and recurrent neural network (RNN) for char-level text generation. There are some latest progress records in text recognition by taking advantage of transformers, but this still needs the CNN as the backbone. Despite various successes by the current hybrid encoder/decoder methods, there is definitely some room to improve with pre-trained CV and NLP models.Microsoft research team unveils â€˜[TrOCR](https://arxiv.org/pdf/2109.10282.pdf),â€™ an end-to-end Transformer-based OCR model for text recognition with pre-trained computer vision (CV) and natural language processing (NLP) models. It is a simple and effective model which is that does not use CNN as the backbone. TrOCR starts with resizing the input text image into 384 Ã— 384, and then the image is split into a sequence of 16 Ã— 16 patches used as the input to image Transformers. The research team used standard transformer architecture with the self-attention mechanism on both encoder and decoder parts where word piece units are generated as recognized text from an input image.# [4 Min Read](https://www.marktechpost.com/2021/10/02/microsoft-ai-unveils-trocr-an-end-to-end-transformer-based-ocr-model-for-text-recognition-with-pre-trained-models/)| [Paper](https://arxiv.org/pdf/2109.10282.pdf) | [Github](https://github.com/microsoft/unilm/tree/master/trocr)&#x200B;https://preview.redd.it/qwyzdqfmrzq71.png?width=1308&format=png&auto=webp&s=b7cde292a34b76e22526ed6d032def0815b62670",POS
219,pziubx,[R] Stochastic Training is Not Necessary for Generalization,https://arxiv.org/abs/2109.14119,koolaidman123,MachineLearning,1970-01-01 00:00:01.633127905,Research,77,32,,NEU
220,q07dzy,PASCAL VOC 2012 evaluation [P],https://www.reddit.com/r/MachineLearning/comments/q07dzy/pascal_voc_2012_evaluation_p/,innerpeacey,MachineLearning,1970-01-01 00:00:01.633222566,Project,0,0,"Hi,I'm new to computer vision so this will sound basic. How do I generate the files needed for evaluation of the pascal voc 2012 test set on the evaluation server.I trained a yolo model using this repo https://github.com/AlexeyABThanks.",POS
221,pzwvo1,[N][CfP] AI for Design and Manufacturing Workshop (ADAM) @ AAAI 2022,https://www.reddit.com/r/MachineLearning/comments/pzwvo1/ncfp_ai_for_design_and_manufacturing_workshop/,Ingenuity39,MachineLearning,1970-01-01 00:00:01.633187148,News,2,2," Hello [r/MachineLearning](https://www.reddit.com/r/MachineLearning/)!Advances in complex engineering systems such as manufacturing and materials synthesis increasingly seek artificial intelligence/machine learning (AI/ML) solutions to enhance their design, development, and production processes. However, despite increasing interest from various subfields, AI/ML techniques are yet to fulfill their full promise in achieving these advances. Key obstacles include lack of high-quality data, difficulty in embedding complex scientific and engineering knowledge in learning, and the need for high-dimensional design space exploration under constrained budgets.This workshop aims to bring together researchers from core AI/ML, design, manufacturing, scientific computing, and geometric modeling. Our intent is to facilitate new AI/ML advances for core engineering design, simulation, and manufacturing. Outcomes include outlining the main research challenges in this area, cross-pollinating collaborations between AI researchers and domain experts in engineering design and manufacturing, and sketching open problems of common interest.Paper submissions are open now and we look forward to your paper submissions on the following (and related) topics:* New theory and fundamentals of AI-aided design and manufacturing,* Novel AI-based techniques to improve modeling of engineering systems,* Integration of AI-based approaches with engineering prototyping and manufacturing,* Novel methods to learn from scarce/sparse, or heterogenous, or multimodal data,* Novel ML methods in the computational material and physical sciences,* Novel ML-accelerated optimization for conceptual/detailed system design,* Novel AI-enabled generative models for system design and manufacturing,* ML-guided rare event modeling and system uncertainty quantification,* Development of software, libraries, or benchmark datasets, and* Identification of key challenges and opportunities for future research.Workshop website:Â [https://adam-aaai2022.github.io/](https://adam-aaai2022.github.io/)Submission website:Â [https://openreview.net/group?id=AAAI.org/2022/Workshop/ADAM](https://openreview.net/group?id=AAAI.org/2022/Workshop/ADAM)Submission deadline: November 12, 2021",POS
222,pz9983,[R] ETH Zurich and NVIDIAâ€™s Massively Parallel Deep RL Enables Robots to Learn to Walk in Minutes,https://www.reddit.com/r/MachineLearning/comments/pz9983/r_eth_zurich_and_nvidias_massively_parallel_deep/,Yuqing7,MachineLearning,1970-01-01 00:00:01.633098727,Research,150,14,A research team from ETH Zurich and NVIDIA proposes a training framework that enables fast policy generation for real-world robotic tasks â€” training time can be reduced by multiple orders of magnitude using massive parallelism on a single workstation GPU. Here is a quick read: [ETH Zurich and NVIDIAâ€™s Massively Parallel Deep RL Enables Robots to Learn to Walk in Minutes.](https://syncedreview.com/2021/10/01/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-115/)The paper *Learning to Walk in Minutes Using Massively Parallel Deep Reinforcement Learning* is on [arXiv](https://arxiv.org/abs/2109.11978?context=cs.LG).,NEU
223,pzkjzv,"[R] Barlow is right, the perceptual system may indeed be guided by some underlying principle",https://www.reddit.com/r/MachineLearning/comments/pzkjzv/r_barlow_is_right_the_perceptual_system_may/,tobby_liu,MachineLearning,1970-01-01 00:00:01.633133908,Research,23,3,"This [article](https://arxiv.org/abs/2109.13102) finds a way to implement the Infomax principle with local, spike-based, and continuous-time learning rules.>It is widely believed that the perceptual system of an organism is optimized for the properties of the environment to which it is exposed. A specific instance of this principle known as the Infomax principle holds that the purpose of early perceptual processing is to maximize the mutual information between the neural coding and the incoming sensory signal. In this article, we show a model to implement this principle accurately with spatio-temporal local, spike-based, and continuous-time learning rules.A long misunderstanding is that because the distribution of the stimulus x is usually unknown, it is impossible to calculate the mutual information between x and the corresponding representation y, and so we can not maximize I(x ; y) accurately. This thought has been proven wrong in this article. Instead, we can apply the gradient-decent method directly.The well-known tuning curves of neurons have been reproduced in this [repository](https://github.com/TobbysGitHub/tuning_curve).&#x200B;[Synaptic strengths before training\(units=6, dots=128\)](https://preview.redd.it/rp93e3oejzq71.png?width=393&format=png&auto=webp&s=6f8b4f3b9e1b68a61bcf356f05e84cc1778a2337)[Synaptic strengths after 1,000,000 training steps](https://preview.redd.it/c5lz53oejzq71.png?width=393&format=png&auto=webp&s=2d47d735576b93f9f4534b3a500a4d9dd87a1cfc)&#x200B;[Synaptic strengths after 13,000,000 training steps](https://preview.redd.it/73g705oejzq71.png?width=393&format=png&auto=webp&s=937b9f36e58111a795a44ada71f64d2c49b659cd)To the best of my knowledge, this is the first work that finds a population of tuning functions that maximizes MI.",POS
224,q08csy,Internship Opportunities in Deep Learning [D],https://www.reddit.com/r/MachineLearning/comments/q08csy/internship_opportunities_in_deep_learning_d/,DarkCorona1221,MachineLearning,1970-01-01 00:00:01.633226242,Discussion,0,3,"Hi, I was wondering how can someone look for interesting internship opportunities in Deep Learning. Any help in this regard would be highly appreciated.",POS
225,pzsz7i,[D] [P] Where can I find Molecular Simulation Trajectory DataSet?,https://www.reddit.com/r/MachineLearning/comments/pzsz7i/d_p_where_can_i_find_molecular_simulation/,RepresentativeYear72,MachineLearning,1970-01-01 00:00:01.633171953,Discussion,1,1,"Hi, I am an undergraduate studying Computer Science and currently doing a project in predicting Collective Variables for studying Molecular Dynamics using Deep Learning. If possible I would like to check models using some datasets already available online. I need multiple trajectories of a single system with the same conditions. If anybody could provide me some resources it would be really helpful.",POS
226,pz8if5,[D] Do you find it hard to communicate the value of ML at your company?,https://www.reddit.com/r/MachineLearning/comments/pz8if5/d_do_you_find_it_hard_to_communicate_the_value_of/,kid_blizzard,MachineLearning,1970-01-01 00:00:01.633096482,Discussion,56,33,"I've been speaking to a lot of MLEs and DS friends and it seems to be a recurring issue for those in industry to feel like their company/leadership doesn't really understand the value of ML and how impactful it can be on the product.Anyone else experiencing the same thing? How do you communicate the potential of your work to decision makers? I guess this is relevant more for those who want to be putting things into prod, not pure researchers.",POS
227,pzqgvb,[D] Thoughts on neural nets simulation based training,https://www.reddit.com/r/MachineLearning/comments/pzqgvb/d_thoughts_on_neural_nets_simulation_based/,IntelligentAd4638,MachineLearning,1970-01-01 00:00:01.633159077,Discussion,2,1,"How far this can take us? I mean simulation comes with its own problems, this can add an overhead beside the problem of deep learning. Do we really need to solve its problem so we can use it effectively with neural nets?Here is a link to Ishan Misra's view on simulation based training. To be honest I agree with him.https://youtu.be/FUS6ceIvUnI?t=2h9m40s",POS
228,pyrjca,[D] Why do authors nuke their OpenReview discussions after paper is accepted?,https://www.reddit.com/r/MachineLearning/comments/pyrjca/d_why_do_authors_nuke_their_openreview/,kdfn,MachineLearning,1970-01-01 00:00:01.633031814,Discussion,153,41,"NeurIPS and ICLR this year both featured public reviewing via OpenReview. In several cases, the authors have deleted their discussion with reviewers after their paper is accepted. Here is [one recent example](https://openreview.net/forum?id=VdvDlnnjzIN). I am not calling out these authors in particular; this is just the first one I found---but I have noticed this occurring a lot over the past few months.What is the rationale behind doing this? For ICLR, I could understand the authors of a rejected submission trying to reduce the amount of baggage on the paper for future conferences. But why would anyone need to delete responses that ultimately end up in acceptance?EDIT: It occurred to me that it is possible that the comments were not deleted; rather, OpenReview has a ""permissions"" setting, in which the authors may have chosen to make their original responses invisible to everyone except the referees and AC. However, the same question applies: why would people need to hide their responses, especially given that reviews are public?",POS
229,pzc7qw,[D] Changing the structure of a (BiLSTM) model: should I increase first the layer size or make it deeper?,https://www.reddit.com/r/MachineLearning/comments/pzc7qw/d_changing_the_structure_of_a_bilstm_model_should/,Ok_Writing9374,MachineLearning,1970-01-01 00:00:01.633107574,Discussion,1,4,"I am currently working on a BiLSTM based model for Named Entity   Recognition and now I want to increase the layers and add more BiLSTM   layers (stacking). However, I wonder in which order it would make the   most sense or is there currently no consensus for this?  At least I couldn't find any papers that specifically addressed this - but maybe I just searched wrong?  Thanks in advance for your help :)",POS
230,pzlwhj,[D] ner model training data recommendations,https://www.reddit.com/r/MachineLearning/comments/pzlwhj/d_ner_model_training_data_recommendations/,guru223,MachineLearning,1970-01-01 00:00:01.633139002,Discussion,0,0,"I'm training a NER model to detect certain address phrases. I actually have a lot of manifest training data (showing the model what it should extract), but I don't have a lot of random data, or at least don't have a good pattern to create samples which the model shouldn't detect as a pattern.My issue, is that I want to extract something like: **Address: 123 Apple Cherry Lane**I've trained the model to detect the address, but there could be other text within the document being scanned which is creating false positives. For example: ""**So we realize that the address was not correct.**"" Since ""address"" exists in the sentence, the entire part after address is being picked up. I have an annotations of 250+ documents where I've tagged things that are addresses, but I dont have a lot of sample sentences in my training documents which contain the word address where something is not tagged.Any suggestions on how to reduce my false positives?I think I need more training data that contain paragraphs and sentences that use ""address"" but don't show up in my annotations manifest. Unsure though...",POS
231,pzhq6a,[D] Delta Lake - Continuous Data Modell Upgrades,https://www.reddit.com/r/MachineLearning/comments/pzhq6a/d_delta_lake_continuous_data_modell_upgrades/,MrAstroThomas,MachineLearning,1970-01-01 00:00:01.633124203,Discussion,3,0,"Hey everyone,I don't know whether this question is suitable for this sub, since it refers more to data pipelining and Cloud technologies; but since some of you may use Delta Lakes for their data storage it may fit here.Modern applications (also ML / AI products) contain a repo with a proper CI/CD pipelines, dev / int / prod stage, data storage, pipelining and also an ML model that is handled e.g., by MLflow.How do you handle continuous updates in the Delta Lake? Imagine the following scenario: You have raw data in a, let's call it Level-0 folder. A pipeline that is event- or time-schedule-driven converts these files to a Delta Lake (Level-1 folder). Level-2 folders are feature engineered files to be thrown into an ML model. Now imagine you update your pipeline (Level-0 to Level-1 or even to Level-2) in your repo and via the CD pipeline e.g., your Python library is deployed to your data processing pipeline. All new (raw) data are now converted based by the new library that contains a data model update. But how do you handle the ""older"" data that still lay around in the delta lake? Are you upgrading everything manually? Do you create a function to update the already existing Delta Lakes?I hope my question is not too confusing and I am looking forward to you ideas and suggestions.Best,Thomas",POS
232,pz4ny2,"[D] Small, neglected, complex datasets for benchmarking automatic machine learning",https://www.reddit.com/r/MachineLearning/comments/pz4ny2/d_small_neglected_complex_datasets_for/,elcric_krej,MachineLearning,1970-01-01 00:00:01.633081718,Discussion,8,8,"I've been working on a benchmark suite for testing automatic ML (basically libraries where you only have to input the target id and the data in a structured). Think autosklearn, auto gluoan, auto h2o, pycaret, etc.I'm doing this for selfish reasons (benchmarking of an automatic ml tool I'm working on). However, I'm open-sourcing every single dataset for which I can find an open license and the runtime environment. The RE is pretty dumb, it just does on-machine parallelism based on the nr of GPUs, but in principle, I should be able to implement multi-machine parallelism soon.At any rate, the issue I have with other such benchmarks (e.g. openML) is that they are too myopic in terms of problem-space. They boil down to classification, or at most classification and regression, based on categorical and numerical features.There is indeed a problem with a large benchmark suite, in that it can't run very large problems due to compute limitations (e.g. image net, translation tasks with an associated learning corpus, protein folding, DICOM classification, etc). However, I don't feel like that should limit it to only the most basics of tasks that can fit in a standard CSV.In my current implementation I've added:- Datasets with text inputs (mixed with date, numeric and categorical inputs)- Datasets with image inputs (mixed with categorical labels, think cifrar100 but feed the superclass + image and have it predict the class)- Timeseries data (where there's an ordering of and relation between all rows of the dataset)On the other hand, this seems to just be scratching the surface of what auto ml should be doing and I'm curious to find more datasets that represent complex problems *but* are not so large as to be computationally unviable to run with a startup budget (or with an individual researcher budget, since that's my goal for this suite).Hence why I'd like to focus on ""neglected"" areas first, since I assume they might actually harbour a lot of datasets for which auto ml techniques could provide real help. As opposed to, say, image classification, which is a nice point of comparison, but realistically you're competing against teams of dozens, building models that run across million-dollar clusters, so you're never touching the edge.Some ideas that come to mind, but for which I haven't found datasets:- Equation solving (text to text or text to number)- Images augmented with text (e.g. x-ray + doctor diagnosis)- Predicting numbers/classes from niche ""imaging"" formats where the typical network might not work well (e.g. electron diffraction patterns, or something else where the proximity of pixels doesn't ""mean"" what it means in macroscopic photography)- Any sort of mixed numeric/categoric and multimedia format (e.g. song recognition identification *but* with the genre or name of the band as an input)- Self-referential / graph datasets (e.g. text with a lot of hyperlinks to other parts of the page)- Non-text, non-multimedia sequences of arbitrary size and high-variance magnitude as either inputs or output- ""Typical"" classification or regression problems where ""traditional"" methods (e.g. boosting, random forests, regressions, plain TDs, clustering, SVMs or even normal MLPs) work poorly for some poorly understood reasons, not being able to outperform humans or not being able to outperform hand-crafted equations- Equation optimization aka parameter finding/tweaking (in cases where just running SGD on the equation themselves is impossible or practically impossible)But again, this is me spit-balling and I haven't really found good examples of such datasets.Also, I might be completely missing the mark as to what's ""interesting"" for generic supervised machine learning to tackle, what classes of problems this would work best for. So I'm curious what other people would think are the interesting niches I should focus my benchmarking on.Note for the pednatic: I get that ""computational budget"" varies 1000x based on the method being used and when you call quits (i.e. assume the model has converged to the best solution it's likely to obtain best on some validation data), but still, input size and a number of rows matter (although, again, I get that even those can be relative due to how you prepare/pre-encode/encode the data and because you can do sampling). But I think the framing still ""makes sense"" even if it has this inherent nebulosity.",POS
233,pyti87,[D] Have we reached some limit in the advancement of CNNs?,https://www.reddit.com/r/MachineLearning/comments/pyti87/d_have_we_reached_some_limit_in_the_advancement/,Cizox,MachineLearning,1970-01-01 00:00:01.633037819,Discussion,52,12,"I asked my professor this question because it seems that every modern CNN architecture is just some variant of ResNet or the residual block idea. ResNeXt is just ResNet + Inception, Xception is also similar, DenseNets are just residual blocks with concatenation instead of adding, and even EfficientNets are using NAS methods. With all the work that companies like Google or Microsoft put into research in CNN and ImageNet, have we reached some kind of stopping point with CNNs for the foreseeable future? Are Residual blocks the answer in a sense? If so, why?",NEG
234,pz6ff9,[D] R-GCN: Modeling Relational Data with Graph Convolutional Networks (Paper Summary),https://www.reddit.com/r/MachineLearning/comments/pz6ff9/d_rgcn_modeling_relational_data_with_graph/,prakhar21,MachineLearning,1970-01-01 00:00:01.633089376,Discussion,4,1,This paper proposes Representation Learning on relational graph-structured data like Knowledge Graphs using Graph Convolution Networks ðŸ”¥paper summary - https://youtu.be/Ys6VdaRguYUPaper details in the comments!,POS
235,pz5c0r,[P] Using rules to speed up labelling by 2x,https://www.reddit.com/r/MachineLearning/comments/pz5c0r/p_using_rules_to_speed_up_labelling_by_2x/,dataqa_ai,MachineLearning,1970-01-01 00:00:01.633084770,Project,3,0,"I have developed a tool to label documents using rules, and ran some experiments on a multi-class problem where I have 3000 Amazon product descriptions that belong to 24 product categories. I show how using a combination of rules and random labelling is much less time consuming and achieves better results than just performing random labelling.https://preview.redd.it/l9gdt47qhtq71.png?width=864&format=png&auto=webp&s=4addc2254cb1785b969d6c3e340448a6b6df32f9For more info: [https://dataqa.ai/case-study/](https://dataqa.ai/case-study/)The tool I developed and used for this problem: [https://github.com/dataqa/dataqa](https://github.com/dataqa/dataqa)",NEG
236,pyfjz7,[R] Deepmind's weather forecasting model 'Nowcasting' provides high-accuracy short-term weather prediction and better than traditional models for both accuracy and usefulness.,https://www.reddit.com/r/MachineLearning/comments/pyfjz7/r_deepminds_weather_forecasting_model_nowcasting/,proof_required,MachineLearning,1970-01-01 00:00:01.632990646,Research,344,28,"In a trial by more than 50 expert forecasters, the Met Office tested â€˜Nowcastingâ€™ model for high-accuracy short-term weather prediction and found it better than traditional models for both accuracy and usefulness.Accurate real-time weather predictions will become an increasingly important part of managing the effect of extreme weather. This research shows the potential of AI to help us tackle some of these challenges.Read more: https://www.nature.com/articles/s41586-021-03854-z  Access the code behind the model: https://github.com/deepmind/deepmind-research/tree/master/nowcasting",POS
237,pypjea,[Project] Making a Poker AI - having trouble with the form of ML to make smart / strong decisions,https://www.reddit.com/r/MachineLearning/comments/pypjea/project_making_a_poker_ai_having_trouble_with_the/,lucky_warrior,MachineLearning,1970-01-01 00:00:01.633025749,Project,15,18,"I have a large dataset from online poker. I have quite a lot of domain knowledge as well, so I find it somewhat easy to do feature engineering for this problem.I'm having some trouble making a ''strong'' decision maker from the data though. I think there is a few tough problems as part of the problem that make it hard.If poker is new to you, the distribution of results looks like this: If you and an opponent have gambled $100 each, the final outcomes will be either +$100 or -$100. And depending on the features, the likelihood of each outcome changes.  So in any given situation, the actual outcome might be -$100 or +$100, but the expected value might be +$5. I feel like the nature of this data makes some performance ''scores'' of a model misleading?   For some context on the current status of where I'm at - I've used a random forest regressor with some engineered features, and the 5 different decisions you can make in poker had scores of like:  Bet: 0.70, Check: 0.70, Fold: 0.99, Call: 0.70, Raise: 0.65.  Folding is very easy to predict as it is always a negative outcome, and it's basically half of the total gambled amount. Then the other actions I would expect to be harder to predict, so it all makes sense.There are a couple of problems I have with the random forest regressor:Feature designing (mostly when the pot is a chop pot, where the players tie - I think that is causing some bad judgement in some situations). There is also some incorrect derivations from the information (maybe it's a form of bias?). Basically in the training data there are situations where I would only make the play with a ''strong hand'' and the outcome was positive. Then in training, because there were no similar decisions with a ''weak hand'' and a strongly negative outcome (because it would be a terrible play I wouldn't do it), the model thinks that if 95% of the features are the same (except for the most important one, the hand strength!), it still thinks it will get a big win. Which is not true. It seems like it needs the experience of making a lot of ''bad'' decisions so that it can make that distinction. I'm not sure how to handle this - either embark on a self learning AI in a sandbox environment where it can learn from scratch making every possible decision. I also tried doing a ''bike stabiliser'' approach. Basically limiting the options available to the AI if certain criteria weren't met. So the program was not allowed to Raise without a hand of certain strength (because it was bluffing too much in bad situations).Then more generally - I've tried doing feature engineering + random forest regressor. Which is somewhat good - it's a good proof of concept. But I'm not sure if I should try a neural network solution with much less feature engineering and see if it can figure this stuff out for itself?I'm basically not sure what the next step should be for me to make this project a success.If anyone knows some good solutions for regression problems with data of this nature (large positive and negative scores, with an expected value in between them). Whereas predicting car value / stock price the values are much closer together.&#x200B;I think another drawback from the random forest regressor is that it can't extrapolate beyond what it has already seen in training. But it had the best performance compared to Linear regression and Logistic regression - so not sure what to do about that...",POS
238,pz5y9g,Has anyone here ever worked for a bank/financial institution? [Discussion],https://www.reddit.com/r/MachineLearning/comments/pz5y9g/has_anyone_here_ever_worked_for_a_bankfinancial/,GandhisLittleHelper,MachineLearning,1970-01-01 00:00:01.633087416,Discussion,0,7,Just started my MSc in machine learning and deep learning and was wondering if anyone here has worked for a bank and can say wether it was a fulfilling job or not?Just not sure what sector I want to apply my skills or work in yet,NEG
239,pyxjsg,[P]AI Biomedical Writer,https://www.reddit.com/r/MachineLearning/comments/pyxjsg/pai_biomedical_writer/,wangyi_fudan,MachineLearning,1970-01-01 00:00:01.633051457,Project,3,6,"Hi all,  I developed a biomedical writer based on a novel character level GPT model: [www.drwang.top](http://www.drwang.top)  On the website, you enter some context (my model has a context window size=1024), then click one-minute writing and wait for one minute to get about 1000 bytes output.  It achieves 0.987 BPC right now (1 week training on a A100 GPU) and was projected to <0.9 BPC at the end of October.  The model has 100M parameters and was developed by C++/cuda from the scratch. On a common PC, it produces text faster than human reading speed. I also successfully ran it on a Raspberry pi 3B+ with 1GB memory at a speed of 2 chars per second.  Maybe this is the first ""affordable"" GPT model: the tiny CPU webserver costs me only \~50$ per year.",POS
240,pz2rwc,[P] Rindow Neural Networks Library R1.3,https://www.reddit.com/r/MachineLearning/comments/pz2rwc/p_rindow_neural_networks_library_r13/,yuichiis,MachineLearning,1970-01-01 00:00:01.633072370,Project,0,0,"We have released Release 1.3 of the Rindow Neural Networks Library.[https://rindow.github.io/neuralnetworks/index.html](https://rindow.github.io/neuralnetworks/index.html)The Rindow Neural Networks is a high-level neural networks library for PHP.You can achieve powerful machine learning on PHP.* You can build machine learning models for DNNs, CNNs, RNNs, and Attentions.* You can use your knowledge of Python and Keras as it is.* Samples of popular Computer Vision and natural language processing are available.* The PHP extension can process data twice as fast as the tensorflow CPU version.* Support GPU acceleration* You don't need a dedicated machine learning environment. It can be done on an inexpensive laptop.* Including interesting sample programs.&#x200B;If you think that doing machine learning in PHP is too slow, that's a mistake.The Rindow Neural Networks Library was designed from the beginning to use a fast tensor arithmetic library.",POS
241,pyoulg,[N] AIPLANS Workshop at NeurIPS: Advances in Programming Languages and Machine Learning,https://www.reddit.com/r/MachineLearning/comments/pyoulg/n_aiplans_workshop_at_neurips_advances_in/,breandan,MachineLearning,1970-01-01 00:00:01.633023689,News,14,0,"Hi /r/MachineLearning! A few colleagues and I are organizing [AIPLANS](https://aiplans.github.io/), a new workshop on programming language theory, mechanized reasoning, and neural-symbolic learning, to be held this December at NeurIPS. We welcome submissions related to one of the following areas:**Mechanized reasoning*** Induction of formal languages (e.g., grammar inference, automata extraction)* Programming language theory (e.g., type theory, category theory, denotational semantics)* Satisfiability checking and symbolic computation (e.g. SAT/SMT solving, Boolean circuits)* Logic and equational reasoning (e.g., Î»-calculus, Ï€-calculus, tensor and combinator calculi)**Neural-symbolic learning*** Neural program synthesis (e.g., search-based, syntax or execution-guided)* Bayesian program learning (e.g., higher-order probabilistic programming)* Neural-symbolic reasoning (e.g., automated program verification and testing)* Neural program extraction (e.g., procedural or relational knowledge distillation)* Inference algorithms (e.g., backpropagation, belief propagation, survey propagation et al.)**Programming language design*** Natural language programming (e.g., machine teaching, programming by example)* Declarative programming (e.g., tabled LP, ILP, constraint programming et al.)* New programming languages for reasoning (e.g., LF/Twelf, HOL, miniKanren, Lâˆƒâˆ€N, et al.)* New programming languages for learning (e.g., Dex, Hasktorch, Torch-Struct, et al.)If you are working on one of these topics, we would be very glad to have your participation! Our call for papers will close Oct. 4th, 2021 AoE. AIPLANS is non-archival and allows dual submission where permitted by a third-party. Further details may be found at the following URL:[https://aiplans.github.io/callforpapers/](https://aiplans.github.io/callforpapers/)Should you have any inquiries or suggestions, please do not hesitate to reach out to us at [aiplans2021@gmail.com](mailto:aiplans2021@gmail.com) and we would be eager to assist!",POS
242,pyfuyu,"[D] Neural Networks, Manifolds and Topology",https://www.reddit.com/r/MachineLearning/comments/pyfuyu/d_neural_networks_manifolds_and_topology/,flyer2403,MachineLearning,1970-01-01 00:00:01.632992286,Discussion,60,13,"I recently stumbled on this great blog post by Chris olah on Neural Networks, Manifolds and topology.https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/Does anyone know any more resources, preferable more recent, on the same topic? How has the theory advanced since then?",POS
243,pyv2ym,[D] Regression input feature selection,https://www.reddit.com/r/MachineLearning/comments/pyv2ym/d_regression_input_feature_selection/,jameswattdoowop,MachineLearning,1970-01-01 00:00:01.633042923,Discussion,4,7,What are the downfalls for selecting non-causal variables as inputs to a model?  I have a dataset where the majority of â€œimportant variablesâ€ would not be a cause of the output variations. Is basically doesnâ€™t make sense physically but predicts well.    Itâ€™s due to some unmeasured cause showing itâ€™s symptoms in other non causes variablesâ€¦.  Iâ€™m hesitant to use a model like this,POS
244,pyk3wc,[D] NeurIPS 2021 Peer Review Consistency experiment,https://www.reddit.com/r/MachineLearning/comments/pyk3wc/d_neurips_2021_peer_review_consistency_experiment/,yusuf-bengio,MachineLearning,1970-01-01 00:00:01.633009563,Discussion,6,2,"I have noticed that some papers submitted to NeurIPS 2021 got 2 distinct review committees (review +AC) assigned, without the other one committee noticing. i.e., seems to be a replication of the NeurIPS 2014 experiment.In case the two committees disagreed on acceptance, the paper was accepted.I was wondering if any of you know more details on this.",POS
245,py13fz,[D] Lambda launches 1x NVIDIA RTX A6000 instances from $2.25/hr,https://www.reddit.com/r/MachineLearning/comments/py13fz/d_lambda_launches_1x_nvidia_rtx_a6000_instances/,mippie_moe,MachineLearning,1970-01-01 00:00:01.632938824,Discussion,172,27,"[https://lambdalabs.com/blog/introducing-nvidia-rtx-a6000-gpu-instances-on-lambda-cloud/](https://lambdalabs.com/blog/introducing-nvidia-rtx-a6000-gpu-instances-on-lambda-cloud/)# TLDR* **Performance boost:** For training NNs, Lambda's 1x RTX A6000 instance is \~80% faster with FP32 & \~35% faster with mix-precision than the 1x Tesla V100 VMs on AWS/GCP/Azure.* **Cost savings:** Lambda's new RTX A6000 instance costs $2.25/hour. Compare this to AWS's p3.2xlarge, which costs $3.06/hour and has a slower GPU, less system memory, and fewer CPU cores.**Reference*** [https://lambdalabs.com/gpu-benchmarks](https://lambdalabs.com/gpu-benchmarks)* [https://aws.amazon.com/ec2/instance-types/p3/](https://aws.amazon.com/ec2/instance-types/p3/)# 1x RTX A6000 specs:* **Price:** $2.25 / hour* **GPU:** 1x NVIDIA RTX A6000 (48 GiB VRAM)* **CPU:** 14 vCPUs @ 2.5 GHz* **RAM:** 100 GiB* **Storage:** 1 TiB NVMe* **Node-to-node bandwidth:** Up to 50 Gbps* **Internet bandwidth:** Up to 10 Gbps",NEU
246,pyss3i,[D] Transformer NLP model similar to GPT-2 345M with nice up-to-date code base and multi-GPU training support?,https://www.reddit.com/r/MachineLearning/comments/pyss3i/d_transformer_nlp_model_similar_to_gpt2_345m_with/,sabouleux,MachineLearning,1970-01-01 00:00:01.633035602,Discussion,0,6,"I am working on an interactive poetry project and I am searching for a model that would be easy to work with.I have worked on a previous project that involved a pre-trained version of the 345M GPT-2 model. It delivered great results for our use case. Larger models also worked great, but we opted for this smaller version since we had very limited compute available for inference â€” this was a personally-funded web-based application, and server time got expensive very quickly.I am working on a new project that both gives us the resources to train and fine-tune that model with our chosen datasets (cloud GPUs got really good and inexpensive in recent years!). We need to train it both in French and English. The datasets we have arenâ€™t huge, they have respectively about 60,000 and 8,000 literary pieces, so using a gigantic model wouldnâ€™t really be beneficial. We donâ€™t have as much of a restriction on inference compute here, as long as it can run fine on a decent CPU at a few words per second. My initial thought was to simply train the same model, but the code base is somewhat old (not compatible past TensorFlow 1.15), which seems to cause issues with newer Ampere GPUs. It also doesnâ€™t support multi-GPU training. I know there is a TensorFlow 2.0 fork, and I know I could spend a bit of time getting multi-GPU working by splitting batches, but time is short, and I figure there must have been a lot of NLP code written since then. So my question is: is there a nice, roughly similarly sized NLP model with a modern codebase youâ€™d recommend for this?",POS
247,py01l6,"[P] Exafunction NLP â€“ Run models like GPT-J and BERT (6x cheaper than OpenAI, 300x cheaper than Hugging Face)",https://www.reddit.com/r/MachineLearning/comments/py01l6/p_exafunction_nlp_run_models_like_gptj_and_bert/,varunkmohan,MachineLearning,1970-01-01 00:00:01.632935765,Project,99,15,"Iâ€™m Varun, cofounder of Exafunction and wanted to share with you a project we've been working on at [https://www.exafunction.com/nlp](https://www.exafunction.com/nlp). Weâ€™re excited about all the use cases of large NLP models like text generation and language comprehension, but found the existing offerings to be quite expensive. Our goal is to serve these models in a cost effective way at scale.The first model weâ€™re starting with is GPT-J, the recently released open source large language model by EleutherAI. Itâ€™s comparable to OpenAIâ€™s GPT-3 Curie in performance. With lots of optimizations, we are able to serve the model for 6x cheaper per token compared to OpenAI.We have a simple HTTPS API that you can try out for free. Hereâ€™s a link with the details - [https://www.exafunction.com/nlp-api](https://www.exafunction.com/nlp-api)Weâ€™re also looking into supporting custom and fine-tuned models since itâ€™s required to get good performance for a lot of applications. Especially for smaller models like BERT, weâ€™re excited about the cost savings we can deliver for users who use many different models. In this case, weâ€™re 100 - 1000x cheaper than others offering similar inference APIs like Hugging Face.We hope this is interesting to you all and please email me at [varun@exafunction.com](mailto:varun@exafunction.com) if thereâ€™s a model youâ€™d like to see supported.",POS
248,pz2kro,[Discussion] [Research] Anyone know a researcher or company that are making AI that can do everything?,https://www.reddit.com/r/MachineLearning/comments/pz2kro/discussion_research_anyone_know_a_researcher_or/,trigteck48,MachineLearning,1970-01-01 00:00:01.633071450,Discussion,0,5,"I know, this request may be a bit ambitious. But, I'm bothered by how neural networks work. The fact that neural network models can be so easily affected by a small change in weights and have no capability to do every task bothers me a bit. Well, a lot actually. Though, I am curious to what people are doing to make AI universal. Does anyone know any research or company that is taking some initiative in this? Looking for secondary resources for a potential research project.  Article that boosted by my curiosity: [https://spectrum.ieee.org/how-deepmind-is-reinventing-the-robot](https://spectrum.ieee.org/how-deepmind-is-reinventing-the-robot)",POS
249,pyjpl1,[N] PCD@Coimbra 2021: Open call for Artworks,https://www.reddit.com/r/MachineLearning/comments/pyjpl1/n_pcdcoimbra_2021_open_call_for_artworks/,sergiommrebelo,MachineLearning,1970-01-01 00:00:01.633008283,News,2,0,Hi![PCD@Coimbra 2021](http://pcdcoimbra.dei.uc.pt) is welcoming Artwork submissions for an exhibition on the theme of Anachronism! The submitted artworks should explore the use of code at any stage of the design process.There are 3 distinct submission tracks: System outputs as posters; systems demonstrations; and performances.Extended submission Deadline: **10/10/2021**The accepted works will be presented in Coimbra (Portugal) during this year edition of PCD@Coimbra (December 2021)More info at [http://pcdcoimbra.dei.uc.pt/2021/open-call](http://pcdcoimbra.dei.uc.pt/2021/open-call)  https://preview.redd.it/ct1cr30o6nq71.png?width=2083&format=png&auto=webp&s=6d3131cbb362d448f393d4f3bf22449e6e9f3f69,POS
250,pyg39s,Creating custom entity embeddings using multiple sources of information [D],https://www.reddit.com/r/MachineLearning/comments/pyg39s/creating_custom_entity_embeddings_using_multiple/,jazzcriminal,MachineLearning,1970-01-01 00:00:01.632993408,Discussion,4,3,"Hi everyone I'm a data scientist at a Job Portal, where I'm specifically dealing with 2 Entities : Users and Jobs.I have been reading up about how to create custom embeddings for entities, but haven't really found something concrete. I know there are a lot of open source encoders (eg Google's Universal Sentence Encoder \[USE\], word2vec etc), but I need to combine multiple attributes from an Entity. I can take individual attributes from the entities, eg. I can take Text information from the User, and convert it into a text embedding using Google's USE, but how can I take multiple attributes from a User like experience (as a number), education (as categorical variable or text), skills (text), applied jobs (as a sequence) and use all of these as input for an embedding.I came across an [article](https://www.kaggle.com/willkoehrsen/neural-network-embedding-recommendation-system) where the author attempted to create entity embeddings by training a neural network for a downstream task, and then taking out the embedding layer for encoding. I would need embeddings which can be updated constantly (by which I mean that the user embedding can be changed as soon as the user information changes). Something like a universal sentence encoder in which I can input information and get an Embedding.I would really appreciate it if someone could point me towards resources for creating custom embeddings, or could tell me about a methodology in general. I'd also like to know how you would approach creating entity embeddings in generalThanks!TL;DR - Need to create entity embeddings for entities whose input information could change day to day",POS
251,py9fe1,[R] CLIPort: What and Where Pathways for Robotic Manipulation,https://arxiv.org/abs/2109.12098,hardmaru,MachineLearning,1970-01-01 00:00:01.632965272,Research,9,4,,NEU
252,py0289,[R] Skilful precipitation nowcasting using deep generative models of radar,https://www.nature.com/articles/s41586-021-03854-z,hardmaru,MachineLearning,1970-01-01 00:00:01.632935817,Research,32,7,,NEU
253,pxprp0,[NEWS] Google Colab(Pro+) gives A100 GPU!,https://www.reddit.com/r/MachineLearning/comments/pxprp0/news_google_colabpro_gives_a100_gpu/,beomi,MachineLearning,1970-01-01 00:00:01.632897902,News,186,64,"Today I just start a new notebook with GPU backend, and I noticed that google colab(pro+, as I currently subscribe) gives me a A100 GPU!Since it is the first time I get the a100 GPU, I just wanted to share this :-) Hope you'll get another a100 too!https://preview.redd.it/zum86t8z1eq71.jpg?width=1280&format=pjpg&auto=webp&s=a777a9f8330b86cdcc0fdff560903089e363d70d",POS
254,py8ra2,"[N] EvoStar 2022, this year with Evolutionary Machine Learning track!",https://www.reddit.com/r/MachineLearning/comments/py8ra2/n_evostar_2022_this_year_with_evolutionary/,jnccorreia,MachineLearning,1970-01-01 00:00:01.632962909,News,7,2,"Dear colleagues,We are organizing the 25th EvoStar, the Leading European Event on Bioâ€‘Inspired Computation that you may found interesting. For those working on Bio-inspired Computation, Evolutionary Computation, Machine Learning, Metaheuristics and Optimization among other related fields, it is a great event to put into consideration to present your work or to follow the latest updates on the topics.It is an event that has co-located conferences which include:- **EuroGP**: 25th European Conference on Genetic Programming:- **EvoApplications**: 25th European Conference on the Applications of Evolutionary and bio-inspired Computation- **EvoCOP**: 22nd European Conference on Evolutionary Computation in Combinatorial Optimisation- **EvoMUSART**: 11th International Conference (and 16th European event) on Artificial Intelligence in Music, Sound, Art and Design.This year there is even a new joint track for **Evolutionary Machine Learning**!More information at:[www.evostar.org/2022](http://www.evostar.org/2022)https://preview.redd.it/69djw0p1hjq71.png?width=1039&format=png&auto=webp&s=d630b45431c7fb1d02cf52b5a9c9ef7beaecb0e2",POS
255,pxr3rz,[R] University of Oxford Researchers Release â€˜PASSâ€™ Dataset With 1.4M+ Images (Free From Humans) For Self-Supervised Machine Learning,https://www.reddit.com/r/MachineLearning/comments/pxr3rz/r_university_of_oxford_researchers_release_pass/,techsucker,MachineLearning,1970-01-01 00:00:01.632904424,Research,90,22,"The development of modern machine learning could not have happened without an extensive research dataset. For quite some time, computer vision has relied on large-scale datasets of images like ImageNet and others sampled from the Internet for pretraining models. The use of datasets is not always ethically and technically sound, as they can contain personal information taken without consent. They also have unclear license usage that biases their results to be inaccurate or misleading in many cases. A more recent development is to use unsupervised methods for model pretraining. This means that instead of using labelled datasets such as ImageNet, we can train our models without any specific input images and labels provided at all.Researchers from the university of oxford have proposed [â€˜PASSâ€™ dataset](https://arxiv.org/pdf/2109.13228.pdf), which contains an extensive collection of images (1.28M) excluding humans and other identifying information such as license plates, signatures, or handwriting and NSFW images. The research group started with a large-scale (100 million random Flickr images) datasetâ€”YFCC100M. They also preferred data collected under the most permissive Creative Common license (CC-BY) to address copyright concerns.# [3 Min Read](https://www.marktechpost.com/2021/09/29/university-of-oxford-researchers-release-pass-dataset-with-1-4m-images-free-from-humans-for-self-supervised-machine-learning/) | [Paper](https://arxiv.org/pdf/2109.13228.pdf) | [Project](https://www.robots.ox.ac.uk/~vgg/research/pass/) | [Code](https://github.com/yukimasano/PASS)&#x200B;https://i.redd.it/ftwjvksoleq71.gif",POS
256,pyk40l,"[D] Machine Learning: Overfitting Is Your Friend, Not Your Foe",https://www.reddit.com/r/MachineLearning/comments/pyk40l/d_machine_learning_overfitting_is_your_friend_not/,DavidLandup,MachineLearning,1970-01-01 00:00:01.633009573,Discussion,0,25,"Hey! I'd love to start a discussion on a topic that has been bugging me.Humans tend to associate labels such as ""good"" or ""bad"" as intrinsic properties of things all around us, and that's an anthropic view of things. I've noticed that quite a few learning resources portray overfitting as \*intrinsically bad\* and a plague to be avoided at all costs, at all times.Overfitting is a \*thing\*, albeit, a thing that makes models practically useless in the context of what we'd like to use them for. However, it's my understanding that overfitting can actually be a \*good sign\* before you finalize the tune a model to generalize well without overfitting data, as it can imply that the model has at least enough entropic capacity to actually generalize well.I've put this down in writing on the following link:\- [Machine Learning: Overfitting Is Your Friend, Not Your Foe](https://stackabuse.com/machine-learning-overfitting-is-your-friend-not-your-foe/)\*\*Disclaimer:\*\*  These are the musings of a man - flawed and prone to misjudgment. The point of writing this is to promote a discussion on the topic, not to be right or contrarian. If any glaring mistakes are present in the writing, *please* let me know.I've love to hear what some of the more experienced practitioners have to say about this take. If you have substantial experience in the field and would take a few minutes to read this and share your thoughts, I'd be sincerely grateful.",POS
257,pyeopo,[R] ByteDance Proposes â€˜DyStyleâ€™: A Novel Dynamic Neural Network For Style Editing,https://www.reddit.com/r/MachineLearning/comments/pyeopo/r_bytedance_proposes_dystyle_a_novel_dynamic/,techsucker,MachineLearning,1970-01-01 00:00:01.632986231,Research,0,1,"In the last few years, AI researchers have been using Generative adversarial networks (GANs) to create images with unprecedented levels of diversity and photorealism, and one such example is StyleGAN. When it comes to enhancing the semantic controllability of StyleGANs, some style manipulation methods worked well in manipulating the style codes on one attribute. At the same time, it was problematic when jointly manipulating multiple attributes.ByteDance (parent company of TikTok) introduces [Dynamic Style Manipulation Network (DyStyle)](https://arxiv.org/pdf/2109.10737.pdf), a dynamic neural network to solve the above problems. DyStyleâ€™s structure and parameters vary by input samples to get flexible and precise attribute control. DyStyle network utilizes a novel easy to hard training process for efficient and stable training of the network.# [4 Min Read](https://www.marktechpost.com/2021/09/30/bytedance-proposes-dystyle-a-novel-dynamic-neural-network-for-style-editing/) | [Paper](https://arxiv.org/pdf/2109.10737.pdf)| [Github](https://github.com/phycvgan/DyStyle/)&#x200B;https://reddit.com/link/pyeopo/video/cmjkjywjclq71/player",POS
258,py82y0,[R] Neural Hybrid Automata: Learning Dynamics with Multiple Modes and Stochastic Transitions,https://arxiv.org/abs/2106.04165,hardmaru,MachineLearning,1970-01-01 00:00:01.632960523,Research,3,1,,NEU
259,pxiuoa,Requirements for Deep Learning Engineer at Tesla [D],https://www.reddit.com/r/MachineLearning/comments/pxiuoa/requirements_for_deep_learning_engineer_at_tesla_d/,noxiousmomentum,MachineLearning,1970-01-01 00:00:01.632872193,Discussion,109,45,"I was going through the job listings for the [Deep Learning Engineer/Scientist at Tesla](https://www.tesla.com/en_GB/careers/search/job/autopilot-deeplearningengineer-scientist-48414), and it is unclear to me on how to educate myself to reach the requirements they have.I thought I'd confer with [r/MachineLearning](https://www.reddit.com/r/MachineLearning/) about this.Specifically, I really have no clue how to acquire expertise on the following topics:>An ideal candidate is very comfortable in cluster environments and understands the related computer systems concepts (CPU/GPU interactions/transfers, latency/throughput bottlenecks during training of neural networks, CUDA, pipelining/multiprocessing, etc).Could anyone point me towards resources to dig deep into the topics outlined above? What are some cluster environments used in industry? How do I get experience working with them?As to my level of knowledge, which I hope is similar to many of the readers who will benefit from the comments on this thread, is at the level of someone who is an MLE at some company in the US.At work, my duties involve data collection, data wrangling, training models (just using my 3090 or the company quadro, which is plenty for our use cases), and putting them up on high availability/low latency servers.",POS
260,pxqrbz,[R] Towards mental time travel: a hierarchical memory for reinforcement learning agents,https://arxiv.org/abs/2105.14039,hardmaru,MachineLearning,1970-01-01 00:00:01.632902680,Research,23,6,,NEU
261,pxsz3v,[R] CARLA: A Python Library to Benchmark Algorithmic Recourse and Counterfactual Explanation Algorithms,https://www.reddit.com/r/MachineLearning/comments/pxsz3v/r_carla_a_python_library_to_benchmark_algorithmic/,YoManHill,MachineLearning,1970-01-01 00:00:01.632913207,Research,10,5,"Abstract: Counterfactual explanations provide means for prescriptive model explanations by suggesting actionable feature changes (e.g., increase income) that allow individuals to achieve favourable outcomes in the future (e.g., insurance approval). Choosing an appropriate method is a crucial aspect for meaningful counterfactual explanations. As documented in recent reviews, there exists a quickly growing literature with available methods. Yet, in the absence of widely available openâ€“source implementations, the decision in favour of certain models is primarily based on what is readily available. Going forward â€“ to guarantee meaningful comparisons across explanation methods â€“ we present CARLA (Counterfactual And Recourse Library), a python library for benchmarking counterfactual explanation methods across both different data sets and different machine learning models. In summary, our work provides the following contributions: (i) an extensive benchmark of 11 popular counterfactual explanation methods, (ii) a benchmarking framework for research on future counterfactual explanation methods, and (iii) a standardized set of integrated evaluation measures and data sets for transparent and extensive comparisons of these methods. We have open sourced CARLA and our experimental results on [GitHub](https://github.com/carla-recourse/CARLA), making them available as competitive baselines. We welcome contributions from other research groups and practitioners.Paper link: [https://arxiv.org/abs/2108.00783](https://arxiv.org/abs/2108.00783)",POS
262,pxz1iw,[D] NLP or computer vision tasks where transformers are less recommended,https://www.reddit.com/r/MachineLearning/comments/pxz1iw/d_nlp_or_computer_vision_tasks_where_transformers/,HafsaCU,MachineLearning,1970-01-01 00:00:01.632932923,Discussion,4,6,Do you know an application or task (NLP or Computer vision) where Transformer-based models (like BERT) do not perform well? Thanks,NEG
263,pxwbt3,[D] [R] Spiking Neural Networks and multiplexing feedforward and feedback signals,https://www.reddit.com/r/MachineLearning/comments/pxwbt3/d_r_spiking_neural_networks_and_multiplexing/,Waste_Screen_5803,MachineLearning,1970-01-01 00:00:01.632925073,Discussion,6,0,Can anyone help me in finding examples or articles on SNNs and multiplexing feedforward and feedback signals similar to the following article?https://www.nature.com/articles/s41593-021-00857-x,POS
264,pxx3kv,[R] NYU & UNC Reveal How Transformersâ€™ Learned Representations Change After Fine-Tuning,https://www.reddit.com/r/MachineLearning/comments/pxx3kv/r_nyu_unc_reveal_how_transformers_learned/,Yuqing7,MachineLearning,1970-01-01 00:00:01.632927422,Research,4,0,"In the paper Fine-Tuned Transformers Show Clusters of Similar Representations Across Layers, a research team from New York University and the University of North Carolina at Chapel Hill uses centered kernel alignment (CKA) to measure the similarity of representations across layers and explore how fine-tuning changes transformers' learned representations. Here is a quick read: [NYU & UNC Reveal How Transformersâ€™ Learned Representations Change After Fine-Tuning.](https://syncedreview.com/2021/09/29/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-114/)The paper *Fine-Tuned Transformers Show Clusters of Similar Representations Across Layers* is on [arXiv](https://arxiv.org/abs/2109.08406).",NEU
265,py5h9k,[D] VGPNN Paper Explained - Diverse Generation from a Single Video Made Possible (5-minute summary),https://www.reddit.com/r/MachineLearning/comments/py5h9k/d_vgpnn_paper_explained_diverse_generation_from_a/,KirillTheMunchKing,MachineLearning,1970-01-01 00:00:01.632951914,Discussion,1,0,"Imagine a model that can take a  single video, and generate diverse high-quality variations of the input video, perform spatial and temporal retargeting, and even create video analogies, and do conditional video inpainting. All in a matter of seconds. From a single video. Let that sink in. Now get ready, because this model actually exists! VGPNN is introduced in a 2021 paper by Niv  Haim, Ben Feinstein, and the team at the Weizmann Institute of Science.  VGPNN uses a generative image patch nearest neighbor approach to put existing single video GANs to shame by reducing the runtime from days for low-res videos to minutes for Full-HD clips.Check out the [full paper summary](https://www.casualganpapers.com/patch-matching-single-video-input-diverse-video-generation/VGPNN-explained.html) on Casual GAN Papers (Reading time \~5 minutes).[VGPNN](https://i.redd.it/dqq4mojuiiq71.gif)Subscribe to [my channel](https://t.me/casual_gan) and follow me on [Twitter](https://twitter.com/KirillDemochkin) for weekly AI paper summaries!",POS
266,py4ofp,[D] How do you build the right incentive structure for data scientists/MLE/researchers?,https://www.reddit.com/r/MachineLearning/comments/py4ofp/d_how_do_you_build_the_right_incentive_structure/,ConfectionSafe954,MachineLearning,1970-01-01 00:00:01.632949480,Discussion,0,0,"In our organisation, DS/MLE/researchers wear multiple hats. We are starting to onboard more and I want to ensure the right incentive structure is in place.In data science teams, there is often a lot of work that might go into maintenance, data exploration etc and I am interested in exploring team incentives and structures such that if you were to automate the job and boost productivity, you can then spend your saved time in other areas that you find more exciting / interesting. I am a fanatic about productivity/tooling in data science and invest heavily in ensuring I do as much work as possible while leaving ample time to do what I really enjoy which is research and MLE. I actively maintain my own ecosystem of a few different tools from bash scripts to Python packages. I want this kind of culture in an org I am leading but was also wondering if there might be issues others might have ran into building this kind of org? Are there any suggestions that can help us improve our internal processes? I know some organisations have an intentional 1 day a week for internal development. Is that maybe a good way forward?",POS
267,pxossu,[P] AI Colorizes your old B/W photos [Neural-Image-Colorizer based on Autoencoders],https://www.reddit.com/r/MachineLearning/comments/pxossu/p_ai_colorizes_your_old_bw_photos/,Heisenberg_082001,MachineLearning,1970-01-01 00:00:01.632893566,Project,13,5,"Trained a deep learning model that will convert your grayscale photos into colorful mode&#x200B;https://preview.redd.it/jccbmbt8pdq71.png?width=817&format=png&auto=webp&s=53dda035dd77886643b2567d4990aa59511fdebe\-AutoEncoder Based Deep learning model that colorizes the old black and white imagesÂ· Used resnet18 for encoder part and up-sampled that latent representation in decoder partÂ· Deployed the simple webapp using streamlit on HerokuThough this is just a based model. I know some ways of improving like using good encoder , training for higher epochs .I would like to hear from you guysSuggestions are always welome :)git -[https://github.com/Pranav082001/Neural-Image-Colorizer](https://github.com/Pranav082001/Neural-Image-Colorizer)Medium BLog- [https://medium.com/@pranav.kushare2001/colorize-your-black-and-white-photos-using-ai-4652a34e967](https://medium.com/@pranav.kushare2001/colorize-your-black-and-white-photos-using-ai-4652a34e967)",POS
268,pxy9nc,[D] Varying action size with LSTM in RL,https://www.reddit.com/r/MachineLearning/comments/pxy9nc/d_varying_action_size_with_lstm_in_rl/,Winter_Lavishness328,MachineLearning,1970-01-01 00:00:01.632930750,Discussion,2,2,I have a resource allocation problem in hand. I want to use RL to solve it. Allocation problem is I have n number of devices each device has k slots. Each slot can get a number from 1 to z. Given that I thought action space for agent at time t as following: Pick a device and then assign a vector of size k or pick a device and then pick a slot and then assign a number. This enables agent to change a assignment for only single device at a time. I want to give agent a flexibility to decide the amount changes at each time step.\- One idea I have is after embedding the state use a LSTM like head to make assignments until LSTM says no assignment (like end of the sentence). But I couldn't think what this NN architecture would look like. Any idea is appreciated.\- Second problem is n and z is not constant. So policy has to adapt to changes in number of devices and numbers. How can I address this issue?,POS
269,py23ee,[N] Baidu AI Research Releases PLATO-XL: Worldâ€™s First Dialogue Generation (NLP) Model Pre-Trained On 11 Billion Parameter,https://www.reddit.com/r/MachineLearning/comments/py23ee/n_baidu_ai_research_releases_platoxl_worlds_first/,techsucker,MachineLearning,1970-01-01 00:00:01.632941834,News,0,0,"Artificial intelligence (AI) applications have a significant impact on our daily lives, making them easier. One of such applications is AI bots that are already proven effective in the automation of day-to-day tasks. These bots gather data and even imitate real-time human discussions, allowing humans to focus on more strategic activities.However, having clear, informative, and engaging conversations in the same manner that humans do is difficult for AI bots. Robots must build high-quality open-domain dialogue systems if they are to serve as emotional companions or intelligent assistants. As pre-training technology improves modelsâ€™ ability to learn from vast amounts of unannotated data, mainstream research concentrates on making better use of massive data to improve open-domain discussion systems.# [4 Min Read](https://www.marktechpost.com/2021/09/29/baidu-ai-research-releases-plato-xl-worlds-first-dialogue-generation-nlp-model-pre-trained-on-11-billion-parameter/) | [Paper](https://arxiv.org/abs/2109.09519) | [BAIDU Blog](http://research.baidu.com/Blog/index-view?id=163)&#x200B;https://preview.redd.it/7rh6fhlxohq71.png?width=1933&format=png&auto=webp&s=b738d89dfd03577ee9d1c49cffa3c47a9f7ba69d",POS
270,pxuchf,[D] Train a neural network with different training data than test data,https://www.reddit.com/r/MachineLearning/comments/pxuchf/d_train_a_neural_network_with_different_training/,mattvilain,MachineLearning,1970-01-01 00:00:01.632918560,Discussion,2,6,"Imagine  we have a classification task, lets say images of dogs and we have to  predict the dog breed. In our training samples we have the images and  also information about the dog in the picture like its size, the size of  its hair, its coat type ... (sorry if the example is not well chosen).  But in the test samples we only have the picture of the dog.My  question is, how can we train a model that will use the additional  information to extract more relevant information in the picture during  training, and can still make good predictions when given only the  picture ?I thought about kind of a  knowledge distillation where we train a model on all the information we  have and this model will teach another model that works only with the  images. Idk if this is a common SOTA problem and if knowledge  distillation can be used for that or if there is other way to approach  the problem. What do you think ?",NEG
271,px3hzd,[D] Has the ResNet Hypothesis been debunked?,https://www.reddit.com/r/MachineLearning/comments/px3hzd/d_has_the_resnet_hypothesis_been_debunked/,Ok_Slice4231,MachineLearning,1970-01-01 00:00:01.632825104,Discussion,303,76,"The **ResNet architecture** was invented to solve the *degradation problem* that has been empirically seen in Very Deep Neural Networks i.e. â€œ34-layer plain net has higher training error throughout the whole training procedure, even though the solution space of the 18-layer plain network is a subspace of that of the 34-layer one.â€œ&#x200B;https://preview.redd.it/7todt74s18q71.png?width=1314&format=png&auto=webp&s=0a1af475ef27fd8355d1527223e74556c6a993e9The natural presumption is that this problem is caused by the **Vanishing Gradient Problem** which has been observed in Recurrent Neural Networks and, to a lesser extent, in Long-Short Term Memory Networks. But the authors of the paper argue that this is, *most likely,* not the case:>We argue that this optimization difficulty is unlikely to be caused by vanishing gradients. These plain networks are trained with BN \[16\], which ensures forward propagated signals to have non-zero variances. We also verify that the backward propagated gradients exhibit healthy norms with BN. So neither forward nor backward signals vanish. In fact, the 34-layer plain net is still able to achieve compet- itive accuracy (Table 3), suggesting that the solver works to some extent. We conjecture that the deep plain nets may have exponentially low convergence rates, which impact the reducing of the training error3. The reason for such optimization difficulties will be studied in the future.I will refer to this as the **â€œResNet Hypothesisâ€**.Many recent papers and tutorials appear to be *assuming* that the ResNet Hypothesis is **false.** I have read numerous papers in which the authors add skip connections in order to *â€œimprove gradient flowâ€* and they cite the original ResNet paper to support this claim. While it is quite plausible that adding skip connections will improve gradient flow, what caused the degradation problem in the **first place**? The idea that skip connections solve the degradation problem by improving gradient flow seems to be in *clear contradiction* with the ResNet Hypothesis; so where did this idea come from? ***Has the ResNet Hypothesis been*** ***~~debunked~~*** ***falsified******?***Edit  1 - I think this is an accurate representation of the average DL researcherâ€™s (myself included) understanding of why ResNets work \[based on u/impossibleforkâ€™s  [answer](https://www.reddit.com/r/MachineLearning/comments/px3hzd/comment/hekymwd/?utm_source=share&utm_medium=web2x&context=3)\]:https://preview.redd.it/wbvvkwlrd8q71.png?width=724&format=png&auto=webp&s=7b3cac7d8baea007d9dd379ce67020a5a65e2c9bEdit 2 - Apologies for using the word â€debunkedâ€ in the original post. Quite irresponsible of me in hindsight. Thanks to /u/ComplexColor for [pointing it out](https://www.reddit.com/r/MachineLearning/comments/px3hzd/comment/hemcuq0/?utm_source=share&utm_medium=web2x&context=3). Unfortunately I am unable to edit the title.",NEG
272,pxquqg,[D] How to do cross validation and hyperparameter optimization using MLFlow?,https://www.reddit.com/r/MachineLearning/comments/pxquqg/d_how_to_do_cross_validation_and_hyperparameter/,quit_daedalus,MachineLearning,1970-01-01 00:00:01.632903157,Discussion,5,1,"Hi.I am trying to learn MLFlow, and so far I can create a single 'experiment' mlflow projects. But from now on, I want to cross validate my models if possible, and do hyperparameter tuning afterwards.How can this be done in MLFlow? How should I tweek my mlflow project so I could easily track cross validation metrics (in a scalable way?) and then package it together with a hyperparameter optimization component?Thank you!EDIT: I use Pytorch Lightning for training my model, and so far have used the mlflow module provided in the library for logging the training of my model.",POS
273,pxscfi,[P] Object detection framework : Detectron2 VS MMDetection,https://www.reddit.com/r/MachineLearning/comments/pxscfi/p_object_detection_framework_detectron2_vs/,Remet0n,MachineLearning,1970-01-01 00:00:01.632910412,Project,3,7,"The project I'm working on involve **object detection** and **single keypoint detection** (onto the object).* **Detectron2** documentation seems better, but the model zoo seems small.* **MMDetection** seems more difficult to use, but the model zoo seems very vast.* **MMdection** does not offer keypoint detection it seems.Anyone has some tipps on which framwork to choose ?",NEG
274,pxr6es,[D] Choosing a Recommender System algorithm,https://www.reddit.com/r/MachineLearning/comments/pxr6es/d_choosing_a_recommender_system_algorithm/,_Arsenie_Boca_,MachineLearning,1970-01-01 00:00:01.632904788,Discussion,3,8,"Hi,I need to choose an algorithm for a **recipe recommender system for supermarkets**. I feel a little overwhelmed by the wide variety of different algorithms. Also, I have somewhat specific requirements / circumstances, that might exclude many techniques:1. There is no explicit feedback or rating. I just have the purchase record on item-level per customer.2. I should take the user profile into account.3. I need to rerank / weight the results based on a heuristic. If the recommendation algo supports this, via weighting during recommendation, otherwise via reranking afterwards.4. I need to filter out specific recipes based on their ingredients -> probably done after the actual recommendation?I feel like 2, 3 and 4 are relatively normal requirements, whereas 1 might make this challenging.What do you think? What does your experienced gut feeling tell you? Content-based, collaborative filtering, hybrid, knowledge-based, ...?Thanks in advance!",POS
275,pxs9x3,"[D] Why does LIME, the post hoc XAI methodology start from instances randomly generated in the neighborhood of the instance to be explained?",https://www.reddit.com/r/MachineLearning/comments/pxs9x3/d_why_does_lime_the_post_hoc_xai_methodology/,bono_kr,MachineLearning,1970-01-01 00:00:01.632910081,Discussion,0,12,"Why does LIME, the post hoc XAI methodology start from instances randomly generated in the neighborhood of the instance to be explained?I think using original full train data could be much more helpful in the accurate explanation of the original model.Is there any evidence or metric which explains randomly generated data show much higher performance in either accuracy or fidelity?",POS
276,pxaqaq,[D] What are the GPT3 alternatives?,https://www.reddit.com/r/MachineLearning/comments/pxaqaq/d_what_are_the_gpt3_alternatives/,Outrageous-Credit-80,MachineLearning,1970-01-01 00:00:01.632848323,Discussion,25,8,Hi Reddit! Can anyone point me to public alternatives to GPT3?From my understanding only Open AI and Microsoft have access to GPT3. GPT2 has open source implementations through HuggingFace. Is this correct?Thanks for the help!,POS
277,pxp08v,[P] I fine-tuned T5 to generate clickbait titles...,https://www.reddit.com/r/MachineLearning/comments/pxp08v/p_i_finetuned_t5_to_generate_clickbait_titles/,Kamran_Santiago,MachineLearning,1970-01-01 00:00:01.632894490,Project,0,0,"    chubakbidpaa.com/cbgen (Colab Gist)One complaint I have received from people is that it only generates one sentence for each set of words. BUT it's a transformer, I'm pretty sure if I manipulated the language model heads somehow I could have made it so it could generate sentences based on conditions for example listicle or Youtube title but sadly the data was not labeled as such. I'm looking for valid ways to use transformers as TTS --- just use a padded tensor of samples instead of vector embeddings for labels. Also use the conditional heads . Do you think it's possible?",NEU
278,pxczp7,[D] Correspondence of Representations in Neural Networks,https://www.reddit.com/r/MachineLearning/comments/pxczp7/d_correspondence_of_representations_in_neural/,honeyandoatmeal,MachineLearning,1970-01-01 00:00:01.632854865,Discussion,2,3," I'm reading papers involving understanding intermediate representations which are learned in neural networks. The purpose of these methods is to be able to compare, say, two runs of the same model with different initializations, or two different models. Here are two particular papers:[https://arxiv.org/pdf/1511.07543.pdf](https://arxiv.org/pdf/1511.07543.pdf)[https://arxiv.org/pdf/1905.00414.pdf](https://arxiv.org/pdf/1905.00414.pdf)I'm hoping to see if anyone can confirm if my understanding is correct. The problem with directly comparing the weights of two models with different initializations is that the correspondence of the weights of these two models is not necessarily the same; that is, the features that are learned at a particular layer in the network need not ""line up"". From the first paper, Section 3: ""Due to symmetries in the architecture and weight initialization procedures, for any given parameter vector that is found, one could create many equivalent solutions simply by permuting the unit orders within a layer (and permuting the outgoing weights accordingly).""The way that I make sense of this is that in convolutional layers, a single output channel is computed by taking the convolution of all the input channels and the corresponding kernel (plus bias). Thus, the sum operation over all channels and kernel dimensions essentially mixes up all the information, so that any permutation of those channels would yield the same output channel. Is this correct?If this is the case, I'm confused about how the second paper works. In there proposed similarity metric, they essentially look at the activation maps that result from N different example inputs, and create an NxN (cosine) similarity matrix which represents how similar each example's corresponding activation map is to all the other example's activation maps. From their paper, Section 3: ""Our key insight is that instead of comparing multivariate features of an example in the two representations (e.g. via regression), one can first measure the similarity between every pair of examples in each representation separately, and then compare the similarity structures.""Why is it that these similarity structures can be directly compared when the weights of the network cannot, as explained in the first paper? Since the similarity structures are computed from the feature maps, does this imply that feature maps share correspondence across different networks? Why can't two networks learn a different ordering of feature maps for a particular layer? Thanks in advance for any insights.",POS
279,px4p6x,Telephone Pictionary with Image Synthesis (Pete-tionary) [Project],https://www.reddit.com/r/MachineLearning/comments/px4p6x/telephone_pictionary_with_image_synthesis/,Pbatch23888,MachineLearning,1970-01-01 00:00:01.632829904,Project,7,2,"**EDIT: Thanks for playing everyone. I've had to close the website for now due to hosting costs.**Hi guys,I've been working on a game which is a mixture of telephone pictionary and image synthesis.You can give it a try here: [https://pictionary.pbatch.net/](https://pictionary.pbatch.net/)You can view the code here: [https://github.com/Pbatch/pete-tionary](https://github.com/Pbatch/pete-tionary)The idea is that each player inputs a text prompt, which then generates a picture using image synthesis (CLIP + luna/chroma magic + GAN enhancement).This will take about **THIRTY** seconds, provided that the website isn't being battered to death.[A dog in a washing machine \(triple\)](https://preview.redd.it/ctdm4uf4e8q71.png?width=1012&format=png&auto=webp&s=14aff14e84bf6cd9f953b2ec43e7f933cbd12d83)Each player then selects the image that best matches their prompt.[A dog in a washing machine \(single\)](https://preview.redd.it/jms1che8e8q71.png?width=630&format=png&auto=webp&s=62076782cb0208025cf4e33febbe2d8426e29320)Once each player has done this, they pass their images on and receive a new image from another player.[the last meal on death row](https://preview.redd.it/cqo4djjde8q71.png?width=618&format=png&auto=webp&s=3ea7c9b16db63c160f71c4b8fc1d3a5ddab4d939)Each player must then try to write a prompt that matches that image. I.e. You might think that the image above looks like ""the last supper"".[The last supper](https://preview.redd.it/lwxw58rie8q71.png?width=1005&format=png&auto=webp&s=cdf75201db3306f4976979a475e4a071934bbc96)The game continues in this fashion until each player has seen each initial image once. You can then view all the stories you have made.[a dog in the washing machine \(final\)](https://preview.redd.it/nmxbc9ane8q71.png?width=811&format=png&auto=webp&s=65d3b774e2a1d73ed29b3d50f6cf5b228a9a1c5d)[the last meal on death row \(final\)](https://preview.redd.it/zhkm5qqoe8q71.png?width=826&format=png&auto=webp&s=39d9f5681fe089b0834005b98e6691f3aa9f32c3)If you're just interested in the algorithm, it is possible to play single player.If the game crashes or bugs out, reset your local storage and then log in again.I hope you have fun and please post any interesting pictures/stories that you create in the comments. Any feedback or advice is greatly appreciated too :)Cheers.",POS
280,px1gwr,"[D] What are the current ""sota"" approaches for audio cut detection?",https://www.reddit.com/r/MachineLearning/comments/px1gwr/d_what_are_the_current_sota_approaches_for_audio/,Slowai,MachineLearning,1970-01-01 00:00:01.632815841,Discussion,16,7,"Hello, hope y'all doing well :)I'm not sure if the title is informative enough so let me just give you the problem statement:""Given an audio recording, answer whether it was cut at some point""===========\\--------------      where = is audio 1, \\ is the cut and --- is audio 2.And example could be the first part of conversation happening inside, then instantly transitioning into same conversation, but now it's happening outside etc.There are few areas which do seem to be highly related to this, such as anomaly detection or segmentation, but I'm just not sure where should I look for this specific problem.I'm quite familiar with sequence modelling, but I haven't done any audio stuff, so I was hoping that some of you guys could help me :)Kind regards :3",POS
281,px1g7c,[D] Inference cost optimization of complex ML pipelines,https://www.reddit.com/r/MachineLearning/comments/px1g7c/d_inference_cost_optimization_of_complex_ml/,Medium_Ad_3555,MachineLearning,1970-01-01 00:00:01.632815742,Discussion,9,14,"I am running a CV pipeline with shared state and if-else conditional logic of a bunch of tf models based on ImageNet + Yolo.   Since  I am using real-time data (images/chunks of video) it's not a good use case for using GPU instances. All the inference/prediction is hosted on self-managed CPU-based C4. EC2 instances.  There are tradeoffs between running it as a monolithic pipeline (7 models per service) vs microservice deployment (1 model per service). Likewise, this pipeline has conditional activation of certain heavy, low fps models which consumes a bunch of CPU. The challenge is to balance IO operations  (input file downloads) vs CPU heavy lifting.  What I'm struggling with is finding the optimal instance and configuration for maximum utilization of CPU/IO and decent latency.   Does anyone have recommendations for libraries/frameworks/articles for these types of problems?",NEG
282,px6vc6,[D] Detecting Data Drift: NLP Models?,https://www.reddit.com/r/MachineLearning/comments/px6vc6/d_detecting_data_drift_nlp_models/,upulbandara,MachineLearning,1970-01-01 00:00:01.632837179,Discussion,3,2,"Hello Everyone,We have a couple of text classification models, and they are ready to put in production.Due to the unavailability of training data, we trained our models using synthetic data generated by a human subject matter expert (SME). Therefore, we suspect that training data distribution might mismatch with serving data distribution.So we are in the process of developing a simple model monitoring system. \[Our approach: Calculate text embedding for training data as well as serving data. Next, calculate cosine similarity to check how close our training and serving data\]We read a couple of articles/tutorials about model monitoring. However, non of them specifically talk about detecting data/model drifting in NLP systems such as text classification models.So we would like to know your recommendation when it comes to detecting data/model drifts.",POS
283,pxdkoo,[D] GANs for graph data,https://www.reddit.com/r/MachineLearning/comments/pxdkoo/d_gans_for_graph_data/,ElEiseinheim,MachineLearning,1970-01-01 00:00:01.632856548,Discussion,0,0,"I was wondering if it would be feasible to implement a GAN to inflate a limited dataset that consists of data curves. For more context, say I am using fuel consumption as input in graphical form and want to estimate the remaining mileage. From my limited GAN knowledge, I assume it would be infeasible to generate images for each and every mile remaining, but for say increments (+-50 miles), would this be possible?Any feedback is helpful!",POS
284,px8den,[D] Industry Conferences and Expos,https://www.reddit.com/r/MachineLearning/comments/px8den/d_industry_conferences_and_expos/,whata_wonderful_day,MachineLearning,1970-01-01 00:00:01.632841572,Discussion,2,4,"What are some of the best industry conferences & expos for ML, particularly around NLP and CV? Places where tooling vendors, ML deployment startups, etc go to & exhibit at and where there's content aimed at practioners.Also curious to know if there's much non-research stuff happening at conferences like ICML and ACL?",POS
285,pwju6t,"What are your thoughts on the ""Reward is enough"" hypothesis? (See paper attached) [D]",https://www.reddit.com/r/MachineLearning/comments/pwju6t/what_are_your_thoughts_on_the_reward_is_enough/,escapevelocitylabs,MachineLearning,1970-01-01 00:00:01.632757521,Discussion,133,45,"In this paper:[Reward is enough (Silver, Sutton et al.)](https://reader.elsevier.com/reader/sd/pii/S0004370221000862?token=49567D6C1DE8BA2C92A920E21910B2554BF9A206CE74D83E5041720E71C07A45FA1DA42D2ABD39046A2FC13177A022D2&originRegion=eu-west-1&originCreation=20210927153355)they hypothesize that Reinforcement Learning is enough for an agent to develop several facets of intelligence, including some that are normally studied with supervised learning techniques.Thoughts?",POS
286,px7fec,[D] Globally Optimum Sparse Decision Trees,https://www.reddit.com/r/MachineLearning/comments/px7fec/d_globally_optimum_sparse_decision_trees/,jj4646,MachineLearning,1970-01-01 00:00:01.632838813,Discussion,0,9,"https://arxiv.org/abs/1904.12847Has anyone ever heard about this algorithm before? Apparently this supposed to be one of the best ""individual decision tree algorithms"" out there.Has anyone used it before? What are your thoughts? How was your experience?Thanks!",POS
287,px4hfg,[D] Spectral Norm in GANs using residual blocks.,https://www.reddit.com/r/MachineLearning/comments/px4hfg/d_spectral_norm_in_gans_using_residual_blocks/,chasep255,MachineLearning,1970-01-01 00:00:01.632829058,Discussion,0,4,"Is it possible to use spectral normalization with a residual connections in my discriminator? From my understanding of how spectral norm works it constrains the linear transformations to have gradients less than one (or maybe exactly one I'm not sure.)  However, the gradient of a residual connection (x + f(x)) would be equal to 1 + df/dx(x).  Therefore even if I apply spectral norm to f(x) this does not guarantee lipschitz continuity.",NEG
288,pwmepa,[D] Inconsistency in Conference Peer Review: Revisiting the 2014 NeurIPS Experiment (Paper Explained),https://www.reddit.com/r/MachineLearning/comments/pwmepa/d_inconsistency_in_conference_peer_review/,downtownslim,MachineLearning,1970-01-01 00:00:01.632764879,Discussion,22,9,"[https://youtu.be/19Q-vMd9bYg](https://youtu.be/19Q-vMd9bYg)The peer-review system at Machine Learning conferences has come under much criticism over the last years. One major driver was the infamous 2014 NeurIPS experiment, where a subset of papers were given to two different sets of reviewers. This experiment showed that only about half of all accepted papers were consistently accepted by both committees and demonstrated significant influence of subjectivity. This paper revisits the data from the 2014 experiment and traces the fate of accepted and rejected papers during the 7 years since, and analyzes how well reviewers can assess future impact, among other things.&#x200B;OUTLINE:0:00 - Intro & Overview1:20 - Recap: The 2014 NeurIPS Experiment5:40 - How much of reviewing is subjective?11:00 - Validation via simulation15:45 - Can reviewers predict future impact?23:10 - Discussion & Comments&#x200B;Paper: [https://arxiv.org/abs/2109.09774](https://arxiv.org/abs/2109.09774)",POS
289,px85hy,[D] When Do You Use Apache Spark Monitoring?,https://www.reddit.com/r/MachineLearning/comments/px85hy/d_when_do_you_use_apache_spark_monitoring/,honorchan1,MachineLearning,1970-01-01 00:00:01.632840945,Discussion,0,0,"*We hear this a lot: how do we know what level of data observability to use in Apache Spark?*We're sharing our take on the topic. This week, [Databand.ai](https://databand.ai/) Senior Software Engineer, Vova Rozhkov, authored a piece on [**Apache Spark Monitoring**](https://databand.ai/blog/apache-spark-monitoring-using-listeners-and-data-quality-libraries/?utm_source=forum&utm_medium=r&utm_group=ml) and talks about how to use Spark API and open-source libraries to get better data observability of your application. If you ever wondered when to use low-level data observability vs. high-level data observability, this post will help you pick the best option based on the problem you're trying to solve.Let us know what you think and tell us if you approach this differently! :)",POS
290,pwk8pu,[D] Hosting a Machine Learning Model on the internet,https://www.reddit.com/r/MachineLearning/comments/pwk8pu/d_hosting_a_machine_learning_model_on_the_internet/,theahmedmustafa,MachineLearning,1970-01-01 00:00:01.632758646,Discussion,20,19,"I am a Machine Learning Engineer who has basically little to no knowledge or experience with Web Development.This idea of running my ML model on the internet has always been of interest to me but I have absolutely no idea how it can be achieved. Here is an example overview:I train a binary cats vs dogs classifier in keras or pytorch and write an inference script all on my local machine. Now I want to deploy this model on a website, say www.catordog.com, such that anyone on the internet can go to this url and upload a picture, and the results of the prediction on that image from my inference script are printed on the website.This sounds pretty basic to me on paper but my total inexperience with web dev makes me wonder if this should have been a ELI5 thread instead.Thanks.",POS
291,px2zgr,[D] Bring your own data AI SaaS service for non-programmers?,https://www.reddit.com/r/MachineLearning/comments/px2zgr/d_bring_your_own_data_ai_saas_service_for/,Over_Intention3342,MachineLearning,1970-01-01 00:00:01.632822959,Discussion,0,8,"Hi,Iâ€™m thinking on building a web service where one can import data, train AI models, and later use these models to make predictions on unseen data. It would accept any kind data that can be expressed as numbers. Unlike Azure or GCP it would be designed for non-programmers. Just point and click in the browser â€“ no experience in data science required. The only requirement: bring your own data. Would anyone be interested in something like this?To give a personal example: I have a list of transactions made by a stock trading system and I trained a model to predict which trades are likely to be profitable. My initial tests show that a typical Donchian channel system can be improved from 8% to 59% average trade profit (that is, by removing some trades) and accuracy can be improved from 31% to 57%.",POS
292,pwh3os,[D] NeurIPS Recommendations go out tomorrow. Post your scores and results once they come in!,https://www.reddit.com/r/MachineLearning/comments/pwh3os/d_neurips_recommendations_go_out_tomorrow_post/,beegica,MachineLearning,1970-01-01 00:00:01.632749392,Discussion,19,55,Good luck everyone! May the chair people's have mercy on your submissions.,POS
293,pwii5g,[R] NYU & UBC Propose Deviation-Based Learning to Advance Recommender System Training,https://www.reddit.com/r/MachineLearning/comments/pwii5g/r_nyu_ubc_propose_deviationbased_learning_to/,Yuqing7,MachineLearning,1970-01-01 00:00:01.632753666,Research,10,0,"A research team from New York University and the University of British Columbia proposes deviation-based learning, a novel approach for training recommender systems that learns user knowledge by observing whether they follow or deviate from recommendations. Here is a quick read: [NYU & UBC Propose Deviation-Based Learning to Advance Recommender System Training.](https://syncedreview.com/2021/09/27/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-112/)The paper *Deviation-Based Learning* is on [arXiv](https://arxiv.org/abs/2109.09816).",POS
294,pwmm0b,"[D][R] Is there a standard architecture for U-Nets, pixel-to-pixel models, VAEs, and the like?",https://www.reddit.com/r/MachineLearning/comments/pwmm0b/dr_is_there_a_standard_architecture_for_unets/,MrAcurite,MachineLearning,1970-01-01 00:00:01.632765460,Discussion,5,8,"I've looked through a bunch of papers that do some version of ""the output of a model is the same shape as the input,"" but typically the architecture that they use is either 1) never discussed in detail, or 2) something that varies significantly from other work.And then when I look through the models available from, say, torchvision.models, none are really set up for size-invariant tasks. No U-Nets, no VAEs, and the segmentation models usually downsample and then do some kind of interpolation upsampling at the end, instead of transpose convolution layers.Is there a standard architecture for these sorts of tasks? I've just been kind of making it up as I go along up until now, but I'd like to be able to produce models that are more directly comparable with work by other people.",NEG
295,pwnsc2,[R] PLATO-XL: Exploring the Large-scale Pre-training of Dialogue Generation,https://www.reddit.com/r/MachineLearning/comments/pwnsc2/r_platoxl_exploring_the_largescale_pretraining_of/,kizumada,MachineLearning,1970-01-01 00:00:01.632768806,Research,5,0,"Abstract: To explore the limit of dialogue generation pre-training, we present the models of PLATO-XL with up to 11 billion parameters, trained on both Chinese and English social media conversations. To train such large models, we adopt the architecture of a unified transformer with high computation and parameter efficiency. In addition, we carry out multi-party aware pre-training to better distinguish the characteristic information in social media conversations. With such designs, PLATO-XL successfully achieves superior performances as compared to other approaches in both Chinese and English chitchat. We further explore the capacity of PLATO-XL on other conversational tasks, such as knowledge grounded dialogue and task-oriented conversation. The experimental results indicate that PLATO-XL obtains state-of-the-art results across multiple conversational tasks, verifying its potential as a foundation model of conversational AI.Paper link: [https://arxiv.org/abs/2109.09519](https://arxiv.org/abs/2109.09519)",POS
296,pvyvet,[R] Graph Neural Networks for Point Cloud Processing,https://i.redd.it/ln38lqj30wp71.png,pinter69,MachineLearning,1970-01-01 00:00:01.632679241,Research,361,20,,NEU
297,pwoz23,[D] IC-GAN Paper Explained - Instance-Conditioned GAN (5-minute summary),https://www.reddit.com/r/MachineLearning/comments/pwoz23/d_icgan_paper_explained_instanceconditioned_gan/,KirillTheMunchKing,MachineLearning,1970-01-01 00:00:01.632772171,Discussion,4,2,"Arenâ€™t you tired of only seeing generated FFHQ-like faces? I bet you are, and if you know just how atrocious the samples from StyleGAN-2  trained on other datasets such as ImageNet really look you should be wildly excited to see Instance Conditioned GAN (IC-GAN) by Arantxa  Casanova and the team at Facebook AI Research! IC-GAN flips the script and uses unaligned images to condition the generator to synthesize samples similar to the input data points. This approach can be thought of as learning overlapping local distributions around the input images,  which lets it train on diverse unaligned images while maintaining the latent space density needed for high-quality image synthesis.Check out the [full paper summary](https://www.casualganpapers.com/unaligned-image-class-conditional-generation/Instance-Conditioned-GAN-explained.html) on Casual GAN Papers (Reading time \~5 minutes).[IC-GAN](https://preview.redd.it/7tp8v7fdo3q71.png?width=1320&format=png&auto=webp&s=1399cb6eae607055a4e7a97e9c088b49893ee2a2)Subscribe to [my channel](https://t.me/casual_gan) and follow me on [Twitter](https://twitter.com/KirillDemochkin) for weekly AI paper summaries!",POS
298,pw6vsy,[D] Machine Learning PhD coming from Industry,https://www.reddit.com/r/MachineLearning/comments/pw6vsy/d_machine_learning_phd_coming_from_industry/,InfamousApathy,MachineLearning,1970-01-01 00:00:01.632706175,Discussion,83,70,"Another one of these posts..More seriously, I joined a FAANG company as a ML Engineer about 2 years ago as a new grad. And it sucks, a lot. Itâ€™s all feature engineering and data pipelining without much thought given to actual ML algorithms.I want to do algorithm work, so Iâ€™m headed back to school. The one thing thatâ€™s holding me back is the opportunity cost. Yes I know, do a PhD for your passion, yada yada. But itâ€™s fucking hard to say bye to 250k/year TC at mid-level.All the discussion around here about financials for PhD graduates seem to be pretty doom and gloom, â€œitâ€™s never financially worth it,â€ â€œclimbing the corporate ladder is more efficient.â€ Can anyone offer a competing perspective? I really donâ€™t see myself doing what Iâ€™m doing for another 2-3 years, but at the same time I donâ€™t want to be completely set behind my peers (friends) financially.",NEG
299,pwqcg4,"[D] In a Lava World, should training penalty be infinite or finite?",https://www.reddit.com/r/MachineLearning/comments/pwqcg4/d_in_a_lava_world_should_training_penalty_be/,beezlebub33,MachineLearning,1970-01-01 00:00:01.632776054,Discussion,2,7,"I am working on an agent that goes around in a 2D environment, trying to reach a goal.  The agent has to avoid lava (like [https://arxiv.org/abs/1711.09883](https://arxiv.org/abs/1711.09883)).  Will training be faster if the agent dies (infinite penalty) or have their score negatively affected (finite penalty) for going into the lava, or does it not matter?   Does it depend on the algorithm that is used?   Does it matter if it is on-policy or off-policy?",NEG
300,pwq7vh,[P] Question about confidence scores in deep CNNs,https://www.reddit.com/r/MachineLearning/comments/pwq7vh/p_question_about_confidence_scores_in_deep_cnns/,chazy07,MachineLearning,1970-01-01 00:00:01.632775674,Project,0,4,"Hello, I am currently working on an image classification project. When I'm visualizing the outputs of my predictions in the validation set, it gives me a prediction of a class and a probability of the prediction being correct. For some of the predictions, I am getting 100% probability/confidence. Does that mean that my model is overfitting?",NEU
301,pw97o1,[R] Compressing Large-Scale Transformer-Based Models: A Case Study on BERT,https://www.reddit.com/r/MachineLearning/comments/pw97o1/r_compressing_largescale_transformerbased_models/,prakharg24,MachineLearning,1970-01-01 00:00:01.632715035,Research,10,0,"Updates on our work on Compressing Large-Scale Transformer-Based Models: A Case Study on BERT ([https://www.reddit.com/r/MachineLearning/comments/fc6sfb/r\_compressing\_largescale\_transformerbased\_models/](https://www.reddit.com/r/MachineLearning/comments/fc6sfb/r_compressing_largescale_transformerbased_models/))After multiple iterations of improvements and including new research works to keep it up to date, our paper was recently **published by TACL** ([https://direct.mit.edu/tacl/article/doi/10.1162/tacl\_a\_00413/107387/Compressing-Large-Scale-Transformer-Based-Models-A](https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00413/107387/Compressing-Large-Scale-Transformer-Based-Models-A)).Hopefully, this can help new NLP researchers get a better understanding of the field. We welcome your feedback.",POS
302,pw2lwc,"[P][D] Visualizing ""zebra -stripes +spots"" using CLIP embedding math",https://www.reddit.com/r/MachineLearning/comments/pw2lwc/pd_visualizing_zebra_stripes_spots_using_clip/,rmxz,MachineLearning,1970-01-01 00:00:01.632691076,Discussion,30,8,"Math on CLIP embeddings can help visualize when/why CLIP considers images to be similar.I created [this github project](https://github.com/ramayer/rclip-server) to visualize CLIP embedding math on databases from the excellent [rclip](/r/MachineLearning/comments/pb6ime/p_rclip_use_clip_to_search_for_your_photos_in_the/) project that /u/39dotyt posted here last month.   A live demo of this system using Wikimedia images can be seen [here](http://image-search.0ape.com).Some interesting results:* [zebra -stripes +spots](http://image-search.0ape.com/search?q=zebra%20-stripes%20%2Bspots) \- Animals that look kinda like zebras but with spots instead of stripes.* [zebra -mammal +fish](http://image-search.0ape.com/search?q=zebra%20-mammal%20%2Bfish) \- Animals that look like zebras but fish instead of mammals.* [zebra -animal +car](http://image-search.0ape.com/search?q=zebra%20-animal%20%2Bcar) \- Objects colored like zebras but more cars than animals.* [zebra -""black and white""](http://image-search.0ape.com/search?q=zebra%20-%22black%20and%20white%22) \- Baby zebras (brown & white) and a Greater Kudu (a brown & white striped 4-legged animal). Of course you could also find the same baby zebra searching for [zebra -big +small](http://image-search.0ape.com/search?q=zebra%20-big%20%2Bsmall) or even more simply, just [baby zebra](http://image-search.0ape.com/search?q=baby%20zebra).* [furry black and white striped animal](http://image-search.0ape.com/search?q=furry%20black%20and%20white%20striped%20animal) \- zebras, lemurs, and other furry black and white animals.* [striped horse-like animal](http://image-search.0ape.com/search?q=striped%20horse-like%20animal) \- more zebras (and horses with stripes)* [zebra habitat -zebra](http://image-search.0ape.com/search?q=zebra%20habitat%20-zebra) \- places that look like somewhere a zebra might liveIt can also do a search based on the difference between the CLIP embeddings of two images directly.  For example, CLIP considers [this image of a spider on a purple flower](http://image-search.0ape.com/search?q=%7B%22image_id%22%3A28754%7D) minus [this image of the same kind of spider on a white flower](http://image-search.0ape.com/search?q=%7B%22image_id%22%3A174054%7D) to be [this set of pictures which is mostly purple flowers without the spider](http://image-search.0ape.com/search?q=%7B%22image_id%22%3A28754%7D%20-%7B%22image_id%22%3A174054%7D).I find this useful for trying to understand what concepts CLIP considers similar and why.",POS
303,pw0xfr,[P] Deep Reinforcement Learning in Rocket League. Objective for the AI - drive as fast as possible.,https://streamable.com/0wxvhz,Roboserg,MachineLearning,1970-01-01 00:00:01.632685707,Project,33,7,,NEU
304,pw14z5,[D] Machine Learning - WAYR (What Are You Reading) - Week 122,https://www.reddit.com/r/MachineLearning/comments/pw14z5/d_machine_learning_wayr_what_are_you_reading_week/,ML_WAYR_bot,MachineLearning,1970-01-01 00:00:01.632686405,Discussion,22,7,"This is a place to share machine learning research papers, journals, and articles that you're reading this week. If it relates to what you're researching, by all means elaborate and give us your insight, otherwise it could just be an interesting paper you've read.Please try to provide some insight from your understanding and please don't post things which are present in wiki.Preferably you should link the arxiv page (not the PDF, you can easily access the PDF from the summary page but not the other way around) or any other pertinent links.Previous weeks :|1-10|11-20|21-30|31-40|41-50|51-60|61-70|71-80|81-90|91-100|101-110|111-120|121-130||----|-----|-----|-----|-----|-----|-----|-----|-----|------|-------|-------|-------||[Week 1](https://www.reddit.com/4qyjiq)|[Week 11](https://www.reddit.com/57xw56)|[Week 21](https://www.reddit.com/60ildf)|[Week 31](https://www.reddit.com/6s0k1u)|[Week 41](https://www.reddit.com/7tn2ax)|[Week 51](https://reddit.com/9s9el5)|[Week 61](https://reddit.com/bfsx4z)|[Week 71](https://reddit.com/d7vno3)|[Week 81](https://reddit.com/f1f0iq)|[Week 91](https://reddit.com/hlt38o)|[Week 101](https://reddit.com/k81ywb)|[Week 111](https://reddit.com/myg8sm)|[Week 121](https://reddit.com/pmzx3g)||||||||||||[Week 2](https://www.reddit.com/4s2xqm)|[Week 12](https://www.reddit.com/5acb1t)|[Week 22](https://www.reddit.com/64jwde)|[Week 32](https://www.reddit.com/72ab5y)|[Week 42](https://www.reddit.com/7wvjfk)|[Week 52](https://reddit.com/a4opot)|[Week 62](https://reddit.com/bl29ov)|[Week 72](https://reddit.com/de8h48)|[Week 82](https://reddit.com/f8fs6z)|[Week 92](https://reddit.com/hu6zq9)|[Week 102](https://reddit.com/kh27nx)|[Week 112](https://reddit.com/n8m6ds)|||[Week 3](https://www.reddit.com/4t7mqm)|[Week 13](https://www.reddit.com/5cwfb6)|[Week 23](https://www.reddit.com/674331)|[Week 33](https://www.reddit.com/75405d)|[Week 43](https://www.reddit.com/807ex4)|[Week 53](https://reddit.com/a8yaro)|[Week 63](https://reddit.com/bqlb3v)|[Week 73](https://reddit.com/dkox1s)|[Week 83](https://reddit.com/ffi41b)|[Week 93](https://reddit.com/iaz892)|[Week 103](https://reddit.com/kpsxtc)|[Week 113](https://reddit.com/njfsc6)|||[Week 4](https://www.reddit.com/4ub2kw)|[Week 14](https://www.reddit.com/5fc5mh)|[Week 24](https://www.reddit.com/68hhhb)|[Week 34](https://www.reddit.com/782js9)|[Week 44](https://reddit.com/8aluhs)|[Week 54](https://reddit.com/ad9ssz)|[Week 64](https://reddit.com/bw1jm7)|[Week 74](https://reddit.com/dr6nca)|[Week 84](https://reddit.com/fn62r1)|[Week 94](https://reddit.com/ijjcep)|[Week 104](https://reddit.com/kzevku)|[Week 114](https://reddit.com/ntu6lq)|||[Week 5](https://www.reddit.com/4xomf7)|[Week 15](https://www.reddit.com/5hy4ur)|[Week 25](https://www.reddit.com/69teiz)|[Week 35](https://www.reddit.com/7b0av0)|[Week 45](https://reddit.com/8tnnez)|[Week 55](https://reddit.com/ai29gi)|[Week 65](https://reddit.com/c7itkk)|[Week 75](https://reddit.com/dxshkg)|[Week 85](https://reddit.com/fvk7j6)|[Week 95](https://reddit.com/is5hj9)|[Week 105](https://reddit.com/l9lvgs)|[Week 115](https://reddit.com/o4dph1)|||[Week 6](https://www.reddit.com/4zcyvk)|[Week 16](https://www.reddit.com/5kd6vd)|[Week 26](https://www.reddit.com/6d7nb1)|[Week 36](https://www.reddit.com/7e3fx6)|[Week 46](https://reddit.com/8x48oj)|[Week 56](https://reddit.com/ap8ctk)|[Week 66](https://reddit.com/cd7gko)|[Week 76](https://reddit.com/e4nmyk)|[Week 86](https://reddit.com/g4eavg)|[Week 96](https://reddit.com/j0xr24)|[Week 106](https://reddit.com/ljx92n)|[Week 116](https://reddit.com/odrudt)|||[Week 7](https://www.reddit.com/52t6mo)|[Week 17](https://www.reddit.com/5ob7dx)|[Week 27](https://www.reddit.com/6gngwc)|[Week 37](https://www.reddit.com/7hcc2c)|[Week 47](https://reddit.com/910jmh)|[Week 57](https://reddit.com/auci7c)|[Week 67](https://reddit.com/cj0kyc)|[Week 77](https://reddit.com/eb4lxk)|[Week 87](https://reddit.com/gcx3uf)|[Week 97](https://reddit.com/j9cbfs)|[Week 107](https://reddit.com/luqbxl)|[Week 117](https://reddit.com/omy345)|||[Week 8](https://www.reddit.com/53heol)|[Week 18](https://www.reddit.com/5r14yd)|[Week 28](https://www.reddit.com/6jgdva)|[Week 38](https://www.reddit.com/7kgcqr)|[Week 48](https://reddit.com/94up0g)|[Week 58](https://reddit.com/azjoht)|[Week 68](https://reddit.com/cp1jex)|[Week 78](https://reddit.com/ehbfst)|[Week 88](https://reddit.com/glm6sv)|[Week 98](https://reddit.com/jhzz9v)|[Week 108](https://reddit.com/m52u5z)|[Week 118](https://reddit.com/ovz52j)|||[Week 9](https://www.reddit.com/54kvsu)|[Week 19](https://www.reddit.com/5tt9cz)|[Week 29](https://www.reddit.com/6m9l1v)|[Week 39](https://www.reddit.com/7nayri)|[Week 49](https://reddit.com/98n2rt)|[Week 59](https://reddit.com/b50r5y)|[Week 69](https://reddit.com/cvde5a)|[Week 79](https://reddit.com/entcxy)|[Week 89](https://reddit.com/gu5t0d)|[Week 99](https://reddit.com/jqjgo2)|[Week 109](https://reddit.com/mf8m6u)|[Week 119](https://reddit.com/p50knh)|||[Week 10](https://www.reddit.com/56s2oa)|[Week 20](https://www.reddit.com/5wh2wb)|[Week 30](https://www.reddit.com/6p3ha7)|[Week 40](https://www.reddit.com/7qel9p)|[Week 50](https://reddit.com/9cf158)|[Week 60](https://reddit.com/bakew0)|[Week 70](https://reddit.com/d1g1k9)|[Week 80](https://reddit.com/euctyw)|[Week 90](https://reddit.com/hddf7j)|[Week 100](https://reddit.com/jz3evt)|[Week 110](https://reddit.com/moy40m)|[Week 120](https://reddit.com/pe2idh)||Most upvoted papers two weeks ago:/u/CatalyzeX_code_bot: [Paper link](https://arxiv.org/abs/2012.09841)/u/Significant_Worth_84: [High-Fidelity GAN Inversion for Image Attribute Editing](https://arxiv.org/abs/2109.06590)Besides that, there are no rules, have fun.",POS
305,pvz25t,[P] UpliftML: A python library for uplift modeling that handles webscale datasets,https://github.com/bookingcom/upliftml,TaXxER,MachineLearning,1970-01-01 00:00:01.632679811,Project,30,5,,NEU
306,pw7cfv,[R] Dropout's Dream Land: Generalization from Learned Simulators to Reality,https://arxiv.org/abs/2109.08342,hardmaru,MachineLearning,1970-01-01 00:00:01.632707838,Research,6,2,,NEU
307,pvwsjo,"[R] The Autodidactic Universe: â€œWe present an approach to cosmology in which the Universe learns its own physical laws. It does so by exploring a landscape of possible laws, which we express as a certain class of matrix models.â€",https://arxiv.org/abs/2104.03902,hardmaru,MachineLearning,1970-01-01 00:00:01.632672497,Research,33,27,,NEU
308,pvs8r5,[D] Facebook Visdom vs Google Tensorboard for Pytorch,https://www.reddit.com/r/MachineLearning/comments/pvs8r5/d_facebook_visdom_vs_google_tensorboard_for/,arsenale,MachineLearning,1970-01-01 00:00:01.632656590,Discussion,53,42,"I'm using pytorch and I'm looking for a visualization framework. I wonder if Visdom is a dead project and if it's better to just use Tensorboard, which is compatible with pytorch and tensorflow.",NEG
309,pw7f3y,[R] Torch.manual_seed(3407) is all you need: On the influence of random seeds in deep learning architectures for computer vision,https://arxiv.org/abs/2109.08203,hardmaru,MachineLearning,1970-01-01 00:00:01.632708087,Research,2,10,,NEU
310,pvwbdb,[P] Robust High-Res Video Matting with Temporal Guidance(Code and Pretrained Models),https://github.com/PeterL1n/RobustVideoMatting,binaryfor,MachineLearning,1970-01-01 00:00:01.632670950,Project,21,0,,NEU
311,pwc26w,[P] Looking for an Online Bounding Box Annotation Tool with Good Collaboration Tools,https://www.reddit.com/r/MachineLearning/comments/pwc26w/p_looking_for_an_online_bounding_box_annotation/,sarmientoj24,MachineLearning,1970-01-01 00:00:01.632727087,Project,0,6,"We have a project for creating an object detection database where we have a supervisor/domain expert and number of annotators. What's the best annotation tool for annotating bounding boxes in an image with the following features:* can be paid or not* online tool* has ""supervisory"" capabilities where the owner can supervise annotations of people* annotations from other annotators can be seen by others* easy-to-use* about 5-7 annotators* images ranging from 10K-50K",POS
312,pvgpfl,"[N][D][R] Alleged plagiarism of â€œImprove Object Detection by Label Assignment Distillation.â€ (arXiv 2108.10520) by ""Label Assignment Distillation for Object Detection"" (arXiv 2109.07843). What should I do?",https://www.reddit.com/r/MachineLearning/comments/pvgpfl/ndr_alleged_plagiarism_of_improve_object/,chuong98,MachineLearning,1970-01-01 00:00:01.632607833,Discussion,328,67,"Hi everyone,So, just a month ago, we were shocked by the [plagiarism alarm](https://www.reddit.com/r/MachineLearning/comments/p59pzp/d_imitation_is_the_sincerest_form_of_flattery/?utm_source=share&utm_medium=web2x&context=3):>the article â€œ**Momentum residual neural networks**â€ by Michael Sander, Pierre Ablin, Mathieu Blondel and Gabriel PeyrÃ©, published at the ICML conference in 2021, hereafter referred to as â€œPaper Aâ€, has been plagiarized by the paperÂ â€œ**m-RevNet: Deep Reversible Neural Networks with Momentum**â€ by Duo Li and Shang-Hua Gao, accepted for publication at the ICCV conference, hereinafter referred to as â€œPaper Bâ€.Today, I found out that our paper (still in conference review) is also severely plagiarized by: ""Minghao Gao, Hailun Zhang (1), Yige Yan (2) ((1) Beijing Institute of Technology, (2) Hohai University)* Our paper:  Improve Object Detection by *Label Assignment Distillation*. [https://arxiv.org/abs/2108.10520](https://arxiv.org/abs/2108.10520)* Their paper: Label Assignment Distillation for Object Detection, [https://arxiv.org/abs/2109.07843](https://arxiv.org/abs/2109.07843)Our paper was first submitted to the conference on Jun 9 2021, and we upload to Arxiv on Aug 24 2021. We show the proof of plagiarism in our Open Github: [https://github.com/cybercore-co-ltd/CoLAD\_paper/blob/master/PlagiarismClaim/README.md](https://github.com/cybercore-co-ltd/CoLAD_paper/blob/master/PlagiarismClaim/README.md)**Updated:** [The issue is resolved.](https://www.reddit.com/r/MachineLearning/comments/pvgpfl/comment/hech52t/?utm_source=share&utm_medium=web2x&context=3) Thanks all for your help, especially [**zyl1024**](https://www.reddit.com/user/zyl1024/) and Jianfeng Wang [**wjfwzzc**](https://www.reddit.com/user/wjfwzzc/) (the Author of original NIPS version draft). We want to close this post, and go back to our normal work. Hope this can serve as a reference should you encounter this problem in the future.**Updated 2:** The official emails between me and  Jianfeng Wang can be found at:[https://github.com/cybercore-co-ltd/CoLAD\_paper/blob/master/PlagiarismClaim/ConfirmLetter.pdf](https://github.com/cybercore-co-ltd/CoLAD_paper/blob/master/PlagiarismClaim/ConfirmLetter.pdf)Best Regard !!!",POS
313,pvuboo,[D] Which approaches for hyperparameter optimization with bayesian optimization can handle continuous and discrete variables?,https://www.reddit.com/r/MachineLearning/comments/pvuboo/d_which_approaches_for_hyperparameter/,MrAchillesTurtle,MachineLearning,1970-01-01 00:00:01.632664610,Discussion,17,2,"So far it is clear, that one-hot encoding with Gaussian Processes or some surrogate models such as Tree Parzen-estimators and Random Forests can naturally handle categorical as well as real-values variables when used for hyperparameter optimization with Gaussian Processes.I want to optimize a search space of mixed variables. Which other approaches are there?For Gaussian Processes I found the following helpful reference:[Dealing with categorical and integer-valued variables in Bayesian Optimization with Gaussian Processes](https://www.sciencedirect.com/science/article/abs/pii/S0925231219315619?casa_token=eisknxvQ608AAAAA:UMq0POv1xZiyOmfx3tH1ooiAUBMrWdeU4PUCJJgq9po_ngXs992MvVw-2j5b9POMrfiS22NhrbWb)But how about approaches such as Neural Networks?#",POS
314,pvsxs6,[D] What are the intended differences between the EMNLP and ACL conferences?,https://www.reddit.com/r/MachineLearning/comments/pvsxs6/d_what_are_the_intended_differences_between_the/,Enamex,MachineLearning,1970-01-01 00:00:01.632659514,Discussion,16,2,"ACL conference (2022): https://2022.aclweb.org/EMNLP conference (2021): https://2021.emnlp.org/call-for-papersBoth have similar topic domains, are hosted by the ACL, and are well known in their subfield. I'm confused what happened to the ACL conference that made it non-yearly, and how EMNLP fits there?Some of the confusion comes from the fact that I've not noticed a major difference in the distribution of topics of published papers from both.",NEG
315,pw029j,How easy is it to switch between subfields in AI? [Discussion],https://www.reddit.com/r/MachineLearning/comments/pw029j/how_easy_is_it_to_switch_between_subfields_in_ai/,want0sl33p,MachineLearning,1970-01-01 00:00:01.632682956,Discussion,2,4,"Iâ€™m about to graduate with an MS in Statistics and am looking to work in ML research after graduation. Iâ€™d like to know if itâ€™s easy to switch between different ML subfields mid career (eg. NLP to Computer Vision and vice versa). In other words, how important is it for me to settle down on a subfield right after college? Thanks in advance.",POS
316,pw2bd2,[R] Significance of class vector vs class distribution in multiclass classification?,https://www.reddit.com/r/MachineLearning/comments/pw2bd2/r_significance_of_class_vector_vs_class/,AbIgnorantesBurros,MachineLearning,1970-01-01 00:00:01.632690136,Research,3,2,"Before I start researching this, thought I'd ask in case folks have insights and can point me in the right direction:My question: does the logical structure of the output vector relative to the label distribution play a role in a classifier performance?Example: a classifier trained in imagenet. There are dozens of classes generally related to cats (and cat-like things, like felines). Those classes will have lots of similar/shared features and activations. Would the converged model perform different if those various cat classes were all placed together in the output layer (say, units 0-n were all the cats in the output layer) vs all n cat classes were placed in the output layer very sparsely (say, every 23 classes in the output layer is a cat class)?",POS
317,pvvhuu,[D] Simple Questions Thread,https://www.reddit.com/r/MachineLearning/comments/pvvhuu/d_simple_questions_thread/,AutoModerator,MachineLearning,1970-01-01 00:00:01.632668418,Discussion,6,134,Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!Thread will stay alive until next one so keep posting after the date in the title.Thanks to everyone for answering questions in the previous thread!,POS
318,pw12pl,[D] Decision Trees based on Evolutionary Algorithms,https://www.reddit.com/r/MachineLearning/comments/pw12pl/d_decision_trees_based_on_evolutionary_algorithms/,SQL_beginner,MachineLearning,1970-01-01 00:00:01.632686198,Discussion,1,1,"Recently I came across some links in which it's claimed that in certain cases, the performance of the standard CART Decision Tree might be improved by using Evolutionary Algorithms (e.g. the Genetic Algorithm).- https://www.jstatsoft.org/article/view/v061i01 (R package)- https://towardsdatascience.com/evolutionary-decision-trees-when-machine-learning-draws-its-inspiration-from-biology-7d427fa7554bThe argument being, that standard decision trees only consider a very small search space of the data, and it is quite likely that standard decision trees will produce ""sub-optimal"" decision trees. On the other hand, decision trees that are created using  evolutionary algorithms can search over a far greater space, and thus have the potential to produce better decision trees (e.g. greater predictive power).Has anyone ever used decision trees based on evolutionary algorithms before? How has your experience been? Did you find that there were any improvements when compared to the standard decision tree?Thanks!",POS
319,pvpc67,[D] InfoGAN how is the lemma applied?,https://www.reddit.com/r/MachineLearning/comments/pvpc67/d_infogan_how_is_the_lemma_applied/,tt19234,MachineLearning,1970-01-01 00:00:01.632642476,Discussion,5,1,"Hi allI am reading the [InfoGAN paper](https://arxiv.org/pdf/1606.03657.pdf) and I can understand how the lemma (attached below) is proved.[The lemma](https://preview.redd.it/03f3iwxiysp71.png?width=3544&format=png&auto=webp&s=ab413713630d87aaa5865020b15f51c51965ca79)The original paper uses the lemma in this way: first it finds out that (equation 4 in the paper)&#x200B;[equation 4](https://preview.redd.it/d480rapnysp71.png?width=3500&format=png&auto=webp&s=9a422241d14253a2f74c037786304fc99515be6f)I can understand this part, however, I am not sure how the lemma is applied later to produce the lower bound $L\_I(G, Q)$ (equation 5 in the paper)&#x200B;[equation 5](https://preview.redd.it/tzirt3wqysp71.png?width=2270&format=png&auto=webp&s=4fc3cfc856b36dab0510871a30f0cb9b64c37e4f)How did the author derive equation 5 from equation 4 using the lemma? More specifically, how do we get line 2 from line 1 of the equation 5?  Thank you in advance!",POS
320,pvi5n5,[P] OpenAI Codex Shell Plugin,https://www.reddit.com/r/MachineLearning/comments/pvi5n5/p_openai_codex_shell_plugin/,tomd_96,MachineLearning,1970-01-01 00:00:01.632613023,Project,14,3,&#x200B;https://i.redd.it/dn972wd0jqp71.gif  You can now let Zsh write code for you using the plugin I wrote: [https://github.com/tom-doerr/zsh\_codex](https://github.com/tom-doerr/zsh_codex)All you need to provide is a comment or a variable name and the plugin will use OpenAI's Codex AI (powers GitHub Copilot) to write the corresponding code.Be aware that you do need to get access to the Codex API.,NEU
321,puz9kw,[R] LoFTR: Detector-Free Local Feature Matching with Transformers,https://v.redd.it/s7o35jnupkp71,Illustrious_Row_9971,MachineLearning,1970-01-01 00:00:01.632542675,Research,499,28,,NEU
322,pvqe9n,[D] Machine Learning Videos with Background History,https://www.reddit.com/r/MachineLearning/comments/pvqe9n/d_machine_learning_videos_with_background_history/,soozie_,MachineLearning,1970-01-01 00:00:01.632647769,Discussion,2,1,I watched mit ocw videos for Artificial Intelligence by Patrick Winston. They were amazing as he provides the intuition as well as the journey of how the algorithms change over time and how they have come to current form. Can the community suggest some similar videos.Link to mit youtube videos : https://m.youtube.com/playlist?list=PLUl4u3cNGP63gFHB6xb-kVBiQHYe_4hSi,POS
323,pvk1ki,[R] Knowledge is reward: Learning optimal exploration by predictive reward cashing,https://arxiv.org/abs/2109.08518,hardmaru,MachineLearning,1970-01-01 00:00:01.632619968,Research,9,4,,NEU
324,pvvpt1,[PROJECT] Indox - text summarization engine,https://www.reddit.com/r/MachineLearning/comments/pvvpt1/project_indox_text_summarization_engine/,alexadar,MachineLearning,1970-01-01 00:00:01.632669076,Project,0,6,"Hi all!  Iâ€™ve developed a cutting-edge summarization engine and want to start a company that will provide AI services to customers. I dropped an article on medium [https://medium.com/@OlexanderKorenyuk/indox-summarizaton-engine-b2fc49864ddf](https://medium.com/@OlexanderKorenyuk/indox-summarizaton-engine-b2fc49864ddf) .If you like, please, look at it, demo area on a website will be very appreciated for a feedback  Thanks!",POS
325,pvgca6,[R] AI Research Group From NVIDIA Unveils An Advanced Framework To Estimate Physically Correct Human Motions,https://www.reddit.com/r/MachineLearning/comments/pvgca6/r_ai_research_group_from_nvidia_unveils_an/,techsucker,MachineLearning,1970-01-01 00:00:01.632606575,Research,12,0,"**Key takeaways:*** In this research, a new framework was introduced for training motion synthesis models from raw video pose estimations without using any of the expensive and time-consuming processes.* The proposed framework refines noisy pose estimates by enforcing physics constraints through contact invariant optimization, including the computation of contact forces.Â * Time-series generative model was then trained on the refined poses, synthesizing both future motion and contact forces.* The detailed analysis of the results demonstrated significant performance boosts in pose estimation via a physics-based refinement and motion synthesis from video.# [5 Min Read](https://www.marktechpost.com/2021/09/25/ai-research-group-from-nvidia-unveils-an-advanced-framework-to-estimate-physically-correct-human-motions/) | [Paper](https://arxiv.org/pdf/2109.09913.pdf)| [Project](https://nv-tlabs.github.io/physics-pose-estimation-project-page/)&#x200B;https://i.redd.it/wob9xuyzzpp71.gif",POS
326,pvqfr6,[R][ICML 2021 Spotlight] DFAC Framework: Factorizing the Value Function via Quantile Mixture for Multi-Agent Distributional Q-Learning,https://www.reddit.com/r/MachineLearning/comments/pvqfr6/ricml_2021_spotlight_dfac_framework_factorizing/,ElsaLab,MachineLearning,1970-01-01 00:00:01.632647990,Research,0,0,"We provided a distributional perspective on value function factorization methods, and introduced a framework, called DFAC, for integrating distributional RL with MARL domains. We achieve State-of-the-art performance on the 5 Super Hard scenarios in the SMAC benchmark.Advanced detail please visit: [https://reurl.cc/Yj9RqD](https://l.facebook.com/l.php?u=https%3A%2F%2Freurl.cc%2FYj9RqD%3Ffbclid%3DIwAR1Y4jsbX-pqbgpVgxG8kUGxIYGGVHk1y3hIhOcoMdtIv_LTUHYCdODoBRs&h=AT0N1iprRu3hMnnM8_lFPw3LOoQveVtb60yZEaMb3BdrtYBTyISWc_SSi3CXidjdytk-0SFQUgzkFHs7pUW2j7gHKOz3mCtzaSyre5z5evgVs8eGNSZNSlR0MuX8VJRkfjycuJQzBw&__tn__=-UK-R&c[0]=AT0psWKdlIhocuNag2iWrDZyQBLNi7HU51EsjQxgb46fjQ48xTW0-a8WO5ztHpukHZzo-AT0Oa0-u1QkjHWvN7BAX0PkPcpS4t1PeQ4bMkV2h__Jzj7YFpl1gZYpr1EDSljoCrXiC-n4VOE7Y0N91VkSbgKhDprXcXFCY_NVaOQI9oyhJ5osIMlmTWrZCiexAc2ysos)  Paper Download: [https://reurl.cc/Xl4NR0](https://reurl.cc/Xl4NR0?fbclid=IwAR0agqQ1y4GLAGP1-3-uaQPVIz39BoyCk4IT4MLBOt0D5bf--jQD_Uml8Ao)  GitHub: [https://reurl.cc/EZpL6k](https://reurl.cc/EZpL6k?fbclid=IwAR3qg5D0KEuQLen5TEJGuvujcks3nyKvZgBhgnf7e8PGr4HQN9-ryfYUe_I)  Presentation Video: [https://reurl.cc/82WL6j](https://reurl.cc/82WL6j?fbclid=IwAR34X78i6ib7_9qqPVzHqe-vJfwe-chmUYbGSuxzp-gnMK3EuivMVeB1CMo)  Demonstration Video: [https://reurl.cc/vge4pL](https://reurl.cc/vge4pL?fbclid=IwAR1Dhyq6slpjNHWtJfUcbEfgiUSsQMlVyUlLYsx-T9VVKXMf9SQ56a6W5D8)ELSA Lab is a research laboratory focusing on Deep Reinforcement Learning, Intelligent Robotics, and Computer Vision. Please visit our website: [https://elsalab.ai/](https://elsalab.ai/?fbclid=IwAR1ZYAD-z-tAwXX4powvEIeyspyfCaV5pBmJ1Wq6JXbOSMrmcOg4x-R_TXU)[\#ArtificialIntelligence](https://www.facebook.com/hashtag/artificialintelligence?__eep__=6&__cft__[0]=AZV462EvjWp8sHwMjUKLVl9ZGsGInltrdWnVsXTD3FLt9uFRUlDuwZfBaP32pH2rofb5zY3PVMijeFOMb5_GOsHnv98ufj1TqbD9nkn9gfmOLTibwo-7UbubNuh9TdxLzBvQQ29is17pihQtk9s3RPd7A69k7EMgnmr3vpmsdo3hABKC3gK3L99Fpdp0tAd7BhE&__tn__=*NK-R) [\#MachineLearning](https://www.facebook.com/hashtag/machinelearning?__eep__=6&__cft__[0]=AZV462EvjWp8sHwMjUKLVl9ZGsGInltrdWnVsXTD3FLt9uFRUlDuwZfBaP32pH2rofb5zY3PVMijeFOMb5_GOsHnv98ufj1TqbD9nkn9gfmOLTibwo-7UbubNuh9TdxLzBvQQ29is17pihQtk9s3RPd7A69k7EMgnmr3vpmsdo3hABKC3gK3L99Fpdp0tAd7BhE&__tn__=*NK-R) [\#DeepLearning](https://www.facebook.com/hashtag/deeplearning?__eep__=6&__cft__[0]=AZV462EvjWp8sHwMjUKLVl9ZGsGInltrdWnVsXTD3FLt9uFRUlDuwZfBaP32pH2rofb5zY3PVMijeFOMb5_GOsHnv98ufj1TqbD9nkn9gfmOLTibwo-7UbubNuh9TdxLzBvQQ29is17pihQtk9s3RPd7A69k7EMgnmr3vpmsdo3hABKC3gK3L99Fpdp0tAd7BhE&__tn__=*NK-R) [\#ReinforcementLearning](https://www.facebook.com/hashtag/reinforcementlearning?__eep__=6&__cft__[0]=AZV462EvjWp8sHwMjUKLVl9ZGsGInltrdWnVsXTD3FLt9uFRUlDuwZfBaP32pH2rofb5zY3PVMijeFOMb5_GOsHnv98ufj1TqbD9nkn9gfmOLTibwo-7UbubNuh9TdxLzBvQQ29is17pihQtk9s3RPd7A69k7EMgnmr3vpmsdo3hABKC3gK3L99Fpdp0tAd7BhE&__tn__=*NK-R)",POS
327,pvg1pd,[P] Vizcom - ML enabled creative tools for artists and designers,https://www.vizcom.co/,binaryfor,MachineLearning,1970-01-01 00:00:01.632605573,Project,6,1,,NEU
328,pw4065,"[D] Why do people thing AI can do stuff such as generate functioning code? In fact why do people think ""AI"", whatever that is, can do EVERYTHING? Isn't that what people believed in the 80s which led to it falling out of favor?",https://www.reddit.com/r/MachineLearning/comments/pw4065/d_why_do_people_thing_ai_can_do_stuff_such_as/,Kamran_Santiago,MachineLearning,1970-01-01 00:00:01.632695873,Discussion,0,14,"I just had an argument with someone who genuinely believed generation of Python code can be automated... I mean how are you going to embed the variable and function names in a vector when they can be anything??  I mean it takes a team of senior devs to read a bad Python code. But a code generated by AI? People thinking coding works like human language aside, how is it even possible to make an AI that ""programs""? That's some AGI shit.I've been in ML world for 4 years now and honestly the demands people have from AI is ridiculous. They want outright sentiency from a black box that takes some numbers, passes it through a series of functions, and outputs another number. If you REALLY think about neural networks and even shallow networks you'll see how ridiculously simple they are and they are entirely dependent on the data you feed it.What scares me is that when we get to the maximum efficiency of ML at its current state, funding's gonna stop. I mean look at OpenAI Jukebox, state-of-art VQ-VAE song generator. At its current state AI can't even generate music, let alone, code. What happens when rich people who fund these researches realize it's not magic?I dunno. I know mods are gonna delete this thread because they delete every thread that isn't promoting some cheap crowd-sourced paper that's copy of another paper. I just thought I should say it.",NEG
329,pvlsyj,[P] I made FaceShop! Instance segmentation + CGAN for editing faces (badly),https://www.reddit.com/r/MachineLearning/comments/pvlsyj/p_i_made_faceshop_instance_segmentation_cgan_for/,greentfrapp,MachineLearning,1970-01-01 00:00:01.632626509,Project,4,0,"Built with a mix of instance segmentation and conditional GANs, and is heavily inspired by the recent DeepSIM paper for training with thin-plate-spline (TPS) augmentation.Model Architectures:* [BiSeNet](https://github.com/zllrunning/face-parsing.PyTorch)* [Pix2PixHD](https://github.com/NVIDIA/pix2pixHD) (from [DeepSIM](https://github.com/eliahuhorwitz/DeepSIM))Play with it here! [tinymodels.io/faceshop](https://tinymodels.io/faceshop) (All models are running on CPU so might be a little slow)",POS
330,pv5nrb,[P][D] Simulation: Life as a Survival Optimization Problem Complicated by Local Maxima,https://www.reddit.com/r/MachineLearning/comments/pv5nrb/pd_simulation_life_as_a_survival_optimization/,brainxyz,MachineLearning,1970-01-01 00:00:01.632572202,Discussion,27,7,"As someone who came to the Machine Learning world from a Medical background, I couldn't help not relating **being stuck at a Local Maximum** to many other life situations. In fact, not a day in my life passes without getting into a situation where I'm not blaming Local Maxima! So I have decided to make a simulation project that helps to visualize this problem from a biological and also a political perspective where liberals and conservatives compete to reach the Global Maximum.  A video explanation of the simulation is here: [https://youtu.be/1p11-oggW1E](https://youtu.be/1p11-oggW1E)[ Liberal vs. Conservative \(Simulation\)](https://preview.redd.it/zeu4f7wf9np71.png?width=1919&format=png&auto=webp&s=6a18713561e42d2ca1e072090ea4585bfdb1f4a9)[Increasing External Threats Gradually to aid reaching Global Maximum](https://i.redd.it/06yt2k7fzmp71.gif)The geographical representation is a very nice way to visualize and relate the topics. For instance, sexual reproduction can be seen as a way to share information between two Local Maxima which can help in reducing the search space. Internal struggle and Existential threats can also be seen as means to help in reaching the Global Maximum.One can even relate this problem to politics! For instance, you can think of a Conservative as someone with a small step changing-rate while a Liberal as someone with a large step adaptation-rate. Political systems like Anarchy can be seen as a situation where no one sticks to any Local Maxima whereas fascism can be seen as everyone sticks to a single Local Maximum.[Liberal vs. Conservative](https://i.redd.it/kljwaht2ymp71.gif)The purpose of this simulation is not to present the state-of-the-art algorithms but to bridge and link the terms that are used in the Machine Learning world and the ones that are used in the biological and the political world by making these toy simulations and easily digestible videos to both parties. I believe the Machine learning world can give so much to the other fields because life in essence is a survival optimization problem where everything is complicated by being stuck at a Local Maximum.Code:[https://github.com/hunar4321/Reaching-global-maximum](https://github.com/hunar4321/Reaching-global-maximum)  Video:[https://youtu.be/1p11-oggW1E](https://youtu.be/1p11-oggW1E)",POS
331,pvlwem,[D] Realistic Text to Image AI,https://www.reddit.com/r/MachineLearning/comments/pvlwem/d_realistic_text_to_image_ai/,JMG518,MachineLearning,1970-01-01 00:00:01.632626899,Discussion,0,10,"Hey guys! I'm attempting to employ text-to-image AI for to generate images of a parking garage and other buildings for inspiration in an architecture project. I have tried to use Clip+VQGAN, Clip+Glass, and Big Sleep all using Google Colab. Unfortunately, my results haven't been realistic. What text-to-image AI is best to use to create realistic images? Also, what are your thoughts on running code using the program ""IntelliJ""? Thanks!",POS
332,pv5sgg,"[News] VGPNN: Generate Video Variations - No dataset or deep learning required, Only Nearest Neighbors!",https://youtu.be/Uy8yKPEi1dg,OnlyProggingForFun,MachineLearning,1970-01-01 00:00:01.632572728,News,16,4,,NEU
333,pvl2wj,[Discussion] Outliers and Missing data,https://www.reddit.com/r/MachineLearning/comments/pvl2wj/discussion_outliers_and_missing_data/,riley_habrakenisacow,MachineLearning,1970-01-01 00:00:01.632623787,Discussion,0,3,"Hi!In a dataset I (non-stats student) have been given (I have no access to raw data), I'm running into a few problems...1. there are two items in the Age variable that are 840 and 580. Given that modifying these values to 84 and 58 is an assumption, is it better to delete these cases or make the value missing in order to retain information for the other variables (which is valid).2. There is missing data in the gender variable... is it fair to recode these instances to 4= 'Prefer not to say'?Thank you in advance for any help! :)",POS
334,pvaix9,[D] Motivation of Contrastive Predictive Coding,https://www.reddit.com/r/MachineLearning/comments/pvaix9/d_motivation_of_contrastive_predictive_coding/,eleswon,MachineLearning,1970-01-01 00:00:01.632588253,Discussion,7,2,"I am going through literature on unsupervised/self-supervised learning and was stuck on the motivation behind CPC as described in ""Representation Learning with Contrastive Predictive Coding"" by Oord et al. From the paper,>""One of the challenges of predicting high-dimensional data is that unimodal losses such as mean-squared error and cross-entropy are not very useful, and powerful conditional generative models which need to reconstruct every detail in the data are usually required. But these models are computationally intense, and waste capacity at modeling the complex relationships in the data x, often ignoring the context c. For example, images may contain thousands of bits of information while the high-level latent variables such as the class label contain much less information (10 bits for 1,024 categories). This suggests that modeling p(x|c) directly may not be optimal for the purpose of extracting shared information between x and c.""Given the context that this paper focuses on time-series prediction, I have a couple questions about this motivation.1. By ""conditional generative models"" are they referring to models like CGANs?2. Why would modeling p(x|c) not promote shared information between x and c? I think I understand their example when the context contains less information, but if a generative model was conditioned on higher-dimensional data, would it necessarily ignore that context as well?",NEG
335,pvgv66,[R] About time intervals in Neural Networks,https://www.reddit.com/r/MachineLearning/comments/pvgv66/r_about_time_intervals_in_neural_networks/,Inudir,MachineLearning,1970-01-01 00:00:01.632608402,Research,0,1,"Hi all,First time posting here, sorry if I'm missing something.I'm researching intrusion detection and prevention systems for network based traffic using recurrent neural networks, and I can't seem to understand one of what I'm assuming is pretty much a key concept on the matter.One of the key factors for these types of systems is identifying traffic like denial of service attacks, but in order to succeed identifying them, it needs information like ""how many packets similar to these are we receiving in a arbitrary interval of time"". This type of information is not present by default on a typical network sniffer - but timestamps are.I've seen a few colleagues just using the timestamps of each network packet as input in the RNN, but isn't this irrelevant? How is the Neural Network gonna work with timestamps when they are literally an ever growing number that never repeats itself? I know we can reduce the number for faster processing but even so it doesn't make sense to me.In my head, what makes sense is implementing something like a counter for packets with the same origin\_IP-destination\_IP pair that resets after something like a few minutes without another hit. This way, the neural network can use the number of times similar traffic has happened as an efficient input. I know this will probably add too much overhead processing in the program logic but a problem at a time lolAm I wrong about assuming that a RNN doesn't care about timestamps as an input? Also, if I simply don't use any temporal or counter reference as an input, will the RNN correctly guess if the traffic is malicious? (in the denial of service scenario).Sorry if I'm missing something basic, haven't worked with neural networks for long. Thanks!",POS
336,puxsbg,[N][CfP] AI for Design and Manufacturing Workshop (ADAM) @ AAAI 2022,https://www.reddit.com/r/MachineLearning/comments/puxsbg/ncfp_ai_for_design_and_manufacturing_workshop/,Ingenuity39,MachineLearning,1970-01-01 00:00:01.632536796,News,20,0,"Hello [r/MachineLearning](https://www.reddit.com/r/MachineLearning/)!Advances in complex engineering systems such as manufacturing and materials synthesis increasingly seek artificial intelligence/machine learning (AI/ML) solutions to enhance their design, development, and production processes. However, despite increasing interest from various subfields, AI/ML techniques are yet to fulfill their full promise in achieving these advances. Key obstacles include lack of high-quality data, difficulty in embedding complex scientific and engineering knowledge in learning, and the need for high-dimensional design space exploration under constrained budgets.This workshop aims to bring together researchers from core AI/ML, design, manufacturing, scientific computing, and geometric modeling. Our intent is to facilitate new AI/ML advances for core engineering design, simulation, and manufacturing. Outcomes include outlining the main research challenges in this area, cross-pollinating collaborations between AI researchers and domain experts in engineering design and manufacturing, and sketching open problems of common interest.Paper submission begins on October 1st and we invite paper submissions on the following (and related) topics:* New theory and fundamentals of AI-aided design and manufacturing,* Novel AI-based techniques to improve modeling of engineering systems,* Integration of AI-based approaches with engineering prototyping and manufacturing,* Novel methods to learn from scarce/sparse, or heterogenous, or multimodal data,* Novel ML methods in the computational material and physical sciences,* Novel ML-accelerated optimization for conceptual/detailed system design,* Novel AI-enabled generative models for system design and manufacturing,* ML-guided rare event modeling and system uncertainty quantification,* Development of software, libraries, or benchmark datasets, and* Identification of key challenges and opportunities for future research.Workshop website:Â [https://adam-aaai2022.github.io/](https://adam-aaai2022.github.io/)Submission website:Â [https://openreview.net/group?id=AAAI.org/2022/Workshop/ADAM](https://openreview.net/group?id=AAAI.org/2022/Workshop/ADAM)Submission deadline: November 12, 2021",POS
337,puy5xf,[R] Pix2seq: A Language Modeling Framework for Object Detection. â€œWe simply cast object detection as a language modeling task conditioned on pixels!â€,https://arxiv.org/abs/2109.10852,hardmaru,MachineLearning,1970-01-01 00:00:01.632538287,Research,15,8,,NEU
338,puo96c,[N] BITS AI Symposium 2021,https://www.reddit.com/r/MachineLearning/comments/puo96c/n_bits_ai_symposium_2021/,three-wise-monkeys,MachineLearning,1970-01-01 00:00:01.632503604,News,58,0,"We invite the AI Community to the BITS AI Symposium 2021, organised by [SAiDL](https://www.saidl.in/) in association with [APPCAIR](https://www.bits-pilani.ac.in/appcair/), the AI Research Centre at BITS Pilani The symposium is targeted towards those in the Student AI Community based in India. It will be a two-day virtual event focused on exposing students to key trends and direction in modern AI, providing insights on how to get started in the field and promoting interaction amongst the student AI community featuring -* **Conversations** on the future of AI with leaders in the field from IBM, TCS, BCG and IIT Delhi and Amazon.* **Talks** on how to get started in AI by BITS Alumni who are up and coming researchers and practitioners at Stanford, CMU, Twitter and more!* A **virtual social gathering** where you can meet and connect with others within the student AI community.Dates: **2nd & 3rd October 2021**More details about the event including the how to register, full speaker lineup and the schedule can be found on our website: [sites.google.com/view/ai-symposium-2021](https://sites.google.com/view/ai-symposium-2021)**Registration** for attending the event can be found at [https://forms.gle/LayhfTppTF4mGzFG8](https://forms.gle/LayhfTppTF4mGzFG8).Please share with those for whom this may be relevent. Looking forward to seeing you there.SAiDL",POS
339,pugf3n,[D] Schmidhuber's critique of the 2021 Turing lecture,https://www.reddit.com/r/MachineLearning/comments/pugf3n/d_schmidhubers_critique_of_the_2021_turing_lecture/,PaganPasta,MachineLearning,1970-01-01 00:00:01.632476868,Discussion,182,123,The tweet: https://twitter.com/SchmidhuberAI/status/1441296380623040512?s=19The article: https://people.idsia.ch/~juergen/scientific-integrity-turing-award-deep-learning.htmlI feel there's some good history of the field in there regardless of your stance on the topic. ðŸ¿,POS
340,pulhkf,[R] Googleâ€™s Zero-Label Language Learning Achieves Results Competitive With Supervised Learning,https://www.reddit.com/r/MachineLearning/comments/pulhkf/r_googles_zerolabel_language_learning_achieves/,Yuqing7,MachineLearning,1970-01-01 00:00:01.632495285,Research,65,7,"A Google AI research team explores zero-label learning (training with synthetic data only) in natural language processing, and introduces Unsupervised Data Generation (UDG), a training data creation procedure designed to synthesize high-quality training data without human annotations. Here is a quick read: [Googleâ€™s Zero-Label Language Learning Achieves Results Competitive With Supervised Learning.](https://syncedreview.com/2021/09/24/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-111/)The paper *Towards Zero-Label Language Learning* is on [arXiv](https://arxiv.org/abs/2109.09193).",POS
341,puxq1v,[R] Recursively Summarizing Books with Human Feedback,https://arxiv.org/abs/2109.10862,dojoteef,MachineLearning,1970-01-01 00:00:01.632536553,Research,6,2,,NEU
342,pvbg0w,[Research] Mark Data for Decision Trees and Reinforcement Learning in Mt4 or Mt5,https://www.reddit.com/r/MachineLearning/comments/pvbg0w/research_mark_data_for_decision_trees_and/,holguinmoraFX,MachineLearning,1970-01-01 00:00:01.632590857,Research,0,0,"HiIn order to start making some experiments I will like to start marking some entry and exit points for trading purposes in a 2 year data base, in time frames of 1min, 5 min, 15 min and 1hour for EURUSD in Mt4 / Mt5In order to mark entry and exit points:Is it possible to download data from 2 years in Mt4 or Mt5?Is it possible to work in the platform Mt4 or Mt5 2 years back, some how mark the points and export the data? If so, how can it be done? How can I export 2 years of data? How can I mark the data? Can I mark the data using vertical lines?Or the only way is to upload the data bases into Mt4 / Mt5 jump back into the a specific data point, and in parallel mark the entries in the .csv / excel file?",POS
343,punjuy,[D] Thinking of Quitting an ML PhD - Chances for another PhD position?,https://www.reddit.com/r/MachineLearning/comments/punjuy/d_thinking_of_quitting_an_ml_phd_chances_for/,throwaway69827065,MachineLearning,1970-01-01 00:00:01.632501485,Discussion,22,12,"Hello everyone and sorry that this question is not about current ML research. I am currently involved in a PhD position which involves ML, but is heavily application focused - I am looking for advice from the ML community for my situation. In our lab, we have two positions for the project I am involved in. The two positions involve two different domains with just a slight overlap. The other position for this project did not produce usable results or was vacant for a long time, so I had to do tasks for both positions for this project. Note that the project itself is heavily application focused. The task we currently carry out do not involve any ML research which is worth of a publication in a noteworthy conference. I am strongly considering quitting this position, as even though I have a new colleague, I still get immense pressure from project partners (mostly private companies) to participate / fix studies of the other domain, for which my collegague is primarely responsible. This leaves almost no room to produce research in my area which can be published in an ML conference on top of my project work. The pressure of project partners  also negatively affects my mental health, as they like to put the blame on us if there is a lack of progress in the project. This is unfortunate for me, because I wanted to work as an ML researcher in the future, for which an PhD position is a somewhat hard requirement. I've also read that PhD positions in ML are hard to come by, e.g., one can wait for several years until one gets accepted in a suitable position. So if I would quit this position, realistically, I would work some years as an ML engineer while looking for other PhD positions.I wanted to ask if quitting an ML PhD would be a hard pass for other ML PhD positions. Ideally, I'd like to finish my PhD of my ML subdomain in position in which I can allot more time to research and also feel less pressured to carry-out non-ML-related project work.",POS
344,puxb7w,[Project]Best way to generate 2D datasets from 3D images?,https://www.reddit.com/r/MachineLearning/comments/puxb7w/projectbest_way_to_generate_2d_datasets_from_3d/,EcstaticComplaint733,MachineLearning,1970-01-01 00:00:01.632534962,Project,3,9,"Hi All,I was just wondering is there an effective way to rapidly generate multiple 2D images from an existing 3D model for object recognition training? I have seen a lot of 2D to 3D conversion tools but was wondering if they other way around is a common approach to reduce the number of photos I need to take. Thanks!",POS
345,pulcre,"[Research][Project] Muzic, an open-source research project on AI music, by researchers from Microsoft Research Asia",https://www.reddit.com/r/MachineLearning/comments/pulcre/researchproject_muzic_an_opensource_research/,tobyoup,MachineLearning,1970-01-01 00:00:01.632494871,Research,23,1,"**Muzic** ([https://github.com/microsoft/muzic](https://github.com/microsoft/muzic)) is a research project on AI music that empowers music understanding and generation with deep learning and artificial intelligence. Muzic was started by [some researchers](https://www.microsoft.com/en-us/research/project/ai-music/) from [Microsoft Research Asia](https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/).&#x200B;Muzic has a logo in both image and video version:[Muzic Logo in Image](https://preview.redd.it/3b9r6701rgp71.png?width=1383&format=png&auto=webp&s=3930ffa0f8b7a31b4ab9c2b3602d1106e9e18690)[Muzic Logo in Video ](https://reddit.com/link/pulcre/video/ktc1byv9rgp71/player)We summarize the scope of our Muzic project in the following figure:[Muzic Concept Map](https://preview.redd.it/u3avdk93rgp71.png?width=7000&format=png&auto=webp&s=4bc6207266fbd6fd6173cf6b065c8daf1d501339)&#x200B;The current work in [Muzic](https://github.com/microsoft/muzic) include:* Music Understanding   * Symbolic Music Understanding: [MusicBERT](https://arxiv.org/pdf/2106.05630.pdf)   * Automatic Lyrics Transcription: [PDAugment](https://arxiv.org/pdf/2109.07940.pdf)* Music Generation   * Song Writing: [SongMASS](https://arxiv.org/pdf/2012.05168.pdf)   * Lyric Generation: [DeepRapper](https://arxiv.org/pdf/2107.01875.pdf)   * Melody Generation: [TeleMelody](https://arxiv.org/pdf/2109.09617.pdf)   * Accompaniment Generation: [PopMAG](https://arxiv.org/pdf/2008.07703.pdf)   * Singing Voice Synthesis: [HiFiSinger](https://arxiv.org/pdf/2009.01776.pdf)&#x200B;We initially release the code of 5 research work: [MusicBERT](https://github.com/microsoft/muzic/blob/main/musicbert), [PDAugment](https://github.com/microsoft/muzic/blob/main/pdaugment), [DeepRapper](https://github.com/microsoft/muzic/blob/main/deeprapper), [SongMASS](https://github.com/microsoft/muzic/blob/main/songmass), and [TeleMelody](https://github.com/microsoft/muzic/blob/main/telemelody). We will release the code of more research work in the future.&#x200B;Welcome to follow Muzic project via our Github Repo: [https://github.com/microsoft/muzic](https://github.com/microsoft/muzic) !",POS
346,puesf5,"[D] Ranking top companies, universities, and countries by ICML and NeurIPS accepted papers",https://www.reddit.com/r/MachineLearning/comments/puesf5/d_ranking_top_companies_universities_and/,AdditionalWay,MachineLearning,1970-01-01 00:00:01.632468548,Discussion,62,24,"https://chuvpilo.medium.com/ai-research-rankings-2020-can-the-united-states-stay-ahead-of-china-61cf14b1216I'm surprised about IBM. Their stock has been declining for a decade. I wonder how they keep attracting talented researchers. Last year they split Cloud + AI from legacy. IBM Watson made big splashes decades ago with Deep Blue and Watson. But they seem to have issues developing great products.I would never have guessed the rankings of the Chinese companies. I would have thought Baidu would be number one, I remember they tried to hire Jeff Hinton a decade ago, and at one point hired Andrew Ng. My guess would have been Baidu, Tencent, Alibaba, and I wouldn't have even thought of Huawei.Here's the 2019 rankingshttps://miro.medium.com/max/4800/1*hBDSPwH-DfFUz0BQ21DbAg.pngHuawei was 19 last year.Some other observations comparing 2019 and 2020.Nvidia has jumped a lot. I'm not surprised, they been doing a ton of research.Toyota is not on there anymore. Last year they were 11. But Volkswagen is on there this year, thought near the bottom.Samsung raised by a lot compared to last year.JP Morgan is the only big bank in the rankings.Twitter doesn't seem to put too much into it, considering how much text and user info they have. I would have imagined they would be very interested in NLP + Rec sys.The publications per capita is really interesting.  The Swiss have around 2x than the next couple countries.",POS
347,puzfwf,[R] Scale Efficiently: Insights from Pre-training and Fine-tuning Transformers,https://arxiv.org/abs/2109.10686,koolaidman123,MachineLearning,1970-01-01 00:00:01.632543395,Research,1,1,,NEU
348,puxr2j,[D] Good papers on artificial creativity and imagination?,https://www.reddit.com/r/MachineLearning/comments/puxr2j/d_good_papers_on_artificial_creativity_and/,throwaway_secondtime,MachineLearning,1970-01-01 00:00:01.632536664,Discussion,0,2,"Has there been any research work done in ML/AI that tried to replicate aspects of human creativity like music or the arts, or emulating human imagination and certain tasks like planning which is one of the use-cases of imagination?",POS
349,pub1jc,[D] What is the best openly available GPT3-like model (i.e. zero shot NLP)?,https://www.reddit.com/r/MachineLearning/comments/pub1jc/d_what_is_the_best_openly_available_gpt3like/,Gradient314,MachineLearning,1970-01-01 00:00:01.632452831,Discussion,62,12,"I don't have access to the OpenAI API, but would like to experiment with the current state of the art. I am looking for either an API to some alternative implementation, or to download a pretrained similar model. 'Best' in the sense of using a general corpus and high performance for zero-shot tasks.",POS
350,puvwwh,[D] Do you know any open-source video classifiers?,https://www.reddit.com/r/MachineLearning/comments/puvwwh/d_do_you_know_any_opensource_video_classifiers/,Hunamed_silva,MachineLearning,1970-01-01 00:00:01.632529618,Discussion,0,4," So I would like to do some experiments in video classification.I have some experience working with traditional structured data and now I'm getting some experiments on object detection, image classification, and image segmentation. I started doing object detection with YOLO and evolved from there.I know that for video classification I can use a feature extractor and a GRU or LSTM but I'm not ready to design the model by myself.Do you know any open source projects like YOLO but for video classification, where I could train a custom model and explore their code?",POS
351,puym6n,[D] Cost Benefit Simulations,https://www.reddit.com/r/MachineLearning/comments/puym6n/d_cost_benefit_simulations/,blueest,MachineLearning,1970-01-01 00:00:01.632540036,Discussion,0,0,"I am interested in looking at something like this:https://cran.r-project.org/web/packages/heemod/vignettes/c_homogeneous.htmlSuppose there is an insurance company. The inaurance company processes insurance claims. Let's say there are 100 people working at the insurance company: everyday, new claims arrive and existing claims are settled - but there is always a backlog.In terms of strategies, the insurance company is considering hiring new employees: they are thinking of 5 new employees (strategy 1: costs $ 200,000), 10 new employees (strategy 2 : costs $ 500,000) or 15 new employees (strategy 3: costs $ 700,000).The logic being, perhaps more employees could result in: fewer backlog through out the year, faster processing time of claims or smaller payouts to the claim filers (e.g. lets assume that each claim has to be processed in under 30 days, if a claim is approaching 30 days - the insurance company tries to negotiate and pay 50% of the amount owed instead of the whole amount).In terms of the ""transitions"", different options can be considered:A) The amount of backlog in the system (e.g. state A = less than 100 claims, state B = 101 to 200 claims, state C = more than 300 claims). Using existing data,  transition matrix can be made to construct this transition matrix (3 Ã— 3).B) The average number of days spent on a claim (e.g. state A = less than 10 days , state B = 11 days to 25 days, state C = more than 25 claims). Using existing data,  transition matrix can be made to construct this transition matrix (3 Ã— 3)C) The average percentage of the full amount saved on a case (e.g. state A = insurance company pays on average pays less than 50% of cases on average , state B = pays between 51% and 75% , state C = pays more than 75%). Using existing data,  transition matrix can be made to construct this transition matrix (3 Ã— 3).My question is: I understand how to run a simulation that shows on any given day, which state the transition matrix (i.e. markov chain) will be in. Question 1: But how can you calculate the cost and benefit (utility) of being in state A, state B and State C? I thought of adding integer scores to each state (e.g. state A = +3, state B = +2, state C = +1). Assuming that its always more advantageous to be in state, you run the simulation for 100 days and add the score on each day. A score 201 could mean that the system was on the whole ""healthier"" than the system with a score of 167. Is there another way of doing this?Question 2: I know the cost of each strategy. But how do you attribute a benefit to each strategy? The best I can think about is trying to look at the historical data available and try to look at the system statistics when more people were hired vs less people.Can someone please provide some advice on this? In general, am I understand the use of cost-benefit simulation correctly? Could this simulation serve as a legitimate method to decide which strategy to select?Thanks!",POS
352,puf97z,[D] (Paper Overview) Pix2Seq: A Language Modeling Framework for Object Detection,https://www.reddit.com/r/MachineLearning/comments/puf97z/d_paper_overview_pix2seq_a_language_modeling/,thedeepreader,MachineLearning,1970-01-01 00:00:01.632470912,Discussion,10,1,"**Video**[https://youtu.be/6Ptp0LfeN6g](https://youtu.be/6Ptp0LfeN6g)**Paper**[https://arxiv.org/abs/2109.10852](https://arxiv.org/abs/2109.10852)**Abstract**This paper presents Pix2Seq, a simple and generic framework for object detection. Unlike existing approaches that explicitly integrate prior knowledge about the task, we simply cast object detection as a language modeling task conditioned on the observed pixel inputs. Object descriptions (e.g., bounding boxes and class labels) are expressed as sequences of discrete tokens, and we train a neural net to perceive the image and generate the desired sequence. Our approach is based mainly on the intuition that if a neural net knows about where and what the objects are, we just need to teach it how to read them out. Beyond the use of task-specific data augmentations, our approach makes minimal assumptions about the task, yet it achieves competitive results on the challenging COCO dataset, compared to highly specialized and well optimized detection algorithms.",POS
353,purqty,Assisting the Human Fact-Checkers: Detecting All Previously Fact-Checked Claims in a Document,https://arxiv.org/abs/2109.07410,firojalam04,MachineLearning,1970-01-01 00:00:01.632514787,,0,1,,NEU
354,pur75t,[D] PhD Thesis Committee,https://www.reddit.com/r/MachineLearning/comments/pur75t/d_phd_thesis_committee/,mliberosis,MachineLearning,1970-01-01 00:00:01.632512993,Discussion,1,16,"Hi,I have had one question that I have received pretty confusing answers to. I am forming my dissertation committee and I have members decided from within and cross department status within my university. The problem is, that the very small sub-area I work in does not have any professor associated in my university. How common or how bad of an idea it is to ask someone from outside my university that I have interacted with, and talked about research during conferences or talked to during my internship (but not as direct mentor), to be on my committee and how does one go about it?  \[US Specific question\]",NEG
355,puc7nx,[R] Physics-based Human Motion Estimation and Synthesis from Videos (ICCV 2021),https://arxiv.org/abs/2109.09913,hardmaru,MachineLearning,1970-01-01 00:00:01.632457189,Research,16,2,,NEU
356,puiqwf,[Research] Training times of modern YOLO-like networks?,https://www.reddit.com/r/MachineLearning/comments/puiqwf/research_training_times_of_modern_yololike/,CorrSurfer,MachineLearning,1970-01-01 00:00:01.632486692,Research,3,4,"In the recent years, there has been a good number of YOLO-type networks for image segmentation or object detection. The papers and Github repositories on this topic (e.g., [https://github.com/Megvii-BaseDetection/YOLOX](https://github.com/Megvii-BaseDetection/YOLOX)) frequently contain the information how fast inference of the learned network is.However, I'm surprised that none of these papers seem to contain some information on how long it takes to train the respective models from scratch. I find this surprising, because in other domains of computer science, computation times to obtain the described artifact are typically given (along with a quick description of the used CPU/GPU/computer - enough to get an idea of the order of magnitude of the computation task's difficult).So for instance computing the weights of the learned YOLOX-s used for the experimental results reported by the authors of YOLOX could take 3 GPU-years, or computing the weights of YOLOX-x could also just take a minute.So why are such times not reported? And....does anybody have some training time/CPU/GPU description tuples of reasonably modern YOLO architectures?",POS
357,pupnpz,[D] NeurIPS archival workshops?,https://www.reddit.com/r/MachineLearning/comments/pupnpz/d_neurips_archival_workshops/,sd576,MachineLearning,1970-01-01 00:00:01.632508015,Discussion,0,8,I think most NeurIPS workshops are non-archival. What are the archival workshops at the conference?,NEU
358,pugw2b,[D] Successful examples of using a VAE encoder for a downstream classifier.,https://www.reddit.com/r/MachineLearning/comments/pugw2b/d_successful_examples_of_using_a_vae_encoder_for/,tall-dub,MachineLearning,1970-01-01 00:00:01.632479054,Discussion,4,2,I am looking for successful examples of using a variational autoencoder for a downstream classifier. I often hear that you can use a VAE for semi-supervised classification where you train the vae until it has a low loss and good reconstructions. You then use the encoder to encode the input to the lower dimension and use the mean that it predicts as an input to a standard neural network. However I have found that I can get better results faster by just using a fully connected network on MNIST and FashionMNIST.I want to know if there are people doing this successfully and possibly read their code. I already know about [this](https://arxiv.org/abs/1906.02242) paper but they take it a lot further with topic modelling and using multiple layers as a representation.,POS
359,pu8gz0,[D] Tool for annotating and seeing other annotations of papers in the cloud,https://www.reddit.com/r/MachineLearning/comments/pu8gz0/d_tool_for_annotating_and_seeing_other/,carlml,MachineLearning,1970-01-01 00:00:01.632443397,Discussion,18,5,"I remember someone posted a website to access papers where you could annotate and read other people's annotations. I couldn't find it, unfortunately. Does anyone have that link? Thanks",POS
360,ptu24e,[D] Fine-tuning GPT-J: lessons learned,https://www.reddit.com/r/MachineLearning/comments/ptu24e/d_finetuning_gptj_lessons_learned/,juliensalinas,MachineLearning,1970-01-01 00:00:01.632399846,Discussion,157,50,"Hello all,We've spent quite some time benchmarking the best fine-tuning techniques for GPT-J at [NLP Cloud](https://nlpcloud.io/?utm_source=reddit&utm_campaign=k431103c-ed8e-11eb-ba80-2242ac130007).   Finding the best solution was not straightforward and we had to look  at  things like speed, server costs, ease of development, accuracy of  the  fine-tuned model... It took time but we ended up with a nice setup  (and  we are now officially proposing GPT-J fine-tuning + automatic  deployment  on our platform).Here are our key takeaways:* The best methodology seems to be the one from the Mesh Transformer Jax team: [https://github.com/kingoflolz/mesh-transformer-jax/blob/master/howto\_finetune.md](https://github.com/kingoflolz/mesh-transformer-jax/blob/master/howto_finetune.md)* Fine-tuning   on GPU is not ideal. Even several GPUs used in parallel with Deepspeed   can be very slow. We used 4 GPUs Tesla T4 in parallel, and it took  1h30  to only compute our first checkpoint (+ 80GB of RAM used...), for a   training dataset made up of 20k examples. Maybe a GPU A100 would be   worth a try.* Fine-tuning   on TPU is very efficient but it takes a TPU v3 because TPUs v2 are   running out of memory. It takes around 15 minutes, for a training dataset   made up of 20k examples, which is really awesome.* The   overall process is not straightforward as it takes several kind of   conversions (converting the datasets to the right format, making a slim   version of the model, converting the weights to Transformers...)In   the end this is worth the effort, because combining fine-tuning and   few-shot learning makes GPT-J very impressive and suited for all sorts   of use cases.If you guys have   different feedbacks about GPT-J fine-tuning, please don't hesitate to   comment, I would love to have your opinion.Hope you found the above useful!",POS
361,pu7rxp,[D] How do you ensure reproducibility?,https://www.reddit.com/r/MachineLearning/comments/pu7rxp/d_how_do_you_ensure_reproducibility/,Throwaway00000000028,MachineLearning,1970-01-01 00:00:01.632440937,Discussion,17,29,"So my main issue here is that I'll train a model but then later add some additional feature along the way. So when I go back to evaluate that model the source code is now different and I have to revert my changes or implement a workaround.What I've done before is automatically copy the source code into the run folder so I can always reproduce it and see what changes I made. But obviously this is not ideal.I guess one of the ML tool providers (wandb, etc) probably have a solution for this. Does any one have personal experience? Thanks in advanceEdit: Thanks for the responses. I guess it's time to learn source control",POS
362,puigpl,[D] GAN loss functions,https://www.reddit.com/r/MachineLearning/comments/puigpl/d_gan_loss_functions/,celviofos,MachineLearning,1970-01-01 00:00:01.632485610,Discussion,0,1,"Hi, i'm working with GANs and i can't decide between using the non saturating GAN loss and the hinge loss. Does someone knows what are the benefits of one with respect to the other. Hinge loss seems to have nice mathematical properties but non saturating loss seems to be used in more papers (specially style gan). &#x200B;Thanks",NEG
363,pucqt1,[D] ONNX Runtime Web,https://www.reddit.com/r/MachineLearning/comments/pucqt1/d_onnx_runtime_web/,KadoSC2,MachineLearning,1970-01-01 00:00:01.632459346,Discussion,5,2,"Recently, ONNX released ONNX runtime web. Have any of you tried it out? I would like to hear your thoughts on it compared to TensorFlow js and its predecessor, Onnx.js.More specifically, I'd like to talk about running Models in the browser in general. When does it make sense?",POS
364,pugc94,[D] VQgan + clip for AMD GPUs,https://www.reddit.com/r/MachineLearning/comments/pugc94/d_vqgan_clip_for_amd_gpus/,goodspells,MachineLearning,1970-01-01 00:00:01.632476455,Discussion,1,9,"I want to run this beautiful AI locally: https://github.com/nerdyrodent/VQGAN-CLIPAs you can see to set up AMD GPUs u need ROCm, which is not officially supported for a lot of cards. (rx5700xt for me)Is there a way to get this thing to work without CUDA for AMD gpus?It would be life changing, thanks.",POS
365,ptzxis,[Project] I analyzed how well Automatic Speech Recognition can transcribe song lyrics,https://www.reddit.com/r/MachineLearning/comments/ptzxis/project_i_analyzed_how_well_automatic_speech/,help-me-grow,MachineLearning,1970-01-01 00:00:01.632417278,Project,18,9,"Speech Recognition technology has advanced to the point where we can use AI to transcribe human speech just as well as a human can. How about song lyrics? Singing and speaking have very different audio patterns. Singing has higher intensity, more variation in pitch, and much more vowel space, up to 40x as much vowel space. With these differences, I predicted that state of the art AI for Speech Recognition would have great difficulty transcribing songs, and only be able to transcribe about 5% correctly. The results were surprising, the AI did better than expected.Read the full article to see [how well the AI actually performs](https://www.assemblyai.com/blog/how-well-does-ai-transcribe-song-lyrics/).Spoiler: Speech Recognition does pretty well for Drake, but not so well for AC/DC.",POS
366,puivi3,[D] Libraries and ML Algorithms to build and learn from Tree-structured data?,https://www.reddit.com/r/MachineLearning/comments/puivi3/d_libraries_and_ml_algorithms_to_build_and_learn/,qwertz_guy,MachineLearning,1970-01-01 00:00:01.632487159,Discussion,0,1,"I have a dataset and part of the data reflects decisions made at different times, a series of decisions basically, and different kind of outcomes that indicate how good **the whole chain of decisions** was in the end.I'm interested in 2 things:- Build (and maybe visualize) these trees from the data- Understand the best decisions and series of decisions, so for example what series of decisions had on average the best outcome and in what scenarios (so conditioned on the rest of the data) were the best decisions madeI'm aware of different ML/DL models and techniques but I can't fit this problem into anything I know yet. Can someone point me to a direction? Are there any Python libraries that could help me with this?Edit: perhaps to clarify: I know the entire space of possible decisions, but I want to learn/understand why a certain decision was picked over another.",POS
367,pu413o,[D] GSN Paper Explained - Unconstrained Scene Generation with Locally Conditioned Radiance Fields (5-minute summary),https://www.reddit.com/r/MachineLearning/comments/pu413o/d_gsn_paper_explained_unconstrained_scene/,KirillTheMunchKing,MachineLearning,1970-01-01 00:00:01.632429105,Discussion,9,0,"[Imagine this in 4K in VR!](https://i.redd.it/hb3o6q73cbp71.gif) NeRFs are great, yet they are primarily used for interpolating views in single object scenes and have severely limited capabilities for extrapolating beyond the input views. Generative Scene Networks (GSN), proposed by Terrance DeVries and his colleagues at Apple University of Guelph and Vector Institute, learn to decompose scenes into a collection of many local radiance fields. This enables the model to be used as a prior to generate novel scenes or complete scenes from sparse 2D observations at higher quality than existing models.Check out the [full paper summary](https://www.casualganpapers.com/iccv-2021-large-free-moving-indoor-scenes-nerf/GSN-explained.html) on Casual GAN Papers (Reading time \~5 minutes).Subscribe to [my channel](https://t.me/casual_gan) and follow me on [Twitter](https://twitter.com/KirillDemochkin) for weekly AI paper summaries!",POS
368,puei9w,[D] Next Frame Prediction Running Live On GPU,https://www.reddit.com/r/MachineLearning/comments/puei9w/d_next_frame_prediction_running_live_on_gpu/,isbtegsm,MachineLearning,1970-01-01 00:00:01.632467209,Discussion,0,0,"What types of next frame prediction models would run live in WebGL (after training)?  I don't care how good they look, I just want to play around with interesting patterns.Especially I thought about training on [this](https://www.illustris-project.org/movies/illustris_movie_dmdens_z0_slicing.mp4) video, it seems that the dynamic here is invariant under translation, reflection and rotation, so maybe I could exploit that fact in my model?  The simplest thing which comes to my mind is fixing some neighbourhood and then doing multiple linear regression, so the color (I take one-dimensional color values as the video seems to be basically a heatmap) of a pixel in the next frame is an affine linear transformation of the colors of its neighbours in the current frame.  Then I'd sample the neighbourhood in multiple, random rotations, apply my model on each and take the mean.The problem with this approach is that I basically just integrate some rings around the pixel of various diameters, which means the model couldn't distinguish a gray pixel sitting on the border between a black area and a white area from a gray pixel sitting in a gray area.  I thought about also incorporating products of the color values, e.g. if the neighbours of `x` are `x, y, x`, I would try to fit a model of the form `f(t + 1, x) = dot(Î², [1.0, f(t, x), f(t, y), f(t, z), f(t, x) * f(t, x), f(t, x) * f(t, y) * ... * f(t, z) * f(t, z)])` where `Î²` is a vector of length 10 in this case.Does that make any sense?  Are there maybe some other simple models which have rotationally invariance baked in directly?",POS
369,pu92vu,[D] Where does the 128 come from?,https://www.reddit.com/r/MachineLearning/comments/pu92vu/d_where_does_the_128_come_from/,TerryCrewsHasacrew,MachineLearning,1970-01-01 00:00:01.632445623,Discussion,2,6,"HiI was reading through the intel's paper regarding ""Enhancing Photorealism Enhancement"" and encounter this image in the paper&#x200B;https://preview.redd.it/gesssulyocp71.png?width=537&format=png&auto=webp&s=17bc36d87a4c4db7d29d6f1f7a076a4450cf9292Was wondering if anyone could tell me where this 128, in the image, comes from, cause the number of G-buffers are less than 128?",POS
370,pu01z9,[D] Transformer and training for a sequence of embeddings from huge-size images,https://www.reddit.com/r/MachineLearning/comments/pu01z9/d_transformer_and_training_for_a_sequence_of/,Ok_Confection9620,MachineLearning,1970-01-01 00:00:01.632417615,Discussion,9,2,"hi all,  I have a problem finding a approach for the following problem: A huge size image (e.g. 60 000 x 100 000 pixel) will be subdivided into N patches of size 224x224 and afterwards feed into a pre-trained feature extractor resulting in a latent dimension L, lets assume the feature extractor is working correctly . Therefore, we have a sequence of length NxL. Each patch should be binary classified, i.e. 0 or 1 (e.g. contains a specific thing or not), which leads to a Multi-label classification problem. My problem is, I just have 50 annotated huge images where each patch is annotated as 0 or 1, and about 1500 unlabelled huge images. As the patch-lvl classification depends on the patches around it, I though of using a Transformer, but I don't find pre-trained weights for this task, when working on a feature space and I don't think 50 annotated images are enough...  Any suggestions? **Summary:**  *Task:* Multilabel classification  *Data:* 50 big images, and a label-vector Nx1 containing 0s and 1s, 1500 unlabelled huge images *Pipeline:* Input: H1xH2 -> feature extractor -> NxL -> Transformer -> Output shape: Nx1",POS
371,pu8b27,[R] Introducing Symmetries to Black Box Meta Reinforcement Learning,https://arxiv.org/abs/2109.10781,hardmaru,MachineLearning,1970-01-01 00:00:01.632442797,Research,2,3,,NEU
372,pu6n8g,[R] Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling? An Extensive Empirical Study on Language Tasks,https://openreview.net/forum?id=Wrtp36cbl61,koolaidman123,MachineLearning,1970-01-01 00:00:01.632437179,Research,3,3,,NEU
373,ptw109,[D] How do you manage your ml models that require a gpu in production? (Question),https://www.reddit.com/r/MachineLearning/comments/ptw109/d_how_do_you_manage_your_ml_models_that_require_a/,zbnone,MachineLearning,1970-01-01 00:00:01.632406018,Discussion,8,18,"I founded a tech startup and we are working with image segmentation models (u-net like). We currently deploy our models to manually setup gpu nodes with tensorflow serving. This requires lots of upkeep and infrastructure around it.  Do you know any alternatives? My dream workflow would be a simple web interface where you can upload your tensorflow serving (or h5 keras, pytorch) models and get a for example basic authentication protected url where you can post your images (or text, videos, audios) to and get the model results back?  If no one knows a platform like it, we are thinking about building something like it internally. If anyone sees a fundamental problem with that approach or as any advice it would be very welcome.  Thank you :)",POS
374,ptxo8e,[D] GitHub repository for grad admissions,https://www.reddit.com/r/MachineLearning/comments/ptxo8e/d_github_repository_for_grad_admissions/,disentangled_latent,MachineLearning,1970-01-01 00:00:01.632410867,Discussion,8,6,"Do ML/DL grad admissions committees go through the code linked for projects in the resume, while reviewing applications?",NEU
375,pu18zt,"[R] Google AI Introduces â€˜WITâ€™, A Wikipedia-Based Image Text Dataset For Multimodal Multilingual Machine Learning",https://www.reddit.com/r/MachineLearning/comments/pu18zt/r_google_ai_introduces_wit_a_wikipediabased_image/,techsucker,MachineLearning,1970-01-01 00:00:01.632420980,Research,2,1,"Image and text datasets are widely used in many machine learning applications. To model the relationship between images and text, most multimodal Visio-linguistic models today rely on large datasets. Historically, these datasets were created by either manually captioning images or crawling the web and extracting the alt-text as the caption. While the former method produces higher-quality data, the intensive manual annotation process limits the amount of data produced. The automated extraction method can result in larger datasets. However, it requires either heuristics and careful filtering to ensure data quality or scaling-up models to achieve robust performance.Â To overcome these limitations, Google research team created a high-quality, large-sized, multilingual dataset called [the Wikipedia-Based Image Text (WIT) Dataset](https://github.com/google-research-datasets/wit). It is created by extracting multiple text selections associated with an image from Wikipedia articles and Wikimedia image links.Â # [5 Min Read](https://www.marktechpost.com/2021/09/23/google-ai-introduces-wit-a-wikipedia-based-image-text-dataset-for-visio-linguistic-models/) | [Github](https://github.com/google-research-datasets/wit) | [Paper](https://arxiv.org/pdf/2103.01913.pdf) | [Google Blog](https://ai.googleblog.com/2021/09/announcing-wit-wikipedia-based-image.html)&#x200B;https://preview.redd.it/32dctgv3oap71.png?width=646&format=png&auto=webp&s=81ee40de71b0e6dc9ae13f1c5819cd5fa13c074a",POS
376,ptp490,Graph Attention Networks (Paper Summary) [D],https://www.reddit.com/r/MachineLearning/comments/ptp490/graph_attention_networks_paper_summary_d/,prakhar21,MachineLearning,1970-01-01 00:00:01.632378019,Discussion,29,3,Inspired by the concept of â€œattentionâ€ this paper applies it to graph-structured data for learning better node representation. ðŸ”¥ ðŸ”¥ Very interesting paper. Do Checkout paper summary at https://youtu.be/v2P1yZhP8csPaper details in the comments!,POS
377,ptwtde,[R] UMass Amherst & Google Improve Few-Shot Learning on NLP Benchmarks via Task Augmentation and Self-Training,https://www.reddit.com/r/MachineLearning/comments/ptwtde/r_umass_amherst_google_improve_fewshot_learning/,Yuqing7,MachineLearning,1970-01-01 00:00:01.632408380,Research,4,0,"A team from University of Massachusetts Amherst and Google Research proposes STraTA, an approach that combines task augmentation and self-training to leverage unlabelled data and improve sample efficiency and performance on NLP tasks. Here is a quick read: [UMass Amherst & Google Improve Few-Shot Learning on NLP Benchmarks via Task Augmentation and Self-Training.](https://syncedreview.com/2021/09/23/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-110/)The paper *STraTA: Self-Training with Task Augmentation for Better Few-Shot Learning* is on [arXiv](https://arxiv.org/abs/2109.06270).",POS
378,ptow36,"[D] ""Regular Life"" Applications of Markov Simulations and Markov Decision Process",https://www.reddit.com/r/MachineLearning/comments/ptow36/d_regular_life_applications_of_markov_simulations/,jj4646,MachineLearning,1970-01-01 00:00:01.632377012,Discussion,15,5,"I have been reading about ""Markov Simulations and Markov Decision Process"" and am trying to understand how these concepts can be used in ""regular and everyday life"". I see these concepts being used a lot in IT and Tech applications, for example - computers learning to play video games like ""tetris"" and ""mario"". But can these concepts be applied in everyday settings? &#x200B;For example,&#x200B;**Problem 1:** Suppose there is a grocery store owner who has some leftover money. With this money, the owner identifies a list of ""actions"" that can be taken:&#x200B;1) Do nothing and deposit the money in the bank (earn 2% interest a year)&#x200B;2) Hire more staff (this will cost money, but the owner believes that this will increase profits and the  staff will be able to complete more orders everyday, e.g. catering orders).&#x200B;3) Buy new cash registers (the existing cash registers are old, and the owner believes that since newer machines will be able to accept credit cards, they will speed up the lines and increase profits)&#x200B;4) Renovate the store (this will cost money, but revamping the store will make it look more attractive and increase profits)&#x200B;5) Hire a consultant ( the will cost money, but the consultant might be able to offer some advice that can increase profits)&#x200B;6) Consider expanding the variety of products that are being offered (this will also cost money, but this could attract new customers and increase profits).&#x200B;Suppose the owner has detailed historical information on all transactions made in the grocery store, all operating costs, all profits, and the number of customers that pass through his store. Also, the owner knows how much each of these ""Actions"" will approximately cost, but he does not know how much profit they will result in.&#x200B; Can the owner somehow use ""Markov Simulations and Markov Decision Process"" to figure out which of these actions he should take (i.e. how should he spend his money in the next year) ? How exactly would he do this? &#x200B;**Problem 2:**&#x200B;Based on the number of transactions and expiry dates, can the owner use ""Markov Simulations and Markov Decision Process"" on a daily basis to figure out an optimal way to decide :&#x200B;\- Which products to reorder and how many units to reorder (e.g. more or less) and what price to sell them at?&#x200B;\- Which products to discontinue?&#x200B;\- Which new products to acquire and what price to sell them at?&#x200B;For example, suppose the owner decides to order more soda drinks because it's the summer. If he orders too much and it doesn't sell, at least the soda won't go bad. He can just hold on to it and get rid off it over the next few months. However, he might have been able to use that money he spent on soda on some other product which would have earned him more profits - the soda didn't make him lose existing profits in the long run, but rather lose potential profits.&#x200B;However, if the owner orders too much turkey on thanksgiving and it doesn't sell - it might all expire and he might completely lose profits before he can even sell it at a discounted rate. But the profit margins on turkey could be more than soda drinks - but soda drinks has a lower risk. If the owner decides to be conservative and only order some turkey - if all the turkey sells and customers want more, he loses out on some profit. But ordering less turkeys provided they don't all sell - he loses less money. The possibilities are endless.&#x200B;In short, could the owner use Markov Simulations and Markov Decision Process for this problem? In this problem, the list of actions are much bigger than the previous problem (e.g. 6 actions vs many actions). I understand that Markov Simulations require somehow ""mapping"" expected rewards to each action based on the ""state"" that the system will be in. I am not sure how to define states in this problem (e.g. an infinite large matrix that describes consumer patterns, e.g. ""high turkey demand and high soda demand"", ""low turkey demand and high soda demand"", ""high turkey demand and low soda demand"", ""low turkey demand and low soda demand"".... now imagine this for 100 items). I suppose that with enough historical data, a Markov Chain can be made that describes these transitions, but for practical measures this Markov Chain will probably need to be collapsed (e.g. ""high meat demand and high drink demand""). &#x200B;Can someone please suggest how Markov Simulations and Markov Decision Process can be used to solve this problem?&#x200B;Thanks!(sorry for the long post)",POS
379,ptbhw1,[D] Graph Machine Learning in Industry,https://www.reddit.com/r/MachineLearning/comments/ptbhw1/d_graph_machine_learning_in_industry/,nd7141,MachineLearning,1970-01-01 00:00:01.632328890,Discussion,162,18,"Please, join us tomorrow for Graph Machine Learning in Industry workshop.6 speakers, 30 min each, talking about successes and failures of applying graphs in the industry.Please, register (free) to get notification and follow-up email about slides and videos. (disable ad block if zoom does not allow you to register).[https://sites.google.com/view/graph-ml-in-industry/home?read\_current=1](https://sites.google.com/view/graph-ml-in-industry/home?read_current=1)&#x200B;YouTube link: [https://www.youtube.com/watch?v=bLN1V5fZD2g](https://www.youtube.com/watch?v=bLN1V5fZD2g)",POS
380,ptnild,[D] Is this a valid Machine Learning Approach?,https://www.reddit.com/r/MachineLearning/comments/ptnild/d_is_this_a_valid_machine_learning_approach/,blueest,MachineLearning,1970-01-01 00:00:01.632371217,Discussion,18,4,"I am trying to better understand the conditional and marginal distributions of the normal probability distribution function: https://online.stat.psu.edu/stat505/lesson/6/6.1""Any distribution for a subset of variables from a multivariate normal, conditional on known values for another subset of variables, is a multivariate normal distribution.""Suppose I have data corresponding to 3 variables : Var_1 , Var_2 and Var_3. I am interested in predicting Var_3 using Var_1 and Var_2.Suppose I fit a multivariate normal distribution to this data - doesn't the multivariate normal distribution have special properties such that the conditional distribution of any of the variables within the multivariate normal distribution will also form a normal distribution? Suppose I want to predict the value of Var_3 when Var_1 = a AND Var_2 = b.Couldn't I just ""fix"" the values of the other two variables and construct a conditional distribution for the response variable Prob (Var_3 | Var_1 = a and Var_2 = b) ? Shouldn't ""Prob (Var_3 | Var_1 = a and Var_2 = b) "" have a normal distribution? Could I not then generate a distribution (e.g. histogram) of acceptable values of this response variable given the ""fixed"" values of the other two variables? I think I should be able to sample from Prob (Var_3 | Var_1 = a and Var_2 = b) given that I have chosen a multivariate normal distribution? Then, I could take the Expected Value of "" Prob (Var_3 | Var_1 = a and Var_2 = b) "" to answer my question? E.g when ""Var_1 = a and Var_2 = b"", Var_3 is most likely to be equal to ""c""?https://imgur.com/a/4aTDkR1Would this be considered a ""generative model""? Is this a correct strategy in general? Does it make mathematical sense?Note: I know that I could just fit a regular regression model to this problem, but I am trying to better understand how probability distribution functions work.Thanks",POS
381,ptwib3,[D] Transformer sequence generation - is it truly quadratic scaling?,https://www.reddit.com/r/MachineLearning/comments/ptwib3/d_transformer_sequence_generation_is_it_truly/,Ifyouletmefinnish,MachineLearning,1970-01-01 00:00:01.632407460,Discussion,2,5," I've read a lot about efficient Transformer architectures (Sparse Transformer, Performer, ...) that attempt to alleviate the quadratic memory/compute complexity of Attention with various tricks.However, I've come across the concept of Key, Value Caching in Transformer-Decoders recently (e.g. [Figure 3 here](https://scale.com/blog/pytorch-improvements)), wherein because each output (and hence each input, since the model is autoregressive) only depends on previous outputs (inputs), we don't need to re-compute Key and Value vectors for all t < t\_i at timestep i of the sequence. My intuition leads me to believe, then, that (unconditioned) inference for a decoder-only model uses an effective sequence length of 1 (the most recently produced token is the only real input that requires computation on), making Attention a linear-complexity operation. This thinking seems to be validated by this [github issue](https://github.com/lucidrains/x-transformers/issues/29), and [this paper (2nd paragraph of Introduction)](https://arxiv.org/pdf/2105.04779.pdf).This of course doesn't invalidate the quadratic complexity issue for the non-autoregressive case (e.g. BERT), but I'm wondering why more papers don't point out that this problem doesn't exist at inference time for autoregressive models like GPT-3? Even the [Sparse Attention paper](https://d4mucfpksywv.cloudfront.net/Sparse_Transformer/sparse_transformers.pdf) (almost 400 citations) says that>""We consider the task of autoregressive sequence generation .... the self-attention portion of the network must compute n weightings for each of n elements, however, which can quickly become intractable as the sequence length grows. ""But that's not strictly true if my understanding is correct. Am I missing something in all of this?Would greatly appreciate a sanity check in all of this, thanks [r/ML](https://www.reddit.com/r/ML/)!",POS
382,ptbtt3,"[D] ""Rethinking XXX"" Papers, a trend in ML",https://www.reddit.com/r/MachineLearning/comments/ptbtt3/d_rethinking_xxx_papers_a_trend_in_ml/,randy_wales_qq,MachineLearning,1970-01-01 00:00:01.632329859,Discussion,82,27,"HelloCurrently, there is a trend of ""rethinking XXX"" in the ML community (publications at top ML conferences). Do they basically just apply the existing XXX into the old problem? What's your view on this?",NEG
383,ptx0sv,[D] How to setup Deep Learning server,https://www.reddit.com/r/MachineLearning/comments/ptx0sv/d_how_to_setup_deep_learning_server/,fireless-phoenix,MachineLearning,1970-01-01 00:00:01.632409000,Discussion,2,5,I recently got access to multiple GPUs and want to set up a remote server that can be accessed remotely by many people for performing their deep learning tasks. What I want from the server:* Access GPU acceleration remotely* User authentication* Permission for users to only work in their directory without affecting root or someone else's directory I would prefer an open source solution. Previously I have tried JupyterHub with DockerSpawner but the service was unable to detect GPU. What do you guys think would be the best solution to my problem? I would appreciate any and all help.,POS
384,ptuon5,"[N] MLCommonsâ„¢ releases MLPerfâ„¢ Inference v1.1 Results with over 1,800 performance and 350 power results for data centers and edge devices",https://www.reddit.com/r/MachineLearning/comments/ptuon5/n_mlcommons_releases_mlperf_inference_v11_results/,gfursin,MachineLearning,1970-01-01 00:00:01.632401926,News,0,1,* [MLCommons press-release](https://mlcommons.org/en/news/mlperf-inference-v11/)* [Datacenter results](https://mlcommons.org/en/inference-datacenter-11/)* [Edge results](https://mlcommons.org/en/inference-edge-11/)* [Reproducibility studies](https://github.com/mlcommons/ck/tree/master/docs/mlperf-automation/reproduce#reproducibility-reports-mlperf-inference-benchmark-v11),NEU
385,ptu9ur,[D] Manipulating Videos with Text,https://www.reddit.com/r/MachineLearning/comments/ptu9ur/d_manipulating_videos_with_text/,kurzai,MachineLearning,1970-01-01 00:00:01.632400633,Discussion,0,2,"I've found some interesting research on modifying what a person is  saying in a video by providing new text and letting the model adjust the  audio and lip movement in the video itself. I haven't found any useful  code repositories though, where I could try it out with my own videos.  Does anyone know of open source implementations of such tools?",POS
386,pt4ehu,"[N] 11th International Conference on Artificial Intelligence in Music, Sound, Art and Design (EvoMUSART)",https://www.reddit.com/r/MachineLearning/comments/pt4ehu/n_11th_international_conference_on_artificial/,evomusart_conference,MachineLearning,1970-01-01 00:00:01.632305603,News,105,19,"Hello colleagues,We are organizing the 11th International Conference on Artificial Intelligence in Music, Sound, Art and Design (EvoMUSART) and we think it may be of interest to many of you. The conference will take place between 20 and 22 of April 2022.If you work with Artificial Intelligence techniques applied to visual art, music, sound synthesis, architecture, video, poetry, design or other creative tasks, you can present your work at this conference.If not, it is also a great opportunity to know all the news of research in these fields.For more information, visit the event's webpage: [http://www.evostar.org/2022/evomusart/](http://www.evostar.org/2022/evomusart/)&#x200B;https://preview.redd.it/3g0wtfh951p71.jpg?width=2083&format=pjpg&auto=webp&s=c48764301097c0986869d0523b2d7e6b13701852",POS
387,ptji0q,[R] Scaling Laws for Neural Machine Translation,https://arxiv.org/abs/2109.07740,hardmaru,MachineLearning,1970-01-01 00:00:01.632356568,Research,4,2,,NEU
388,ptbu5g,[R] The Two Hilbert Spaces for Nonlocal Operators,https://www.reddit.com/r/MachineLearning/comments/ptbu5g/r_the_two_hilbert_spaces_for_nonlocal_operators/,AcademicOverAnalysis,MachineLearning,1970-01-01 00:00:01.632329885,Research,13,0,"Dynamic Mode Decomposition is an operator theoretic approach to the study of dynamical systems. The way it got its start was by looking at high dimensional systems, and stacking the snapshots into a matrix that was decomposed later. The eigendecomposition of this matrix provided Dynamic Modes, which gave the dominant modes of the system.However, this approach is extremely limiting, where you can only have as many samples of your space as you have dimensions before you saturate the rank of the matrix. The led to the use of extended DMD methods, where the states were thrown into a high dimensional feature space. Inevitably, this gave rise to infinite dimensional feature spaces and Koopman operators, which is how the connections with reproducing Kernel Hilbert Spaces first came about.Our work has been towards expanding the scope of Dynamic Mode Decompositions, where our first manuscripts in the field removed the Koopman operator from the analysis, and instead used Liouville operators. By directly accessing these operators, we are removing the requirements that a system is discretizable (forward complete/invariant). But this still leverages operators of functions of the state.To further extend these ideas to include nonlocal operators within their scope, we need to operate on Hilbert Spaces composed of functions that send continuous signals or trajectories to signals and trajectories. By introducing spaces like these, we can now look at operators that are nonlocal, which includes second order dynamical systems as well as fractional order dynamical systems.  Have a watch! I look forward to hearing your thoughts on this. YouTube: [https://youtu.be/lecuDgWxs2Y](https://youtu.be/lecuDgWxs2Y)arXiv Links:DMD for Higher Order Dynamical Systems (Signal Valued RKHSs): [https://arxiv.org/abs/2101.02646](https://arxiv.org/abs/2101.02646)Occupation Kernel Hilbert Spaces for the Spectral Analysis of Nonlocal Operators: [https://arxiv.org/abs/2102.13266](https://arxiv.org/abs/2102.13266)",POS
389,psss7y,[Project] Natural language processing course - Looking for feedback,https://www.reddit.com/r/MachineLearning/comments/psss7y/project_natural_language_processing_course/,sb2nov,MachineLearning,1970-01-01 00:00:01.632260291,Project,136,109,"Iâ€™m Sourabh, I lead one of the core Tensorflow teams at Google Brain and worked on data products at Coursera with Andrew Ng. Kaushik Rangadurai, ML Engineer at Facebook and I are leading a live, cohort based course on NLP starting November 1st. [https://corise.com/course/natural-language-processing](https://corise.com/course/natural-language-processing).We wanted to share what weâ€™ve learned in machine learning over the years. You can join the first run of the course (capped at about 30 students) below. If youâ€™re open to giving feedback on the class on how we can do better, happy to give a discount.",POS
390,ptdnlu,"[N][R] Want to leverage synthetic data for 3d reconstruction, but don't want to deal with the photometric domain gap? (ICRA 2021 talk)",https://www.reddit.com/r/MachineLearning/comments/ptdnlu/nr_want_to_leverage_synthetic_data_for_3d/,alexk_wong,MachineLearning,1970-01-01 00:00:01.632334992,News,3,1,"Want to leverage synthetic data for 3d reconstruction, but don't want to deal with the photometric domain gap?TLDR: Here is a gif showing an [overview of our approach](https://github.com/alexklwong/learning-topology-synthetic-data/blob/master/figures/scaffnet_fusionnet_overview.gif)Check out the [extended version of our ICRA 2021 talk](https://www.youtube.com/watch?v=zGKH-OKPJD4) for Learning Topology from Synthetic Data for Unsupervised Depth Completion. This is joint work with Safa Cicek and Stefano Soatto at the UCLA Vision Lab.In the talk, we will walk you through on how we learn dense topology from sparse geometry e.g. point clouds. This lets us use the abundance of synthetic data, where high quality ground truth comes for free, without having to deal with images and thus bypassing the photometric domain gap.After learning the initial coarse estimate of the scene from just sparse points, we bring the image back into the picture by performing cross modality fusion. This lets us learn the residual over the initial topology estimate and amend any mistakes, yielding a fast and accurate architecture that achieves the state-of-the-art while using fewer parameters than competing methods.Note: our method is unsupervised, so you don't need ground truth to trainFor those interested, here are our source code with pretrained mdoels (it is light-weight so it runs on your local machine!) and arxiv version of our paper.paper: https://arxiv.org/pdf/2106.02994.pdfcode + more cool figures: https://github.com/alexklwong/learning-topology-synthetic-data",POS
391,ptgr5z,[D] grammatical algorithms vs genetic algorithms,https://www.reddit.com/r/MachineLearning/comments/ptgr5z/d_grammatical_algorithms_vs_genetic_algorithms/,blueest,MachineLearning,1970-01-01 00:00:01.632346680,Discussion,3,6,"I am familiar with the standard genetic algorithm, recently I came across the ""grammatical genetic algorithm"" - apparently there is a whole set of algorithms called ""grammatical algorithms"".E.g. https://cran.r-project.org/web/packages/gramEvol/index.htmlApparently grammatical evolution algorithms are in some cases faster than standard genetic algorithms, but I can't quite seem to figure out why. I read over the vignette - I just don't yet understand why the need of a ""grammar""? Is the purpose of the grammar to serve as a set of optimization constraints that in turn reduce the possible search space? How does the ""grammar"" change the nature of the optimization problem?Can someone please help me understand the differences  between genetic algorithms and grammatial algorithms, and in which cases it might be better to use one compared to the other?For example, suppose I have a multi-dimensional function that I am trying to optimize (e.g. calculate the weight parameters for a neural network). Would it make sense to use the grammatical algorithm to optimize this function?Thanks",POS
392,psyd28,Next generation reservoir computing (Nature Communications),https://www.reddit.com/r/MachineLearning/comments/psyd28/next_generation_reservoir_computing_nature/,hardmaru,MachineLearning,1970-01-01 00:00:01.632279123,Research,28,4,"Article link (Open-access): https://www.nature.com/articles/s41467-021-25801-2*Abstract*Reservoir computing is a best-in-class machine learning algorithm for processing information generated by dynamical systems using observed time-series data. Importantly, it requires very small training data sets, uses linear optimization, and thus requires minimal computing resources. However, the algorithm uses randomly sampled matrices to define the underlying recurrent neural network and has a multitude of metaparameters that must be optimized. Recent results demonstrate the equivalence of reservoir computing to nonlinear vector autoregression, which requires no random matrices, fewer metaparameters, and provides interpretable results. Here, we demonstrate that nonlinear vector autoregression excels at reservoir computing benchmark tasks and requires even shorter training data sets and training time, heralding the next generation of reservoir computing.",POS
393,ptbczi,[Discussion] [Research] Pre-trained Models for Breast Cancer Image Classification,https://www.reddit.com/r/MachineLearning/comments/ptbczi/discussion_research_pretrained_models_for_breast/,chriskalahiki,MachineLearning,1970-01-01 00:00:01.632328509,Discussion,3,4,"Hi all,I am new to this subreddit, so I apologize in advance if this should go elsewhere. I am working on a research project with the immediate goal of classifying breast cancer images using a convolutional neural network. I wanted to try and leverage transfer learning to improve results and/or speed up training. I am new to finding pre-trained models to use though. Tensorflow Hub seems to have models that are, for the most part, trained on ImageNet or CIFAR-10. Is there a repository somewhere that would have more useful pre-trained models?",POS
394,psz69t,[Discussion] Is the VQ-VAE variational?,https://www.reddit.com/r/MachineLearning/comments/psz69t/discussion_is_the_vqvae_variational/,Gradient314,MachineLearning,1970-01-01 00:00:01.632282113,Discussion,9,6,"What confuses me is that there is no resampling step in the latent layer. Of course you could argue that the encoder predicts a single-valued distribution, but with the same argument any ole autoencoder is *technically* a variational autoencoder. If there is no randomness in the latent layer, how do we expect the model to understand that close points in the latent layer should refer to close points in its source domain?Talking about this paper: [https://arxiv.org/abs/1711.00937](https://arxiv.org/abs/1711.00937)",NEG
395,pt8b9s,[R] DeepMind Paper Provides Insights on Detoxifying Large Language Models,https://www.reddit.com/r/MachineLearning/comments/pt8b9s/r_deepmind_paper_provides_insights_on_detoxifying/,Yuqing7,MachineLearning,1970-01-01 00:00:01.632319732,Research,0,0,"In the paper Detoxifying Language Models, a DeepMind research team critically discusses toxicity evaluation and mitigation for contemporary transformer-based English large language models and provides insights on safer model use and deployment. Here is a quick read: [DeepMind Paper Provides Insights on Detoxifying Large Language Models.](https://syncedreview.com/2021/09/22/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-109/)The paper *Challenges in Detoxifying Language Models* is on [arXiv](https://arxiv.org/abs/2109.07445).",POS
396,pskxp7,[R] A Farewell to the Bias-Variance Tradeoff? An Overview of the Theory of Overparameterized Machine Learning,https://arxiv.org/abs/2109.02355,hardmaru,MachineLearning,1970-01-01 00:00:01.632237693,Research,59,21,,NEU
397,pskwhy,"[R] 3D Annotation Of Arbitrary Objects In The Wild. ""The proposed pipeline allows creating 3D and 2D annotations of arbitrary objects without needing accurate 3D models of the objects prior to data collection and annotation""",https://www.reddit.com/r/MachineLearning/comments/pskwhy/r_3d_annotation_of_arbitrary_objects_in_the_wild/,hietalajulius,MachineLearning,1970-01-01 00:00:01.632237593,Research,50,0,"[https://arxiv.org/abs/2109.07165](https://arxiv.org/abs/2109.07165)  **Abstract:**Recent years have produced a variety of learning based methods in the context of computer vision and robotics. Most of the recently proposed methods are based on deep learning, which require very large amounts of data compared to traditional methods. The performance of the deep learning methods are largely dependent on the data distribution they were trained on, and it is important to use data from the robot's actual operating domain during training. Therefore, it is not possible to rely on pre-built, generic datasets when deploying robots in real environments, creating a need for efficient data collection and annotation in the specific operating conditions the robots will operate in. The challenge is then: how do we reduce the cost of obtaining such datasets to a point where we can easily deploy our robots in new conditions, environments and to support new sensors? As an answer to this question, we propose a data annotation pipeline based on SLAM, 3D reconstruction, and 3D-to-2D geometry. The pipeline allows creating 3D and 2D bounding boxes, along with per-pixel annotations of arbitrary objects without needing accurate 3D models of the objects prior to data collection and annotation. Our results showcase almost 90% Intersection-over-Union (IoU) agreement on both semantic segmentation and 2D bounding box detection across a variety of objects and scenes, while speeding up the annotation process by several orders of magnitude compared to traditional manual annotation.",POS
398,psn6vk,[D] List of Literature Review Tools,https://www.reddit.com/r/MachineLearning/comments/psn6vk/d_list_of_literature_review_tools/,_Arsenie_Boca_,MachineLearning,1970-01-01 00:00:01.632244229,Discussion,23,16,"Searching and categorizing references is a tedious task that requires a lot of manual work. In recent years, many tools/services have been proposed, that aim to make it more efficient. I have collected a few of those tools below. Search* [https://scholar.google.de](https://scholar.google.de)* [https://www.semanticscholar.org/](https://www.semanticscholar.org/)* [https://arxiv.org/](https://arxiv.org/)* [http://www.arxiv-sanity.com/](http://www.arxiv-sanity.com/)* [https://sci-genie.com/](https://sci-genie.com/)Fancy visualizations* [https://citationgecko.azurewebsites.net/](https://citationgecko.azurewebsites.net/)* [https://www.connectedpapers.com/](https://www.connectedpapers.com/)Organization* [https://www.zotero.org/](https://www.zotero.org/)* [https://www.mendeley.com/](https://www.mendeley.com/)* [https://www.citavi.com/](https://www.citavi.com/de)* [https://www.bibcitation.com/](https://www.bibcitation.com/)What does your tool stack look like in literature reviews? Are there others than the ones mentioned above? Do you particularly like or dislike any of them?",POS
399,psnyft,[D] Probabilistic Modelling of Large Count Data,https://www.reddit.com/r/MachineLearning/comments/psnyft/d_probabilistic_modelling_of_large_count_data/,WigglyHypersurface,MachineLearning,1970-01-01 00:00:01.632246465,Discussion,21,16,"I'm trying to diagnose a deep network's failure to learn for my use case. I'm building an autoencoder for tabular data, including some count variable outputs. These counts have a wide range (from 1 to 1,000,000+) and a large number of 1s. However, whenever I try to model these counts my network fails to learn anything. Remove them and the model works fine. The model has no issues for count variables with a smaller range. Things I've tried:1) Using a poisson distribution as a head - leads to nans in loss late in training. 2) Using a negative binomial distribution as the head - total count parameter becomes almost zero, logits parameter stays in the 7-9 range. 3) Using a negative binomial mixture with 1 to 5 mixtures - same as 2 just for each distribution.4) Using a 1 inflated negative binomial mixture - same. 5) gradient clipping, scaling inputs, log transform + scaling count inputs, positive value bias initializations, switching optimizers, setting min total counts in binomial distributions to 1Nothing has helped. Modelling counts after a log transform works, but is ugly and a little hacky. Any thoughts on stable training for count variables with a very wide range, without transformation, would be helpful. Currently using tensorflow and tensorflow probability.",NEG
400,pt3j6b,[R] what happens if you use Bayesian filter before object detection in tracking,https://www.reddit.com/r/MachineLearning/comments/pt3j6b/r_what_happens_if_you_use_bayesian_filter_before/,ozgurerkent,MachineLearning,1970-01-01 00:00:01.632301486,Research,1,0,"[https://hal.inria.fr/hal-03335282/](https://hal.inria.fr/hal-03335282/) ""GridTrack: Detection and Tracking of Multiple Objects in Dynamic Occupancy Grids"", what happens if you use Bayesian filter before object detection in tracking? You can also attend ICVS'21 [http://icvs.acin.tuwien.ac.at/index.html](http://icvs.acin.tuwien.ac.at/index.html)",POS
401,pt2k3o,[D] Can I apply '4-way k-shot learning' to a dataset with '4 classification types'?,https://www.reddit.com/r/MachineLearning/comments/pt2k3o/d_can_i_apply_4way_kshot_learning_to_a_dataset/,BoxWorld_Kor,MachineLearning,1970-01-01 00:00:01.632296641,Discussion,0,1,"Hello I am studying few shot learning.Most of the papers use '**Omniglot**' or '**miniImageNet**' to compare the performance of models using '**5-way**' or '**10-way**'.(these datasets have at least 100 classes.)&#x200B;However, If the dataset I want to solve has **4 different classes**,is it okay to train the model in '**4-way**'?",POS
402,psj8dm,[D] GPT-3 is a LIAR - Misinformation and fear-mongering around the TruthfulQA dataset (Video Critique),https://www.reddit.com/r/MachineLearning/comments/psj8dm/d_gpt3_is_a_liar_misinformation_and_fearmongering/,ykilcher,MachineLearning,1970-01-01 00:00:01.632232577,Discussion,28,50,"[https://youtu.be/aX8phGhG8VQ](https://youtu.be/aX8phGhG8VQ)A new benchmark paper has created quite an uproar in the community. TruthfulQA is a dataset of 817 questions probing for imitative falsehoods where language models become less truthful, the larger they get. This surprising counter-intuitive finding validates many people's criticisms of large language models, but is it really the correct conclusion?&#x200B;OUTLINE:0:00 - Intro0:30 - Twitter Paper Announcement4:10 - Large Language Models are to blame!5:50 - How was the dataset constructed?9:25 - The questions are adversarial12:30 - Are you surprised?!&#x200B;Paper: [https://arxiv.org/abs/2109.07958](https://arxiv.org/abs/2109.07958)",POS
403,psapqm,"[R] Primer: Searching for Efficient Transformers for Language Modeling. â€œWe use evolution to design a new Transformer variant, called Primer. Primer has a better scaling law, and is 3X to 4X faster for training than Transformer for language modeling.â€",https://arxiv.org/abs/2109.08668,hardmaru,MachineLearning,1970-01-01 00:00:01.632195713,Research,130,17,,NEU
404,psj5hl,[R] HuggingFace Uses Block Pruning to Speedup Transformer Training While Maintaining Accuracy,https://www.reddit.com/r/MachineLearning/comments/psj5hl/r_huggingface_uses_block_pruning_to_speedup/,Yuqing7,MachineLearning,1970-01-01 00:00:01.632232307,Research,21,1,"A research team from Hugging Face introduces a block pruning approach targeting both small and fast models, which learns to eliminate full components of the original model while effectively dropping a large number of attention heads. Here is a quick read: [Hugging Face Uses Block Pruning to Speedup Transformer Training While Maintaining Accuracy.](https://syncedreview.com/2021/09/21/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-108/)The paper *Block Pruning For Faster Transformers* is on [arXiv](https://arxiv.org/abs/2109.04838).",POS
405,psijq4,[P] Knowledge Graph Completion With CoDEx,https://www.reddit.com/r/MachineLearning/comments/psijq4/p_knowledge_graph_completion_with_codex/,tdls_to,MachineLearning,1970-01-01 00:00:01.632230257,Project,15,2,"# Knowledge Graph Completion With CoDExI have been looking into Knowledge Graph completion task and collected some resources that I would to share with you here.1. Use [Graph Representation Learning: The Graph Neural Network Model](https://www.cs.mcgill.ca/~wlh/grl_book/files/GRL_Book-Chapter_5-GNNs.pdf)  in order to answer questions like Why do we need graph neural networks? What is a graph neural network? What is message passing? What are the typical GNNs? How do you use node and edge features?. This is a book chapter and I would recommend reading the whole chapter.2. Use [A Survey on Knowledge Graphs: Representation, Acquisition and Applications](https://arxiv.org/abs/2002.00388)  in order to answer questions like How to represent and use knowledge graphs?. Read sections 2 and 33. Use [Pay Attention, Relations are Important](https://deepakn97.github.io/blog/2019/Knowledge-Base-Relation-Prediction/)  in order to answer questions like How to use attention mechanism on knowledge graph completion task? Use this item in combination with the paper L[earning Attention-based Embeddings for Relation Prediction in Knowledge Graphs](https://arxiv.org/abs/1906.01195) to gain knowledge on how [KBAT](https://arxiv.org/pdf/1911.03903.pdf) works.4. Use [Learning Attention-based Embeddings for Relation Prediction in Knowledge Graphs](https://arxiv.org/abs/1906.01195)  in order to answer questions like How to apply attention mechanism to GNNs? What is relation prediction on knowledge graphs? Read section 3 and 4. If interested, read section 2 for related works.5. Use [CoDEx: A Comprehensive Knowledge Graph Completion Benchmark](https://arxiv.org/pdf/2009.07810.pdf)  in order to answer questions like What is CoDEx? What problems does CoDEx solve compared with previous knowledge graph completion benchmarks? What is the performance of some classical knowledge graph completion models on CoDEx?. Read section 2, 3, and 5.We are forming a discussion group to go through these resources and implement some parts of it. If you are interested to join, please let me knowps. there's an interactive version of these resources [here](https://ai.science/l/d32fa8ed-5eb7-40a5-a774-853c0e737a39@/recipes)",POS
406,psyyas,[Discussion] Is MLP Mixer memory optimised,https://www.reddit.com/r/MachineLearning/comments/psyyas/discussion_is_mlp_mixer_memory_optimised/,ole72444,MachineLearning,1970-01-01 00:00:01.632281279,Discussion,0,10,"MLP Mixer uses FC layers instead of CNNs. Doesn't that mean it will have to deal with more parameters overall in the network and hence more memory? One of the reasons convolutions were preferred was due to the reduction in number of model parameters as compared to FC networks. Also, they've mentioned this in their paper that MLP Mixer is focussed more on scale/deployability. Aren't memory constraints factored in?Am I missing/misinterpreting something?",POS
407,pt34a5,"[N] Salesforce Research Introduces â€˜Merlionâ€™, An Open-Source Machine Learning Library For Time Series Intelligence",https://www.reddit.com/r/MachineLearning/comments/pt34a5/n_salesforce_research_introduces_merlion_an/,techsucker,MachineLearning,1970-01-01 00:00:01.632299411,News,0,2,"Time series are a way to visualize and analyze the behavior of complex systems. They can represent key performance indicators for computing resources such as memory utilization or request latency, business metrics like revenue per day active users (DAU), marketing campaigns in social media reactions from customers through clickthrough rate. It is important to forecast the trends and values of key metrics accurately and rapidly detect a ny anomalies in those numbers. In software industries, anomaly detection is a critical machine learning technique to automate the identification of issues and improve IT system availability. It notifies the operator in a timely manner when something unexpected happens and helps them resolve underlying issues.Salesforce research team introduces [Merlion](https://arxiv.org/pdf/2109.09265.pdf), an open-source Python library for time series intelligence. Merlion offers an end-to-end machine learning framework comprised of loading data, transforming it into useable formats, and building models. Once training has been completed, post-processing can include many aspects such as evaluating model performance or making predictions on what will happen in the future based on historical trends with some degree of accuracy.# [5 Min Read](https://www.marktechpost.com/2021/09/22/salesforce-research-introduces-merlion-an-open-source-machine-learning-library-for-time-series-intelligence/) | [Paper](https://arxiv.org/pdf/2109.09265.pdf) | [Code](https://github.com/salesforce/Merlion)&#x200B;https://preview.redd.it/erh18xymm0p71.png?width=984&format=png&auto=webp&s=4b229b8f1a6c22a5dea3047575c4ed0b2d65702e",POS
408,psni6i,[D] New Research Paper video Explainer: Block Pruning For Faster Transformers by Hugging Face,https://www.reddit.com/r/MachineLearning/comments/psni6i/d_new_research_paper_video_explainer_block/,gauravc2796,MachineLearning,1970-01-01 00:00:01.632245127,Discussion,6,0," Hi,A new research paper explainer has been released:Block Pruning For Faster Transformers by Hugging Face [https://youtu.be/CyJdzkcdGl0](https://youtu.be/CyJdzkcdGl0)Video explains the basics of pruning, distillation, paper overview, and also codes.TLDR:1. Large pre-trained NN to have billions of parameters to train. It is computationally expensive to load or download these models.2. Here comes pruning and distillation. Both of these techniques try to eliminate parameters (weights) with very little drop in accuracy.3. If we want to prune weights when finetuning our model, then movement pruning works best for that. However, the time and space requirements to train these models again to drop unnecessary could be expensive.4. Here comes block pruning where it creates square blocks in the weight matrix and then tries to perform movement pruning on the blocks rather than weights.For an in-depth overview refer to the video explainer.If you like this content, do share it with friends and support the channel. If any opinions, clarifications, or anything you can mention in the comments.",POS
409,psrtur,[N] Open Sourcing Checkpoint ðŸ›‚,https://www.reddit.com/r/MachineLearning/comments/psrtur/n_open_sourcing_checkpoint/,StochasticSolver,MachineLearning,1970-01-01 00:00:01.632257712,News,2,5,"The Domino R&D team is open-sourcingÂ [Checkpoint](https://bit.ly/3CBYOcH), a tool that introduces a lightweight ""pull request for machine learning"" on top of your model registry (in this case MLflow). We are also publishing a [research note](https://medium.com/domino-research/checkpoint-300db528efeb) explaining why we think the code PR is only a part of the ML versioning story.Check out the [quickstart](https://bit.ly/3lKsBJt) and give us feedback.",POS
410,psivqv,GraphSAGE: Inductive Representation Learning in Large Graphs (Paper Walkthrough) [D],https://www.reddit.com/r/MachineLearning/comments/psivqv/graphsage_inductive_representation_learning_in/,prakhar21,MachineLearning,1970-01-01 00:00:01.632231403,Discussion,6,2,"GraphSAGE proposes a general, inductive framework that leverages node feature information (e.g., text attributes, node degree) to efficiently generate node embeddings for nodes not seen during training time. ðŸ”¥ Watch Paper summary at https://youtu.be/3AzphNf5ja8Paper details in the comments!",POS
411,psasw4,[R] Modern Evolution Strategies for Creativity: Fitting Concrete Images and Abstract Concepts,https://www.reddit.com/r/MachineLearning/comments/psasw4/r_modern_evolution_strategies_for_creativity/,RichardRNN,MachineLearning,1970-01-01 00:00:01.632196055,Research,28,1,"Website with samples: https://es-clip.github.io/Code: https://github.com/google/brain-tokyo-workshop/Paper: https://arxiv.org/abs/2109.08857Saw a [tweet](https://twitter.com/alanyttian/status/1440124243241422855) about this work that uses evolution strategies to fit simple shapes to CLIP (and also images), and produces cool looking abstract art.**Abstract**Evolutionary algorithms have been used in the digital art scene since the 1970s. A popular application of genetic algorithms is to optimize the procedural placement of vector graphic primitives to resemble a given painting. In recent years, deep learning-based approaches have also been proposed to generate procedural drawings, which can be optimized using gradient descent. In this work, we revisit the use of evolutionary algorithms for computational creativity. We find that modern evolution strategies (ES) algorithms, when tasked with the placement of shapes, offer large improvements in both quality and efficiency compared to traditional genetic algorithms, and even comparable to gradient-based methods. We demonstrate that ES is also well suited at optimizing the placement of shapes to fit the CLIP model, and can produce diverse, distinct geometric abstractions that are aligned with human interpretation of language.",POS
412,pssamx,[P] Experimenting with GPT-2 for Song Generation without Fine-Tuning,https://www.reddit.com/r/MachineLearning/comments/pssamx/p_experimenting_with_gpt2_for_song_generation/,Turbulent_Dog_628,MachineLearning,1970-01-01 00:00:01.632258957,Project,1,0,"Hey all!&#x200B;**Motivation:** I like NLP, and I like music, so I always thought the demos of using generative models to write music were pretty cool. However, it seemed like most of these demos required fine-tuning on song data, which is cumbersome and computationally expensive. Of course, GPT-3 does a terrific job without any fine-tuning, but the weights aren't open-sourced on HuggingFace, so I thought it'd be a more interesting/fun challenge to try to use GPT-2.**Goal:** Use post-processing hacks to generate song lyrics using GPT-2. Specifically, I took advantage of Word2Vec (in gensim) and CMU's Pronouncing Dictionary. The Hook is supposed to be AAAA and the Verse is supposed to be ABABABAB, but would be cool to experiment with different rhyme schemes / internal rhymes.**Results:** Results were mixed, but I thought they were pretty funny, so here you go. I slapped this together in a few hours of free time so I'm sure there are lots of improvements that could be made: I'm leaving the source code here if anyone wants to play around with it.&#x200B;*Example Song Lyrics:*Topic: ""Pain"">\[hook\]:  >  >I dont know how to  >  >Say this but new  >  >Not like youre going to  >  >Be able to tell me do  >  >\[verse\]:  >  >I dont hey  >  >What im going to they  >  >With my life right today  >  >But its gonna be okay  >  >In the halfway  >  >If you want me to stay  >  >Out on a limb and say  >  >That this isnt good they  >  >\[hook\]:  >  >I dont know how to  >  >Say this but new  >  >Not like youre going to  >  >Be able to tell me doTopic: ""Love"">\[hook\]:  >  >I dont know how to  >  >Say this but new  >  >Not like youre going to  >  >Be able to tell me do  >  >\[verse\]:  >  >Love is like a  >  >Fire in the night and  >  >You cant help but do  >  >It burning inside of you  >  >Its hard to  >  >Believe that there are so few  >  >People out there who dont  >  >Know what theyre talking do  >  >\[hook\]:  >  >I dont know how to  >  >Say this but new  >  >Not like youre going to  >  >Be able to tell me doTopic: ""Success"">\[hook\]:  >  >I dont know how to  >  >Say this but new  >  >Been a long time since u  >  >ve had the opportunity to kazoo  >  >\[verse\]:  >  >Im going to  >  >Be the best i can be  >  >you know what i do  >  >you dont have to  >  >Look at me like that do  >  >its just a he  >  >Of time before were two  >  >In this we  >  >\[hook\]:  >  >I dont know how to  >  >Say this but new  >  >Been a long time since u  >  >ve had the opportunity to kazoo&#x200B;    import gensim    import string    import torch    import pronouncing        from nltk.data import find    from tqdm import tqdm    from transformers import GPT2LMHeadModel, GPT2Tokenizer        tokenizer = GPT2Tokenizer.from_pretrained(""gpt2"")    model = GPT2LMHeadModel.from_pretrained(""gpt2"", pad_token_id=tokenizer.eos_token_id)        NUM_VERSE_LINES = 8    NUM_HOOK_LINES = 4    LINE_LENGTH = 6    NUM_BEAMS = 10    TOPIC = ""success""    CONTEXT = f""Lyrics for song about {TOPIC}.""        VERSE_TO_LINES = {}    word2vec_sample = str(find(""models/word2vec_sample/pruned.word2vec.txt""))    W2V_MODEL = gensim.models.KeyedVectors.load_word2vec_format(        word2vec_sample, binary=False    )            def rhyme_together(lines):        new_lines = []        max_rhymes, keep_index = -1, None        for index, line in enumerate(lines):            rhyme = line.split()[-1]            num_rhymes = len(pronouncing.rhymes(rhyme))            if num_rhymes > max_rhymes:                max_rhymes, keep_index = num_rhymes, index        rhymes = pronouncing.rhymes(lines[keep_index].split()[-1])        for index, line in enumerate(lines):            if index == keep_index:                new_lines.append(line)                continue            line_lst = line.split()            last_word = line_lst[-1]            best_similarity, new_last_word = 0, last_word            for rhyme in rhymes:                try:                    sim = W2V_MODEL.similarity(last_word, rhyme)                    if sim > best_similarity:                        best_similarity, new_last_word = sim, rhyme                except KeyError:                    continue            new_line_lst = line_lst[:-1] + [new_last_word]            new_line = "" "".join(new_line_lst)            new_lines.append(new_line)        return new_lines            def rhymify_hook(hook):        hook = rhyme_together(hook)        return hook            def rhymify_verse(verse):        verse[::2] = rhyme_together(verse[::2])        verse[1::2] = rhyme_together(verse[1::2])        return verse            def isalpha_space(text_output):        return all(            [                x.isspace()                or x in string.ascii_lowercase                or x in string.ascii_uppercase                or x in ""',.:;!?""                for x in text_output            ]        )            def generate(input_ids, prev_length=None):        curr_length = len(input_ids[0])        outputs = model.generate(            input_ids,            temperature=1,            repetition_penalty=5.0,            max_length=curr_length + LINE_LENGTH,            min_length=curr_length + LINE_LENGTH,            num_beams=NUM_BEAMS,            num_return_sequences=NUM_BEAMS,            early_stopping=True,            diversity_penalty=0.5,        )        output = outputs[0]        text_output = tokenizer.decode(output[curr_length:], skip_special_tokens=True)        ind = 1        while not isalpha_space(text_output) and ind < len(outputs):            output = outputs[ind]            text_output = tokenizer.decode(output[curr_length:], skip_special_tokens=True)            ind += 1        text_output = text_output.strip().capitalize().translate(str.maketrans('', '', string.punctuation))        text_output = """".join([x for x in text_output if x != ""\n""])        new_length = len(output)        output = output.view(1, -1)        return text_output, output, curr_length            def generate_hook():        lines = []        input_ids = tokenizer.encode(CONTEXT + "" Hook:"", return_tensors=""pt"")        print(""Generating hook..."")        prev_length = None        for _ in tqdm(range(NUM_HOOK_LINES)):            text_output, input_ids, prev_length = generate(input_ids, prev_length)            lines.append(text_output)        lines = rhymify_hook(lines)        VERSE_TO_LINES[0] = lines            def generate_verse():        lines = []        input_ids = tokenizer.encode(CONTEXT + "" Verse:"", return_tensors=""pt"")        print(f""Generating verse..."")        prev_length = None        for _ in tqdm(range(NUM_VERSE_LINES)):            text_output, input_ids, prev_length = generate(input_ids, prev_length)            lines.append(text_output)        lines = rhymify_verse(lines)        VERSE_TO_LINES[1] = lines            def generate_song():        generate_verse()        generate_hook()            def print_song():        print()        print(""===Song Lyrics==="")        print(TOPIC)        hook = ""\n[hook]:\n"" + ""\n"".join(VERSE_TO_LINES[0]) + ""\n""        print(hook)        print(f""[verse]:"")        print(""\n"".join(VERSE_TO_LINES[1]) + ""\n"")        print(hook)            def main():        generate_song()        print_song()            if __name__ == ""__main__"":        main()",POS
413,psmonx,[D] Are there appropriate datasets for Fake News detection with related social media posts?,https://www.reddit.com/r/MachineLearning/comments/psmonx/d_are_there_appropriate_datasets_for_fake_news/,theamaru,MachineLearning,1970-01-01 00:00:01.632242800,Discussion,2,3,"I've been working on some ideas for Fake News detection, but the publicly available datasets usually only contain articles that are either fake or real. Datasets like FakeNewsNet are usually reported as a benchmark in the papers I read, but even though that the papers are recent (2021), that dataset is already broken for nearly a year.Usually, these fake news datasets only contain references to tweets or Facebook posts, that one needs to download with a script and some API key. If the referenced post was deleted (as it is promoting fake news), the data is gone.When I was downloading FakeNewsNet, it took ages and in the end, I only had around 250 MB of the 5 GB dataset. All retweets of the fake news were missing.Does anybody have a copy of a more complete FakeNewsNet dataset? Do you guys and gals know of any alternative?",NEG
414,psr1hr,[D] In which forms did self-supervised learning exist before deep learning?,https://www.reddit.com/r/MachineLearning/comments/psr1hr/d_in_which_forms_did_selfsupervised_learning/,PaganPasta,MachineLearning,1970-01-01 00:00:01.632255405,Discussion,1,4,"The title says it all. I'd be surprised if it didn't exist before the deep learning era. However, any recent article I have read only goes back couple of years on it. Wikipedia is also infested with recent self-promoting edits.",POS
415,psl8mj,[D] Advances on structured data modeling in past 5 years ?,https://www.reddit.com/r/MachineLearning/comments/psl8mj/d_advances_on_structured_data_modeling_in_past_5/,JurrasicBarf,MachineLearning,1970-01-01 00:00:01.632238593,Discussion,2,6,"Got handed a tabular problem at work. Current models use Catboost to extract juice. Before I begin, I'm wondering if there are any new hacks/advances made that I can use to boost performance.p.s. we absolutely only care about generalized performance, no need for interpretability.I'm thinking of leveraging clusters from UMAP as features for starters + Stacking.",POS
416,pspsow,"[D] Divide the data into different groups, and train independently?",https://www.reddit.com/r/MachineLearning/comments/pspsow/d_divide_the_data_into_different_groups_and_train/,randy_wales_qq,MachineLearning,1970-01-01 00:00:01.632251785,Discussion,0,10,"If I have a training data, I divide them onto different group. Then, for each group, I use a different neural network to train seperately/independently.  Do you think this will be better than using a single neural network and train with ALL training data?",POS
417,prw2ly,[R] DeepMindâ€™s Bootstrapped Meta-Learning Enables Meta Learners to Teach Themselves,https://www.reddit.com/r/MachineLearning/comments/prw2ly/r_deepminds_bootstrapped_metalearning_enables/,Yuqing7,MachineLearning,1970-01-01 00:00:01.632148289,Research,137,11,"A research team from DeepMind proposes a bootstrapped meta-learning algorithm that overcomes the meta-optimization problem and myopic meta objectives, and enables the meta-learner to teach itself. Here is a quick read: [DeepMindâ€™s Bootstrapped Meta-Learning Enables Meta Learners to Teach Themselves.](https://syncedreview.com/2021/09/20/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-107/)The paper *Bootstrapped Meta-Learning* is on [arXiv](https://arxiv.org/abs/2109.04504).",NEG
418,prxpvy,[D] Current situation about M1 support of python libraries ?,https://www.reddit.com/r/MachineLearning/comments/prxpvy/d_current_situation_about_m1_support_of_python/,bangbangcontroller,MachineLearning,1970-01-01 00:00:01.632153435,Discussion,95,54," I am a Data Scientist and intended to buy a new M1 macbooks, air or pro, for my data science works but there were a lot of feedbacks which mentions that most of the libraries are not configured for ARM. There are also some people say ""M1 is machine learning beast"". I am mostly using Pandas, Tensorflow, Scikitlearn, Numpy, Flask and Plotly for my studies.What are your experiences about the usage of M1 chip with python libraries? Which libraries you use perfectly and which are not supported?Is it a good idea to buy M1 for Data Science or is it better to wait?",POS
419,ps0d02,"[P] ""A truck with the text JCN"" - CLIP is scarily good at surveillance forensic search",https://www.reddit.com/r/MachineLearning/comments/ps0d02/p_a_truck_with_the_text_jcn_clip_is_scarily_good/,PonenCreatesThings,MachineLearning,1970-01-01 00:00:01.632161408,Project,52,12,"Query: ""A truck with the text JCN""[CLIP image response](https://github.com/johanmodin/clifs/blob/master/media/jcn.jpg)It started as an idea from the CLIP paper, in which the authors mentioned that CLIP, while impressive, didn't work very well in a surveillance setting. I would disagree, it works uncannily good.I put together a small proof-of-concept of using CLIP in video forensic search. It takes videos, breaks them up into images and patches and encodes them. These encodings are matched by similarity with a user's input and the corresponding frame is returned.The very simple, PoC app with a web UI and some (in my opinion) crazy example queries can be found here: https://github.com/johanmodin/clifs",NEG
420,ps2rww,[D] Paper Explained - Topographic VAEs learn Equivariant Capsules (Full Video Analysis),https://www.reddit.com/r/MachineLearning/comments/ps2rww/d_paper_explained_topographic_vaes_learn/,ykilcher,MachineLearning,1970-01-01 00:00:01.632168747,Discussion,22,0,"[https://youtu.be/pBau7umFhjQ](https://youtu.be/pBau7umFhjQ)Variational Autoencoders model the latent space as a set of independent Gaussian random variables, which the decoder maps to a data distribution. However, this independence is not always desired, for example when dealing with video sequences, we know that successive frames are heavily correlated. Thus, any latent space dealing with such data should reflect this in its structure. Topographic VAEs are a framework for defining correlation structures among the latent variables and induce equivariance within the resulting model. This paper shows how such correlation structures can be built by correctly arranging higher-level variables, which are themselves independent Gaussians.&#x200B;OUTLINE:0:00 - Intro1:40 - Architecture Overview6:30 - Comparison to regular VAEs8:35 - Generative Mechanism Formulation11:45 - Non-Gaussian Latent Space17:30 - Topographic Product of Student-t21:15 - Introducing Temporal Coherence24:50 - Topographic VAE27:50 - Experimental Results31:15 - Conclusion & Comments&#x200B;Paper: [https://arxiv.org/abs/2109.01394](https://arxiv.org/abs/2109.01394)Code: [https://github.com/akandykeller/topographicvae](https://github.com/akandykeller/topographicvae)",POS
421,psokos,[D] Tutorials in ML conferences,https://www.reddit.com/r/MachineLearning/comments/psokos/d_tutorials_in_ml_conferences/,EpicProf,MachineLearning,1970-01-01 00:00:01.632248248,Discussion,0,3,"I am looking for call for tutorials in a ML conference, any suggestions?Thanks a lot!",NEU
422,ps5ubp,[R] Facebook AI Introduces A New Image Generation Model Called â€˜IC-GANâ€™ That Creates High-Quality Images of Unfamiliar Objects And Scenes,https://www.reddit.com/r/MachineLearning/comments/ps5ubp/r_facebook_ai_introduces_a_new_image_generation/,techsucker,MachineLearning,1970-01-01 00:00:01.632178410,Research,8,5,"Generative adversarial networks (GANs) have been used for few years to generate photorealistic images of objects or scenes that are very similar in style and content. However, until now, these models could only produce output related to datasets they were trained on â€“ which had limitations because there was usually less diversity among those files than what you would find when generating new ideas. A conventional GAN trained on images of cars shows impressive results when asked to generate other images of cars or automobiles. But the trained GAN will likely fail if given a flower or any object outside its automotive data set.Failure to show non-identical objects from the training dataset is a huge limitation, and it definitely needs to be resolved to meet the demand. Facebook is trying to solve the above problem by introducing [Instance-Conditioned GAN (IC-GAN)](https://github.com/facebookresearch/ic_gan?). The [IC-GAN](https://arxiv.org/pdf/2109.05070.pdf) is a new image generation model that can produce high-quality images with some input, even if it doesnâ€™t appear in the training set. The unique thing about the IC-GAN model is that it can generate realistic, unforeseen image combinationsâ€”for example, a camel in snow or zebras running through an urban cityscape.It is no surprise that IC-GANs could be used to create visual examples for data sets with these new capabilities. This would allow artists and creators alike more expansive AI-generated content by creating art from photos or videos in the same way an artist might draw a picture using pencils and paintbrushes at their disposal.# [5 Min Read](https://www.marktechpost.com/2021/09/20/facebook-ai-introduces-a-new-image-generation-model-called-ic-gan-that-creates-high-quality-images-of-unfamiliar-objects-and-scenes/) | [Paper](https://arxiv.org/abs/2109.05070?) | [Code](https://github.com/facebookresearch/ic_gan?) | [Facebook Blog](https://ai.facebook.com/blog/instance-conditioned-gans/) &#x200B;https://preview.redd.it/l1f1ubcvmqo71.png?width=1392&format=png&auto=webp&s=cf59a4f9c79b58a3dfca29a732ae05826e8a601e",POS
423,pryveo,[D] Computer Vision as Inverse Computer Graphics?,https://www.reddit.com/r/MachineLearning/comments/pryveo/d_computer_vision_as_inverse_computer_graphics/,chip_0,MachineLearning,1970-01-01 00:00:01.632156929,Discussion,22,11,"Inverse Computer Graphics aims to solve the problem of computer vision end-to-end by having a model that can take an image (or sequence of images taken from different view points with known relative positioning), and output a 3D mesh of the world that generated these images.Many of the renowned researchers hold up Inverse Computer Graphics as one of the benchmarks for AI. [Hinton](https://www.cs.toronto.edu/~hinton/csc2535/notes/lec6b.pdf) is definitely the most prominent, and a lot of his recent research (like Capsule Networks) aims to address that.[Karpathy](https://twitter.com/karpathy/status/1391904502458978305?lang=en) got in the game as well, although I am not sure how exactly the research he cites relates to this problem.I personally find this area very interesting since I love both 3D graphics and AI.What do you all think of this AI dream, and do you have any favorite ideas or approaches that you think can solve it some day?",POS
424,psej45,[P] Looking for help with some derivations (Generative Local Metric Learning for Nearest Neighbor Classification),https://www.reddit.com/r/MachineLearning/comments/psej45/p_looking_for_help_with_some_derivations/,wakeupandshave,MachineLearning,1970-01-01 00:00:01.632212529,Project,1,0,"I am working on deep metric learning, or maybe its ""contrastive learning"" as I don't explicitly select negative/dissimilar examples, and using kNN as the classifier.I feel like this paper ([https://proceedings.neurips.cc/paper/2010/hash/01386bd6d8e091c2ab4c7c7de644d37b-Abstract.html](https://proceedings.neurips.cc/paper/2010/hash/01386bd6d8e091c2ab4c7c7de644d37b-Abstract.html)) might offer some insight into what I'm doing, however, some of the steps/statements are unclear. If anyone could be bothered to go through this, that would be much appreciated!I agree with Equation 2, except I think there must be a factor of 2 in front, and that is assuming the notation p\_1(x) = p(x, c\_1) is the joint probability (density?) of the class label c\_1 and point x. And assuming only 2 classes.Then, chapter 3 starts, and straight away I am lost where they take the ""Expectation of a probability density function"", speficially E\[p(x\_NN)\] over the hypersphere of radius d\_N. While the result of taking this expectation I agree with, I don't understand what is the resultant mathematical object and therefore how it could be used.",POS
425,ps6pxv,Does anyone know what ML/Neuro project Google X is working on? [D] [T],https://www.reddit.com/r/MachineLearning/comments/ps6pxv/does_anyone_know_what_mlneuro_project_google_x_is/,HenryWu001,MachineLearning,1970-01-01 00:00:01.632181424,Discussion,3,5,"I have attempted to find an answer for this question already - if I've missed a thread and this is a duplicate I apologize.I was going through the Google X website, and noticed a whole bunch of machine learning and neuroscience positions. Given that Alphabet already has Research/Brain/AI and DeepMind, I was curious as to what AI/ML project would be embedded into X (particularly given Brain ""graduated"" out of X years ago).PhD Residency in:* AI / ML: Coding & Program Synthesis* AI / ML: NLP, Abstraction, Reasoning* Artifical Intelligence* Computational Neuroscience, Large Scale Simulation (is Google doing a Blue Brain / Human Brain project run?)* Time-series forecasting - causal inference* Computer Vision - Machine LearningEngineering:* Machine Learning Lead Engineer, Early Stage Project* Machine Learning Software Engineer (Causality Researcher), Early Stage X Project* Machine Learning Software Engineer (Predictive Modelling), Early Stage Project* Machine Learning Software Lead, X* Staff Software Engineer / Statistician, Early Stage X ProjectSo yeah, are there any press releases / rumors / insider info about what machine learning and/or computational neuroscience project X is working on at the moment. And why its secret in X rather than Brain or DeepMind?",POS
426,prycw5,[R] Does Knowledge Distillation Really Work?,https://arxiv.org/abs/2106.05945,hardmaru,MachineLearning,1970-01-01 00:00:01.632155351,Research,9,7,,NEU
427,przv13,"[D] Discussing abductive inference with the author of ""The Myth of Artificial Intelligence""",https://www.reddit.com/r/MachineLearning/comments/przv13/d_discussing_abductive_inference_with_the_author/,bendee983,MachineLearning,1970-01-01 00:00:01.632159881,Discussion,7,0,"In his book *The Myth of Artificial Intelligence*, computer scientist Erik J. Larson discusses the shortcomings of current approaches to AI. While there are several books on this topic, Larson puts much focus on abductive inference, something that is missing in current AI systems.I discussed abductive inference with Larson in a recent interview. Key highlights:\- Abductive inference is the cognitive ability to come up with hypotheses and intuitions, select the most likely causes out of many possibilities\- Abduction is very different from induction (e.g., ML) and deduction (e.g., symbolic AI). They cannot be reduced to each other. You can't reach general AI by scaling one type of inference. You need all three.\- Currently, there is not theory of abductive inference\- The current climate of AI research is skewed toward pouring more money and efforts into data-centric systems (ML/DL) at the cost of stifling efforts to explore new pathways to AIRead the full discussion here:[https://bdtechtalks.com/2021/09/20/myth-of-artificial-intelligence-erik-larson/](https://bdtechtalks.com/2021/09/20/myth-of-artificial-intelligence-erik-larson/)",NEG
428,ps3iij,[D] Is it possible (and useful) to use knowledge distillation for problems with highly-dimensional outputs?,https://www.reddit.com/r/MachineLearning/comments/ps3iij/d_is_it_possible_and_useful_to_use_knowledge/,adenml,MachineLearning,1970-01-01 00:00:01.632171005,Discussion,2,4,"I understand the role of distillation for classification tasks.However, does it even make sens to use knowledge distillation for other tasks, where the output is highly dimensional, such as image segmentation? Why / why not?Also, more concretely, what would be the teacher layers that you would enforce the student to learn from?",NEU
429,ps8mhu,[R] Towards Zero-Label Language Learning,https://arxiv.org/abs/2109.09193,koolaidman123,MachineLearning,1970-01-01 00:00:01.632188066,Research,1,1,,NEU
430,prycja,[D] How to incorporate data uncertainty in Neural Network model?,https://www.reddit.com/r/MachineLearning/comments/prycja/d_how_to_incorporate_data_uncertainty_in_neural/,Muunich,MachineLearning,1970-01-01 00:00:01.632155321,Discussion,7,12,"I am seeking advice for the best way to incorporate data uncertainty into my neural network model given my particular application and available a priori knowledge about the uncertainty of the data.My application is a regression task in the domain of 3D modelling to estimate a scalar value f\_i at a 3D point **x**\_i given a set of training points with each having an associated scalar value f. E.g. spatial interpolation.I have a priori knowledge about the uncertainty of the position of the training points. For a point **x**\_i = (x\_i, y\_i, z\_i) the uncertainty of point the x coordinate x\_i = +/- Î”x, and similarily for the y and z coordinate. Moreover, points that have larger z-coordinates have larger amounts of uncertainty.I see three options:1. Adding noise using a normal distribution to the coordinates of the training points based on the uncertainty of the coordinates. Each epoch, sample from this distribution.2. Using Bayesian deep learning techniques and hope that the learned posterior distribution of the weights and bias of NN parameters somehow captures the inherent data uncertainties. 3. Combination of 1) and 2)Wondering if anyone out there has some experience with incorporating data uncertainty and could provide some advice on this, and/or feedback on my options I noted above.Any help much appreciated.",POS
431,ps1akv,[D] How to track progress in deep learning model building procedure.,https://www.reddit.com/r/MachineLearning/comments/ps1akv/d_how_to_track_progress_in_deep_learning_model/,mrtac96,MachineLearning,1970-01-01 00:00:01.632164312,Discussion,3,4,"I am working on a problem, where i will try different deep learning models, augmentations, losses. At the end, there are alot of combinations and messy things. Is there a tool that can keep progress of what ever we try?",NEG
432,pre6dw,[D] Why is Facebook putting so much into Machine Learning relative to its business needs?,https://www.reddit.com/r/MachineLearning/comments/pre6dw/d_why_is_facebook_putting_so_much_into_machine/,AdditionalWay,MachineLearning,1970-01-01 00:00:01.632078199,Discussion,231,94,"I'm thinking maybe FB is the 2nd biggest corporate ML research institution, betting that Google is #1. It has the biggest research ML framework in Pytorch. What incentive is there for FB to put this much into ML, when most of their profits is for advertising, of which advertisers mostly use harvested user data?",POS
433,prcaoq,[D] Secure Live Collaboration in Jupyter Lab - https://elc.github.io/posts/jupyter-collaborative/,https://v.redd.it/po104xc5vho71,EzequielCastano,MachineLearning,1970-01-01 00:00:01.632072296,Discussion,290,17,,NEU
434,prreqo,[D] Are transformer-type models ready to replace MLP as the default general purpose model?,https://www.reddit.com/r/MachineLearning/comments/prreqo/d_are_transformertype_models_ready_to_replace_mlp/,svantana,MachineLearning,1970-01-01 00:00:01.632130719,Discussion,12,31,"For the general continuous case of R^(N) \--> R^(M) data with unknown structure, deep ReLU MLPs have been the go-to model for the last 10 years or so. Before that it was SVMs and GMMs. But recently transformer-type models such as DeepMind's Perceivers have had some good results while basically plug-n-play. These models are more expressive than MLPs, but I understand they can be harder to train. What do you think, are they ready for primetime or are more breakthroughs needed?",POS
435,ps508r,[Discussion] RotatE vs GNNs for link prediction,https://www.reddit.com/r/MachineLearning/comments/ps508r/discussion_rotate_vs_gnns_for_link_prediction/,someguyonline00,MachineLearning,1970-01-01 00:00:01.632175622,Discussion,0,7,"What are the pros and cons of using knowledge graph embedding methods like RotatE vs using GNNs for link prediction (the specific task is identifying drug repurposing candidates based on a knowledge graph of diseases, genes, etc. and relationships between them)? I would be using a very, very large knowledge graph if that helps.I havenâ€™t been able to find comparisons of these methods, and Iâ€™m sure thatâ€™s because of some misunderstanding on my part, but I donâ€™t know what that misunderstanding is.",NEG
436,praphz,"[R] Applying Artificial Intelligence & Machine Learning In Drug Discovery & Design - Dr. Ola Engkvist, Ph.D., Head, Molecular AI, Discovery Sciences, R&D, AstraZeneca",https://www.youtube.com/watch?v=BiMOxWMw4Lg,ObjectiveGround5,MachineLearning,1970-01-01 00:00:01.632067388,Research,96,0,,NEU
437,prua6p,[D] Paper on disentanglement of image representations and they are limitations and weaknesses?,https://www.reddit.com/r/MachineLearning/comments/prua6p/d_paper_on_disentanglement_of_image/,ThresholdTuner,MachineLearning,1970-01-01 00:00:01.632142491,Discussion,3,1,"I'm working on the disentanglement of image representations in generative models, but one thing I realize is that finding correct papers to look at is not trivial since many works under different titles could be classified as disentanglement such as gans, 2D-3D transformation, VAEs, image-to-image translation. Does anyone have some recommendations?",NEU
438,prewtd,"[P] I made tinymodels.io to deploy models quickly, feel free to use for your prototypes and web demos",https://www.reddit.com/r/MachineLearning/comments/prewtd/p_i_made_tinymodelsio_to_deploy_models_quickly/,greentfrapp,MachineLearning,1970-01-01 00:00:01.632080501,Project,22,11,"TLDR: I found it annoying to deploy models, so I built [tinymodels.io](https://tinymodels.io) to do super quick deployments.Feel free to try it out! It's relatively cheap to maintain so I don't mind opening it up for people to use it for prototypes and web demos.Drop a comment for any feature requests!",POS
439,prtqxw,[D] Significant differences in Training on 1 GPU vs 8GPU ?,https://www.reddit.com/r/MachineLearning/comments/prtqxw/d_significant_differences_in_training_on_1_gpu_vs/,PaganPasta,MachineLearning,1970-01-01 00:00:01.632140592,Discussion,2,12,"I am looking at the official code for MOCO-v2 linear evaluation here: [https://github.com/facebookresearch/moco](https://github.com/facebookresearch/moco)The command to execute suggests running a 256 sized batch across 8 GPUs for a Resnet-50. Do keep in mind that for the linear evaluation the model is frozen till the end and only the final fully connected layer is fine-tuned.>Inside the code:parser.add\_argument('-b', '--batch-size', default=256, type=int,    metavar='N',    help='mini-batch size (default: 256), this is the total batch size of all GPUs on the current node when using Data Parallel or Distributed Data Parallel')This training can be done on a single GPU(16GB), so what is the reason to use 8 to do the same task?&#x200B;\*\*EDIT\*\*  There appears to be an old thread on the same topic: [https://www.reddit.com/r/MachineLearning/comments/m781bv/d\_moco\_training\_using\_a\_single\_gpu/](https://www.reddit.com/r/MachineLearning/comments/m781bv/d_moco_training_using_a_single_gpu/)and an issue: [https://github.com/facebookresearch/moco/issues/47](https://github.com/facebookresearch/moco/issues/47)  These both talk about differences arising from independent batch-norm strategies. Perhaps, this is also a factor in linear evaluation as well?    ",POS
440,prk1xx,[D] Object-NeRF Paper Explained - Learning Object-Compositional Neural Radiance Field for Editable Scene Rendering (5-minute summary),https://www.reddit.com/r/MachineLearning/comments/prk1xx/d_objectnerf_paper_explained_learning/,KirillTheMunchKing,MachineLearning,1970-01-01 00:00:01.632098140,Discussion,6,0,"[Object-NeRF](https://preview.redd.it/aycfasmvzjo71.png?width=600&format=png&auto=webp&s=20d14fecfca1cb6c18244b88f31b9e8d56ed1d2d)NeRF models have come a long way since the initial â€œexplosionâ€ last year. Yet one of the things they still canâ€™t quite handle is scene compositionality, meaning that the model is not aware of the distinct objects that make up the scene. Object NeRF aims to tackle this issue using a dual-branch model that separately encodes the global context of the scene and each object in it. This approach not only reaches competitive levels of quality with current SOTA methods on static scenes but also enables object-level editing. For example, adding or moving furniture in a real-world scene.Check out the [full paper summary](https://www.casualganpapers.com/multi-object-nerf-3d-scene-editing/Object-NeRF-explained.html) on Casual GAN Papers (Reading time \~5 minutes).Subscribe to [my channel](https://t.me/casual_gan) and follow me on [Twitter](https://twitter.com/KirillDemochkin) for weekly AI paper summaries!",POS
441,pr4y4p,[D] Geometric Deep Learning Blueprint (Video on MLST),https://www.reddit.com/r/MachineLearning/comments/pr4y4p/d_geometric_deep_learning_blueprint_video_on_mlst/,timscarfe,MachineLearning,1970-01-01 00:00:01.632044607,Discussion,70,6,"YT: [https://youtu.be/bIZB1hIJ4u8](https://youtu.be/bIZB1hIJ4u8)Pod: [https://anchor.fm/machinelearningstreettalk/episodes/60-Geometric-Deep-Learning-Blueprint-Special-Edition-e17i495](https://anchor.fm/machinelearningstreettalk/episodes/60-Geometric-Deep-Learning-Blueprint-Special-Edition-e17i495)""Symmetry, as wide or narrow as you may define its meaning, is one idea by which man through the ages has tried to comprehend and create order, beauty, and perfection."" and that was a quote from Hermann Weyl, a German mathematician who was born in the late 19th century.Hey folks. Hope you don't mind me posting this here. I have stopped posting MLST stuff on this reddit, but I feel that this one in particular is pretty technical and of an academic nature and is relevant.We spoke with Professor Michael Bronstein (head of graph ML at Twitter) and Dr.  Petar VeliÄkoviÄ‡ (Senior Research Scientist at DeepMind), and Dr. Taco Cohen and Prof. Joan Bruna about their new proto-book Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges.  There is a long list of references given in the YouTube comments. Hope you enjoy!TOC: \[00:00:00\] Tim Intro\[00:01:55\] Fabian Fuchs article\[00:04:05\] High dimensional learning and curse\[00:05:33\] Inductive priors\[00:07:55\] The proto book\[00:09:37\] The domains of geometric deep learning\[00:10:03\] Symmetries\[00:12:03\] The blueprint\[00:13:30\] NNs don't deal with network structure (TedX)\[00:14:26\] Penrose - standing edition\[00:15:29\] Past decade revolution (ICLR)\[00:16:34\] Talking about the blueprint\[00:17:11\] Interpolated nature of DL / intelligence\[00:21:29\] Going tack to Euclid\[00:22:42\] Erlangen program\[00:24:56\] â€œHow is geometric deep learning going to have an impactâ€\[00:26:36\] Introduce Michael and Petar\[00:28:35\] Petar Intro\[00:32:52\] Algorithmic reasoning \[00:36:16\] Thinking fast and slow (Petar)\[00:38:12\] Taco Intro\[00:46:52\] Deep learning is the craze now (Petar)\[00:48:38\] On convolutions (Taco)\[00:53:17\] Joan Bruna's voyage into geometric deep learning\[00:56:51\] What is your most passionately held belief about machine learning? (Bronstein)\[00:57:57\] Is the function approximation theorem still useful? (Bruna)\[01:11:52\] Could an NN learn a sorting algorithm efficiently (Bruna)\[01:17:08\] Curse of dimensionality / manifold hypothesis (Bronstein)\[01:25:17\] Will we ever understand approximation of deep neural networks (Bruna)\[01:29:01\] Can NNs extrapolate outside of the training data? (Bruna)\[01:31:21\] What areas of math are needed for geometric deep learning? (Bruna)\[01:32:18\] Graphs are really useful for representing most natural data (Petar)\[01:35:09\] What was your biggest aha moment early (Bronstein)\[01:39:04\] What gets you most excited? (Bronstein)\[01:39:46\] Main show kick off + Conservation laws\[01:49:10\] Graphs are king\[01:52:44\] Vector spaces vs discrete\[02:00:08\] Does language have a geometry? Which domains can geometry not be applied? +Category theory\[02:04:21\] Abstract categories in language from graph learning\[02:07:10\] Reasoning and extrapolation in knowledge graphs\[02:15:36\] Transformers are graph neural networks?\[02:21:31\] Tim never liked positional embeddings\[02:24:13\] Is the case for invariance overblown? Could they actually be harmful?\[02:31:24\] Why is geometry a good prior?\[02:34:28\] Augmentations vs architecture and on learning approximate invariance\[02:37:04\] Data augmentation vs symmetries (Taco)\[02:40:37\] Could symmetries be harmful (Taco)\[02:47:43\] Discovering group structure (from Yannic)\[02:49:36\] Are fractals a good analogy for physical reality?\[02:52:50\] Is physical reality high dimensional or not?\[02:54:30\] Heuristics which deal with permutation blowups in GNNs\[02:59:46\] Practical blueprint of building a geometric network architecture\[03:01:50\] Symmetry discovering procedures\[03:04:05\] How could real world data scientists benefit from geometric DL?\[03:07:17\] Most important problem to solve in message passing in GNNs\[03:09:09\] Better RL sample efficiency as a result of geometric DL (XLVIN paper)\[03:14:02\] Geometric DL helping latent graph learning\[03:17:07\] On intelligence\[03:23:52\] Convolutions on irregular objects (Taco)",POS
442,prc4hl,[D] TalkNET Voice Cloning - This video is entirely narrated by vocal synthesis,https://youtu.be/MjaE0FjDHc8,cloud_weather,MachineLearning,1970-01-01 00:00:01.632071750,Discussion,14,5,,NEU
443,pqpl7m,[R] Decoupling Magnitude and Phase Estimation with Deep ResUNet for Music Source Separation,https://v.redd.it/xc8och9egao71,Illustrious_Row_9971,MachineLearning,1970-01-01 00:00:01.631982556,Research,847,45,,NEU
444,prfa0x,[P] DLPrimitives - wondering about best development direction,https://www.reddit.com/r/MachineLearning/comments/prfa0x/p_dlprimitives_wondering_about_best_development/,artyombeilis,MachineLearning,1970-01-01 00:00:01.632081671,Project,4,16,"I'm working on [DLPrimitives](https://github.com/artyom-beilis/dlprimitives) it already gives promising results... But I'm really wondering what to prioritize:1. Add more useful operators (dropout, upscale, lstm, prelu, mse-loss etc) to make DLPrimitives fully featured?2. Try to improve existing OpenCL frameworks like Caffe (or PlaidML) by using DLPrimitives core operations?3. Start working on pytorch OpenCL backend - that is huge undertaking?4. Work on support of float16/bfloat16?5. Continue improving performance by integrating with open source implementations for Arm-Mali, Intel?Every task is important.It is logical to add more operators so DLPrimitives - DL framework can be useful for real world tasks - it can be done relatively fast since most of operators aren't that complex.But in order to make it really useful (and not niche) it need to be integrated to at least one of the popular frameworks like Pytorch, TF or Mxnet. On the other hand implementing pytorch backend is huge task that will take lots of time - but it is actually the true goal.I can go with improving Caffe-OpenCL were I mostly need to fix several performance critical layers by using dlprimitives... ahhh and fix Caffe memory management since Keras/PT uses 1/4 of the memory Caffe uses. It can be good POC but Caffe is actually dead - I already have working POC.Hard to decide.",POS
445,pram95,The distributed deep learning framework OneFlow v0.5.0RC came out! [Project],https://www.reddit.com/r/MachineLearning/comments/pram95/the_distributed_deep_learning_framework_oneflow/,Just0by,MachineLearning,1970-01-01 00:00:01.632067127,Project,5,0,"We just launched OneFlow v0.5.0RC. With this release, OneFlow has the same API with PyTorch in eager mode, but more powerful and friendly API for distributed training. The consistent tensor enables model parallelism and pipeline parallelism without manual programming!Changelog link:[https://github.com/Oneflow-Inc/oneflow/releases/tag/v0.5rc1](https://github.com/Oneflow-Inc/oneflow/releases/tag/v0.5rc1)**Highlights*** First class support for eager execution. The deprecated APIs are moved to oneflow.compatible.single\_client* Drop-in replacement of import torchfor existing Pytorch projects. You could test it by inter-changing import oneflow as torch and import torch as flow* [nn.Module](https://docs.oneflow.org/master/basics/04_build_network.html#module) for eager execution* [nn.Graph](https://docs.oneflow.org/master/basics/08_nn_graph.html) for lazy execution* [DDP](https://oneflow.readthedocs.io/en/master/nn.html#oneflow.nn.parallel.DistributedDataParallel) for data parallel",POS
446,prg651,[D] STraTA: Self Training with Task Augmentation for Better Few shot Learning (Paper Explained),https://youtu.be/0yriOQbNWmo,deeplearningperson,MachineLearning,1970-01-01 00:00:01.632084478,Discussion,0,0,,NEU
447,pr108u,[D] Can someone please suggest how to begin working on a problem like this?,https://www.reddit.com/r/MachineLearning/comments/pr108u/d_can_someone_please_suggest_how_to_begin_working/,jj4646,MachineLearning,1970-01-01 00:00:01.632024504,Discussion,10,6,"Suppose I have the following problem:https://imgur.com/a/hWNJWQvSuppose you access to the hospital records: you have the history about how different patients passed through the different ""stages"" of the hospital (each row represents a unique patient). A patient can enter the hospital, and then meets the triage nurse. From there, they are sent to an examination room. If the condition is not serious, they are discharged from the hospital. If the condition is serious, they are sent to a proper hospital ward where they are kept there until they are discharged. However, if the patient enters the hospital in a very serious condition, then they are immediately sent to a ward and then discharged. I think that this problem can be modelled as a Markov Chain - either as a discrete time Markov Chain or as a continuous time Markov Chain. Using the available data, you can build a transition matrix which will describe the probabilities from moving between ""states"" (i.e. ""stages"") in the hospital. Once you have built the Markov Chain, you can calculate the probability the patient is in any of these ""states"" on the ""n-th"" day that they have entered the hospital.I am interested in something slightly different: When you look at this data, naturally you will see that the hospital does not admit only one patient at a time and then accept the second patient after the first patient has been discharged (i.e. dedicate the hospital to serving one patient at a time) - this would be crazy. Naturally, the hospital serves many patients - there are times when more patients might be transitioning between certain ""states"", and other times when fewer patients are transitioning between other ""states"". Naturally, the hospital can be more or less busy, and this will ultimately affect the ""speed"" at which patients will pass through the different ""stages"".Question: Can a Markov Chain be built in such a way that the transition probabilities are contingent on how many patients are in the hospital, how many patients are in each state and the rate at which they are transitioning? For instance, when a new patient arrives, the analysis could say: seeing as they are currently 100 patients in the hospital, 20 patients are waiting to see the triage nurse and 50 are in the exam room - the Markov Chain estimates you will have to wait approximately  10 hours before you might have your own room in the hospital ward?Is something like this possible?Thanks!",POS
448,pqs8kz,[D] Jax and the Future of ML,https://www.reddit.com/r/MachineLearning/comments/pqs8kz/d_jax_and_the_future_of_ml/,scraper01,MachineLearning,1970-01-01 00:00:01.631991383,Discussion,16,20,"Wondering about what the ML community thinks about Jax, mainly contrasts between experiences using Jax versus Tensorflow 2.0 or Pytorch. Looking at both industry and research, if someone want's to get really good at a specific ML framework what would you personally recommend? Which framework in your opinion has the better future prospects? I've been investing a lot of time in Tensorflow. Mainly because of the tflite interface. However i've been wondering if the time investment is future proof now that Google has two ML frameworks. I personally expect Google to eventually merge both Jax and Tf keeping the Keras API and the model compression utilities, and droping the gradient tape plus Tensorflows low level interfaces in favour of the Jax programming model. But thats my opinion. Never have used Jax myself, but i've read it's main features and keep hearing it's great, so now i'm wondering if learning a new framework today is worth the time investment.Really interested to read your take on this.",POS
449,pqhqjv,[R] Google AI Introduces Two New Families of Neural Networks Called â€˜EfficientNetV2â€™ and â€˜CoAtNetâ€™ For Image Recognition,https://www.reddit.com/r/MachineLearning/comments/pqhqjv/r_google_ai_introduces_two_new_families_of_neural/,techsucker,MachineLearning,1970-01-01 00:00:01.631948931,Research,113,13,"Training efficiency has become a significant factor for deep learning as the neural network models, and training data size grows. [GPT-3](https://arxiv.org/abs/2005.14165) is an excellent example to show how critical training efficiency factor could be as it takes weeks of training with thousands of GPUs to demonstrate remarkable capabilities in few-shot learning.To address this problem, the Google AI team introduce two families of neural networks for image recognition. First isÂ [EfficientNetV2](https://arxiv.org/abs/2104.00298), consisting of CNN (Convolutional neural networks) with a small-scale dataset for faster training efficiency such asÂ [ImageNet1k](https://www.image-net.org/)Â (with 1.28 million images). Second is a hybrid model calledÂ [CoAtNet](https://arxiv.org/abs/2106.04803), which combinesÂ [convolution](https://en.wikipedia.org/wiki/Convolution)Â andÂ [self-attention](https://en.wikipedia.org/wiki/Self-attention)Â to achieve higher accuracy on large-scale datasets such asÂ [ImageNet21](https://www.image-net.org/)Â (with 13 million images) andÂ [JFT](https://ai.googleblog.com/2017/07/revisiting-unreasonable-effectiveness.html)Â (with billions of images). As per the research report by Google,Â [EfficientNetV2](https://arxiv.org/abs/2104.00298)Â andÂ [CoAtNet](https://arxiv.org/abs/2106.04803)Â both are 4 to 10 times faster while achieving state-of-the-art and 90.88% top-1 accuracy on the well-establishedÂ [ImageNet](https://www.image-net.org/)Â dataset.# [7 Min Read](https://www.marktechpost.com/2021/09/17/google-ai-introduces-two-new-families-of-neural-networks-called-efficientnetv2-and-coatnet-for-image-recognition/) | [Paper (CoAtNet)](https://arxiv.org/abs/2106.04803) | [Paper (EfficientNetV2)](https://arxiv.org/abs/2104.00298) | [Google blog](https://ai.googleblog.com/2021/09/toward-fast-and-accurate-neural.html) | [Code](https://github.com/google/automl/tree/master/efficientnetv2)&#x200B;https://preview.redd.it/ipmkyt7eo7o71.png?width=1392&format=png&auto=webp&s=22764f4268a6c12acb85b8b71a7331cc6446d984",POS
450,pr3k9z,[D] Semi-supervised machine learning algorithms,https://www.reddit.com/r/MachineLearning/comments/pr3k9z/d_semisupervised_machine_learning_algorithms/,SQL_beginner,MachineLearning,1970-01-01 00:00:01.632037117,Discussion,0,10,"Suppose you have a big collection of audio (human speech) clips in the English language. You want to make an algorithm that hears someone talk and identify if it is English or Not English. The problem is, you only have English audio to train your model.Are there any popular algorithms that can be used for this problem? Can autoencoders be used?Thanks",POS
451,pqu1ef,"[D] In a recommendation system, do you need to create a new model each time a new item is added to the data set?",https://www.reddit.com/r/MachineLearning/comments/pqu1ef/d_in_a_recommendation_system_do_you_need_to/,uvcrtok,MachineLearning,1970-01-01 00:00:01.631997505,Discussion,7,11,"I only know, that you need to compute pairwiseÂ cosineÂ similarity scoresÂ between all items.",NEU
452,pqwj3v,[P] Go Motion simplifies stop motion animation with machine learning,https://www.reddit.com/r/MachineLearning/comments/pqwj3v/p_go_motion_simplifies_stop_motion_animation_with/,nickbild,MachineLearning,1970-01-01 00:00:01.632006249,Project,4,0,"&#x200B;https://i.redd.it/l4dfl86xeco71.gifA CSI camera is connected to a Jetson Xavier NX. This camera continually captures images of a scene. Using the trt\_pose\_hand hand pose detection model, the Jetson is able to determine when a hand is in the image frame.Each time all hands leave the frame, a single image is saved as part of the stop motion sequence. In this way, it is possible to continually manipulate the scene, momentarily removing one's hands from view of the camera after each adjustment, and have a stop motion sequence automatically generated that contains only the relevant image frames.More info: [https://github.com/nickbild/go\_motion](https://github.com/nickbild/go_motion)",POS
453,pqnik6,[Discussion] Employees of big tech companies: Do you have access to enough hardware to do your personal research,https://www.reddit.com/r/MachineLearning/comments/pqnik6/discussion_employees_of_big_tech_companies_do_you/,sim_inf,MachineLearning,1970-01-01 00:00:01.631975591,Discussion,12,25,"People who work at Google, Microsoft, Facebook, Amazon,If an employee wants to work on ML/NLP research projects defined by herself after working hours (e.g., in evenings), can she use the company machines and GPUs? Assuming that she is fine to publish under the company affiliation or add acknowledgement.In what situations she can? (depending on engineering or scientist position? depending on department she is working at? depending on her previous publication records? depending on the research project? etc)If she can, does she have to negotiate with an individual and establish personal connections?  Or she can go through the regular channels and not owe anybody (e.g., the manager).Thank you.Edit: I appreciate people's creative alternative workarounds. But I think my question above is clear, please let me know if any part of it is unclear and causes confusion. Thanks.",POS
454,pq64fk,"[R] [R for Rant] Empty github repo with ""code to replicate our findings"" for a 2020 Neurips main conference paper by accomplished researcher (>1000 citations on Google Scholar) with big name collaborators. Why?!?",https://www.reddit.com/r/MachineLearning/comments/pq64fk/r_r_for_rant_empty_github_repo_with_code_to/,AuspiciousApple,MachineLearning,1970-01-01 00:00:01.631904308,Research,390,119,"I don't get how that's acceptable. Repo is proudly and prominently linked in the paper, but it's empty. If you don't wanna release it, then don't promise it.Just wanted to rant about that.I feel like conferences should enforce a policy of ""if code is promised, then it needs to actually be public at the time the proceedings are published, otherwise the paper will be retracted"". Is this just to impress the reviewers? I.e. saying you release code is always a good thing, even if you don't follow through?",POS
455,pqr1ou,[D] Controllable Generation from Pre-trained Language Models via Inverse Prompting (Paper Summary),https://www.reddit.com/r/MachineLearning/comments/pqr1ou/d_controllable_generation_from_pretrained/,prakhar21,MachineLearning,1970-01-01 00:00:01.631987358,Discussion,1,1,"Open-ended Text Generation systems usually suffer from problems like Low relevance and Out-of-context generations. ðŸ‘¾This paper proposes an easy, intuitive yet effective method for Controllable Text Generation using Pre-trained Transformers Language Models. ðŸ”¥ Paper Summary: https://youtu.be/6RRdXnNd6XMPaper details in the comments!",POS
456,pqodmw,[N] A talk on DALLÂ·E mini architecture (VQ-GAN + BART pipeline understanding),https://www.reddit.com/r/MachineLearning/comments/pqodmw/n_a_talk_on_dalle_mini_architecture_vqgan_bart/,khalidsaifullaah,MachineLearning,1970-01-01 00:00:01.631978538,News,4,0,"I've recently given a talk on our DALLÂ·E mini. It's more on its theoretical aspect rather than the technical ones. **It Provides a beginner-friendly understanding of the modules used, VQ-GAN and BART.**[**\[YouTube Link\]**](https://youtu.be/ui0X6ozE3bI)&#x200B;**Talk Abstract**:*The ability to control image generation with natural language is very fascinating and opens a lot of new opportunities in the field of multimodal machine learning. OpenAI's recent blog about their DALLÂ·E project shows the potential of models, but unfortunately, the model has not been released. As we know these state-of-the-art models require massive amounts of computation and parameters to train, our goal here with DALLÂ·E mini is to show that one can still achieve reasonable performance on this multimodal task with far more accessible means of compute. Even though DALLÂ·E mini is about 30 times smaller than the original and trained on a much smaller dataset, it demonstrates interesting zero-shot capabilities.  In this talk, we will get to know DALLÂ·E mini in detail, and explain how it is capable of achieving such results thanks to the use of pre-trained models such as the VQ-GAN and BART. We will dig deeper into the theoretical aspects of these models to understand what happens under the hood in the DALLÂ·E mini pipeline.*",POS
457,pqnalf,[P] Audio DeepDream,https://www.reddit.com/r/MachineLearning/comments/pqnalf/p_audio_deepdream/,char-tan,MachineLearning,1970-01-01 00:00:01.631974808,Project,5,7,"Hey r/MachineLearning!Longtime lurker and wanted to share a side project I have been working on recently, applying the DeepDream algorithm to audio. DeepDream art was one of my first exposures to machine learning and I was curious to see how it would work out with audio.&#x200B;[Example of output](https://reddit.com/link/pqnalf/video/bo5x03kbbbo71/player)I trained VGG19 models to classify mel spectrograms from two datasets; UrbanSounds8k and free-music-archive (small). These trained models are then used to ""dream"" audio features onto input mel spectrograms. Thinking the project has applications in art generation and potentially also model interpretability.[https://github.com/char-tan/DreamSound](https://github.com/char-tan/DreamSound)EDIT: reduced volume of video, thanks to u/StoneCypher for pointing out the loudness",POS
458,ppy7k4,[N] Inside DeepMind's secret plot to break away from Google,https://www.reddit.com/r/MachineLearning/comments/ppy7k4/n_inside_deepminds_secret_plot_to_break_away_from/,MassivePellfish,MachineLearning,1970-01-01 00:00:01.631877465,News,413,140,"Article https://www.businessinsider.com/deepmind-secret-plot-break-away-from-google-project-watermelon-mario-2021-9by Hugh Langley and Martin Coulter> For a while, some DeepMind employees referred to it as ""Watermelon."" Later, executives called it ""Mario."" Both code names meant the same thing: a secret plan to break away from parent company Google.> > DeepMind feared Google might one day misuse its technology, and executives worked to distance the artificial-intelligence firm from its owner for years, said nine current and former employees who were directly familiar with the plans. > > This included plans to pursue an independent legal status that would distance the group's work from Google, said the people, who asked not to be identified discussing private matters.> > One core tension at DeepMind was that it sold the business to people it didn't trust, said one former employee. ""Everything that happened since that point has been about them questioning that decision,"" the person added.> > Efforts to separate DeepMind from Google ended in April without a deal, The Wall Street Journal reported. The yearslong negotiations, along with recent shake-ups within Google's AI division, raise questions over whether the search giant can maintain control over a technology so crucial to its future.> > ""DeepMind's close partnership with Google and Alphabet since the acquisition has been extraordinarily successful â€” with their support, we've delivered research breakthroughs that transformed the AI field and are now unlocking some of the biggest questions in science,"" a DeepMind spokesperson said in a statement. ""Over the years, of course we've discussed and explored different structures within the Alphabet group to find the optimal way to support our long-term research mission. We could not be prouder to be delivering on this incredible mission, while continuing to have both operational autonomy and Alphabet's full support.""> > When Google acquired DeepMind in 2014, the deal was seen as a win-win. Google got a leading AI research organization, and DeepMind, in London, won financial backing for its quest to build AI that can learn different tasks the way humans do, known as artificial general intelligence.> > But tensions soon emerged. Some employees described a cultural conflict between researchers who saw themselves firstly as academics and the sometimes bloated bureaucracy of Google's colossal business. Others said staff were immediately apprehensive about putting DeepMind's work under the control of a tech giant. For a while, some employees were encouraged to communicate using encrypted messaging apps over the fear of Google spying on their work.> > At one point, DeepMind's executives discovered that work published by Google's internal AI research group resembled some of DeepMind's codebase without citation, one person familiar with the situation said. ""That pissed off Demis,"" the person added, referring to Demis Hassabis, DeepMind's CEO. ""That was one reason DeepMind started to get more protective of their code.""> > After Google restructured as Alphabet in 2015 to give riskier projects more freedom, DeepMind's leadership started to pursue a new status as a separate division under Alphabet, with its own profit and loss statement, The Information reported.> > DeepMind already enjoyed a high level of operational independence inside Alphabet, but the group wanted legal autonomy too. And it worried about the misuse of its technology, particularly if DeepMind were to ever achieve AGI.> > Internally, people started referring to the plan to gain more autonomy as ""Watermelon,"" two former employees said. The project was later formally named ""Mario"" among DeepMind's leadership, these people said.> > ""Their perspective is that their technology would be too powerful to be held by a private company, so it needs to be housed in some other legal entity detached from shareholder interest,"" one former employee who was close to the Alphabet negotiations said. ""They framed it as 'this is better for society.'""> > In 2017, at a company retreat at the Macdonald Aviemore Resort in Scotland, DeepMind's leadership disclosed to employees its plan to separate from Google, two people who were present said.> > At the time, leadership said internally that the company planned to become a ""global interest company,"" three people familiar with the matter said. The title, not an official legal status, was meant to reflect the worldwide ramifications DeepMind believed its technology would have.> > Later, in negotiations with Google, DeepMind pursued a status as a company limited by guarantee, a corporate structure without shareholders that is sometimes used by nonprofits. The agreement was that Alphabet would continue to bankroll the firm and would get an exclusive license to its technology, two people involved in the discussions said. There was a condition: Alphabet could not cross certain ethical redlines, such as using DeepMind technology for military weapons or surveillance. > > In 2019, DeepMind registered a new company called DeepMind Labs Limited, as well as a new holding company, filings with the UK's Companies House showed. This was done in anticipation of a separation from Google, two former employees involved in those registrations said.> > Negotiations with Google went through peaks and valleys over the years but gained new momentum in 2020, one person said. A senior team inside DeepMind started to hold meetings with outside lawyers and Google to hash out details of what this theoretical new formation might mean for the two companies' relationship, including specifics such as whether they would share a codebase, internal performance metrics, and software expenses, two people said.> > From the start, DeepMind was thinking about potential ethical dilemmas from its deal with Google. Before the 2014 acquisition closed, both companies signed an ""Ethics and Safety Review Agreement"" that would prevent Google from taking control of DeepMind's technology, The Economist reported in 2019. Part of the agreement included the creation of an ethics board that would supervise the research. > > Despite years of internal discussions about who should sit on this board, and vague promises to the press, this group ""never existed, never convened, and never solved any ethics issues,"" one former employee close to those discussions said. A DeepMind spokesperson declined to comment.> > DeepMind did pursue a different idea: an independent review board to convene if it were to separate from Google, three people familiar with the plans said. The board would be made up of Google and DeepMind executives, as well as third parties. Former US president Barack Obama was someone DeepMind wanted to approach for this board, said one person who saw a shortlist of candidates.> > DeepMind also created an ethical charter that included bans on using its technology for military weapons or surveillance, as well as a rule that its technology should be used for ways that benefit society. In 2017, DeepMind started a unit focused on AI ethics research composed of employees and external research fellows. Its stated goal was to ""pave the way for truly beneficial and responsible AI."" > > A few months later, a controversial contract between Google and the Pentagon was disclosed, causing an internal uproar in which employees accused Google of getting into ""the business of war."" > > Google's Pentagon contract, known as Project Maven, ""set alarm bells ringing"" inside DeepMind, a former employee said. Afterward, Google published a set of principles to govern its work in AI, guidelines that were similar to the ethical charter that DeepMind had already set out internally, rankling some of DeepMind's senior leadership, two former employees said.> > In April, Hassabis told employees in an all-hands meeting that negotiations to separate from Google had ended. DeepMind would maintain its existing status inside Alphabet. DeepMind's future work would be overseen by Google's Advanced Technology Review Council, which includes two DeepMind executives, Google's AI chief Jeff Dean, and the legal SVP Kent Walker.> > But the group's yearslong battle to achieve more independence raises questions about its future within Google.> > Google's commitment to AI research has also come under question, after the company forced out two of its most senior AI ethics researchers. That led to an industry backlash and sowed doubt over whether it could allow truly independent research.> > Ali Alkhatib, a fellow at the Center for Applied Data Ethics, told Insider that more public accountability was ""desperately needed"" to regulate the pursuit of AI by large tech companies. > > For Google, its investment in DeepMind may be starting to pay off. Late last year, DeepMind announced a breakthrough to help scientists better understand the behavior of microscopic proteins, which has the potential to revolutionize drug discovery.> > As for DeepMind, Hassabis is holding on to the belief that AI technology should not be controlled by a single corporation. Speaking at Tortoise's Responsible AI Forum in June, he proposed a ""world institute"" of AI. Such a body might sit under the jurisdiction of the United Nations, Hassabis theorized, and could be filled with top researchers in the field. > > ""It's much stronger if you lead by example,"" he told the audience, ""and I hope DeepMind can be part of that role-modeling for the industry.""",POS
459,pqccik,[D] What makes a paper good?,https://www.reddit.com/r/MachineLearning/comments/pqccik/d_what_makes_a_paper_good/,ElEiseinheim,MachineLearning,1970-01-01 00:00:01.631925332,Discussion,14,10,"I'm about to submit a paper for publishing and but was wondering what the community finds to be a good research paper? Currently I've tailored it a bit more towards what I'm searching in a paper, which is to basically explain every single thing that I did (from why this optimizer to why this learning rate decay). Do you guys just look straight to the results? Maybe the conclusion? Do you look for results in the abstract? Is tabular data as annoying for you guys as it is for me?",POS
460,pq352a,[D] How often do industry ML research internships end in failure?,https://www.reddit.com/r/MachineLearning/comments/pq352a/d_how_often_do_industry_ml_research_internships/,doafkmebfl,MachineLearning,1970-01-01 00:00:01.631895029,Discussion,61,23,"I was lucky enough to get a research internship this summer (NLP, Big Tech Company that does lots of research). The goal was to work on a self-contained basic research problem and publish a paper. Unfortunately, the problem was harder than it had seemed, nothing I tried worked, and we probably won't be able to submit anything.I'm wondering how common this experience is? How much of a disaster is this outcome?Most grad students who got an industry internship that I know of was able to successfully publish something, or at worst have some kind of positive results that make it to arXiv. But obviously, nobody will ever talk about their failures, so maybe this is actually a fairly common outcome.If you had an ML research internship that didn't go well and are willing to share about it, it might make me feel a little better. Or, maybe most people usually do succeed -- that would be good to know too.",POS
461,pqi2h5,[D] how to evaluate a method on class-imbalanced dataset,https://www.reddit.com/r/MachineLearning/comments/pqi2h5/d_how_to_evaluate_a_method_on_classimbalanced/,Representative_War57,MachineLearning,1970-01-01 00:00:01.631950629,Discussion,2,3,"I am work on a retrieval task and find the result varies with the different splitting of training set and retrieval candidate. After carefully checking the data, I find that the data label is class imbalanced. Some major class have 100k samples but some minor one may just have 5k samples.But it takes long time to train the model and it is impossible to enumerate different splitting one by one.I think there are similiar problems besides retrieval. In such a situation, how to make a reasenable configuration to evaluate the solution? thx",POS
462,pqcey1,[D] CLIP Paper Explained - Learning Transferable Visual Models From Natural Language Supervision (5-Minute Summary),https://www.reddit.com/r/MachineLearning/comments/pqcey1/d_clip_paper_explained_learning_transferable/,KirillTheMunchKing,MachineLearning,1970-01-01 00:00:01.631925604,Discussion,6,0,"I have mentioned CLIP so many times in my posts that you might think I am being paid to promote it. Unfortunately, I am not, but a lot of my favorite projects use CLIP, and it is time to finally get into the nitty-gritty of the powerhouse that is CLIP. CLIP is model from 2020 that is inspired by ideas from Alec Radford, Jong Wook Kim, and the good folks at OpenAI.[CLIP Architecture](https://preview.redd.it/g4y5ojb4r5o71.png?width=2162&format=png&auto=webp&s=dc2a1e23d7bbce45b1403c9bf2ea2d31992bc8ad)Check out the [full paper summary](https://www.casualganpapers.com/zero-shot-contrastive-loss-image-text-pretraining/CLIP-explained.html) on Casual GAN Papers (Reading time \~5 minutes).Subscribe to [my channel](https://t.me/casual_gan) and follow me on [Twitter](https://twitter.com/KirillDemochkin) for weekly AI paper summaries!",POS
463,pqekol,[D] Relationship between MCMC and MAP estimates,https://www.reddit.com/r/MachineLearning/comments/pqekol/d_relationship_between_mcmc_and_map_estimates/,ottawalanguages,MachineLearning,1970-01-01 00:00:01.631934297,Discussion,3,3,"I'm still relatively new to understanding the bayesian mentality.1) MCMC (e.g metropolis hasting) finds out the posterior distribution of the parameters of interest. MCMC requires taking many samples from the posterior distribution and creating a histogram.2) the MAP estimate can be used to select a value of this parameter from the posterior distribution that well summarizes the posterior distribution. The MAP estimate can be expressed as the argmax(prior * likelihood).My questions:1) Can ""argmax(prior * likelihood)"" be calculated using any optimization algorithm? For example, the Genetic Algorithm or Gradient Descent? For example, if we wanted to estimate the ""lambda"" parameter from an exponential distribution, could the Genetic Algorithm be used to evaluate the ""argmax(prior * likelihood)"" and identify the final lambda value?2) It seems like MAP estimate is used to identify a final value of the parameter, whereas the posterior distribution of the parameter generated by MCMC is used to create a ""credible interval"" around the MAP estimate of the parameter?Thanks!",POS
464,ppsh12,"[R] Israeli Researchers Unveil DeepSIM, a Neural Generative Model for Conditional Image Manipulation Based on a Single Image",https://www.reddit.com/r/MachineLearning/comments/ppsh12/r_israeli_researchers_unveil_deepsim_a_neural/,techsucker,MachineLearning,1970-01-01 00:00:01.631849580,Research,91,10,"In recent years, deep neural networks have been proven effective at performing image manipulation tasks for which large training datasets are available such as, mapping facial landmarks to facial images. When dealing with a unique image, finding suitable training data that includes many samples of the same input-output pairing is often difficult. In some cases, when you use a large dataset to create your model, it may lead to unwanted outputs that do not preserve the specific characteristics of what was desired.Â Generative models like the ones used in neural networks can be trained to generate new images based on just one input. This exciting research direction holds the potential for these techniques to extend beyond basic image manipulation methods and create more unique art styles or designs with endless possibilities. Researchers at [The Hebrew University of Jerusalem have developed a new method, called â€˜DeepSIM,â€™](https://arxiv.org/pdf/2109.06151.pdf) for training deep conditional generative models from just one image pair. The DeepSIM method is an incredibly powerful tool that can solve various image manipulation tasks, including shape warping, object rearrangement, and removal of objects; addition or creation of new ones. It also allows for painting/photorealistic animated clips to be created quickly.# [5 Min Read](https://www.marktechpost.com/2021/09/16/israeli-researchers-unveil-deepsim-a-neural-generative-model-for-conditional-image-manipulation-based-on-a-single-image/) | [Paper](https://arxiv.org/pdf/2109.06151.pdf) | [Project](http://www.vision.huji.ac.il/deepsim/) | [Code](https://github.com/eliahuhorwitz/DeepSIM)&#x200B;https://reddit.com/link/ppsh12/video/ntztn2i3hzn71/player",POS
465,ppxo99,[R] Slides and Videos of ICML 2021: Thirty-eighth International Conference on Machine Learning (https://icml.cc/Conferences/2021/Schedule?type=Tutorial),https://www.reddit.com/r/MachineLearning/comments/ppxo99/r_slides_and_videos_of_icml_2021_thirtyeighth/,pirahansiah,MachineLearning,1970-01-01 00:00:01.631875000,Research,20,0,ICML 2021: Thirty-eighth International Conference on Machine Learning (https://icml.cc/Conferences/2021/Schedule?type=Tutorial)https://icml.cc/virtual/2021/calendar You can see and download the slides and videos,NEU
466,pq19my,[R] MIT Presents New Approach for Sequence-to-Sequence Learning with Latent Neural Grammars,https://www.reddit.com/r/MachineLearning/comments/pq19my/r_mit_presents_new_approach_for/,Yuqing7,MachineLearning,1970-01-01 00:00:01.631889043,Research,10,0,"A new MIT CSAIL paper presents an alternative, hierarchical approach to sequence-to-sequence learning with quasi-synchronous grammars to improve sequence-to-sequence modelsâ€™ compositional generalization on diagnostic tasks. Here is a quick read: [MIT Presents New Approach for Sequence-to-Sequence Learning with Latent Neural Grammars.](https://syncedreview.com/2021/09/17/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-106/) The paper *Sequence-to-Sequence Learning with Latent Neural Grammars* is on [arXiv](https://arxiv.org/abs/2109.01135).",POS
467,pqdbeg,[D] ICRA video submission,https://www.reddit.com/r/MachineLearning/comments/pqdbeg/d_icra_video_submission/,randy_wales_qq,MachineLearning,1970-01-01 00:00:01.631929186,Discussion,0,1,"What exactly is that ICRA \[Optional\] Video submission. If my paper has nothing to do with the real robot, is it encouraged \*NOT\* to submit a video?",POS
468,pq8lnq,[R] MTSCNeuralForecast: Time series forecasting and clustering.,https://www.reddit.com/r/MachineLearning/comments/pq8lnq/r_mtscneuralforecast_time_series_forecasting_and/,chess9145,MachineLearning,1970-01-01 00:00:01.631912183,Research,3,1,"Python and R scripts for time series forecasting, anomaly detection, imputing missing values, and multivariate time series forecasting using neural networks.[https://github.com/manitadayon/MTSCNeuralForecast](https://github.com/manitadayon/MTSCNeuralForecast)",POS
469,pq127g,[D] What were some of the popular image processing techniques pre-deep learning era?,https://www.reddit.com/r/MachineLearning/comments/pq127g/d_what_were_some_of_the_popular_image_processing/,Illustrious_Ad_637,MachineLearning,1970-01-01 00:00:01.631888364,Discussion,6,5,"More specifically, for object detection or segmentation. Why did we stop using those?",NEG
470,ppv47i,"[D] Importance of the ""Representer Theorem"" in Machine Learning",https://www.reddit.com/r/MachineLearning/comments/ppv47i/d_importance_of_the_representer_theorem_in/,ottawalanguages,MachineLearning,1970-01-01 00:00:01.631861503,Discussion,11,11,"[https://en.wikipedia.org/wiki/Representer\_theorem](https://en.wikipedia.org/wiki/Representer_theorem)I keep hearing reference to the ""Representer Theorem"" in Machine Learning, but I can't seem to figure out why this is so important? I have tried going over it several times, but all I can seem to infer is that somehow, this theorem justifies the importance of Kernels in Machine Learning (e.g. the Kernel Trick - a more computationally effective way to ""shuttle"" data between different dimensions).Can anyone please comment on this?Thanks",POS
471,pq2tg1,[D] How far we are from an AI being able of code and entire UI website just with its UI design (image) as input?,https://www.reddit.com/r/MachineLearning/comments/pq2tg1/d_how_far_we_are_from_an_ai_being_able_of_code/,6eer,MachineLearning,1970-01-01 00:00:01.631894046,Discussion,1,7,"Iâ€™m curious guys, whatâ€™s your thoughts?",POS
472,ppz92r,[N] [R] Call for participation in the MediaEval 2021 Emotion and Theme Recognition in Music task,https://www.reddit.com/r/MachineLearning/comments/ppz92r/n_r_call_for_participation_in_the_mediaeval_2021/,diibv,MachineLearning,1970-01-01 00:00:01.631881874,News,3,0,"We  are pleased to announce the third year of the Emotion and Theme Recognition in Music task held within the MediaEval 2021 evaluation campaign.The Benchmarking Initiative for Multimedia Evaluation (MediaEval) organizes an annual cycle of scientific  evaluation tasks in the area of multimedia access  and retrieval. In our task, we invite the participants to try their skills at predicting mood  and theme tags associated with music  recordings using audio analysis and machine learning algorithms.The  task is framed as an auto-tagging problem with tags specific to moods  and themes (e.g.,  happy, dark, epic, melodic, love, film, space). The  task uses the  MTG-Jamendo dataset: [https://mtg.github.io/mtg-jamendo-dataset/](https://mtg.github.io/mtg-jamendo-dataset/), presented at the Machine Learning for Music Discovery Workshop at ICML 2019: [https://hdl.handle.net/10230/42015](https://hdl.handle.net/10230/42015)All interested researchers are warmly welcomed to participate. The  deadline  for all submissions for the challenge is November 5. Participants will be able to present their results at the MediaEval  Multimedia Benchmark  Workshop, to be held on December 6-8 in Bergen,  Norway with opportunity  for online participation: [https://multimediaeval.github.io/](https://multimediaeval.github.io/)The registration for the task is available at the MediaEval website. A full description of the task is available here: [https://multimediaeval.github.io/2021-Emotion-and-Theme-Recognition-in-Music-Task/](https://multimediaeval.github.io/2021-Emotion-and-Theme-Recognition-in-Music-Task/)",POS
473,ppbjqw,[D] How to maintain ML models?,https://www.reddit.com/r/MachineLearning/comments/ppbjqw/d_how_to_maintain_ml_models/,xiaojirong,MachineLearning,1970-01-01 00:00:01.631792045,Discussion,248,55,"I work as a Data Scientist as a small (but somewhat successful) consultancy firm. After a few failures, we've managed to deliver in the past years quite a number of successful projects with clients. But we (and me in particular, as the DS that's been here the longest) are starting to have a bit of a scaling up problem:\- We have models everywhere: in our cloud, in client's VPCs, on-premise, etc.\- We are mainly data scientists, and not ML engineers, so while our models are good, all the infrastructure around it is a bit lacking, especially:\- Errors on the models (weak performance, etc.) are usually only noticed by the clients after some time, and are extremely painful to investigate, since all the models live in different environments, and we're essentially blind on what happens with them until the clients calls us.\- Updating a model, whether to fix a bug or for improvement, is also usually painful, for similar reasons. So I'd be curious to know the kind of frameworks / methodologies you use to simplify maintenance of ML models: how to monitor them, update them, and something we're starting to think about: how to automate the ""feedback"" loop to automatically retrain the models. Any commercial or open source tools?",NEG
474,ppqa1e,[R] Language Models are Few-shot Multilingual Learners,https://arxiv.org/abs/2109.07684,hardmaru,MachineLearning,1970-01-01 00:00:01.631841151,Research,9,1,,NEU
475,ppqyfk,[R] Searching for More Efficient Dynamic Programs,https://arxiv.org/abs/2109.06966,hardmaru,MachineLearning,1970-01-01 00:00:01.631843724,Research,6,1,,NEU
476,ppmx0s,[Discussion] Gpu for personal use,https://www.reddit.com/r/MachineLearning/comments/ppmx0s/discussion_gpu_for_personal_use/,Hub_Pli,MachineLearning,1970-01-01 00:00:01.631828891,Discussion,12,29,"Hey guys, I am starting my phd in psychology which will involve creating some interesting deep learning prediction models. I also work on a grant where I do topic modelling and in the meantime I am learning RL and NLP. My parents were so generous as to offer to fund me a decent gpu for deep learning and I was wondering whether I could get any suggestions on what would be a good pick nowadays. Anything above 3000 $ is probably off the table.Also a general post on what are some good gpu's on a personal budget.",POS
477,ppfqlr,[P] easyopt: zero-code hyperparameters optimization framework,https://www.reddit.com/r/MachineLearning/comments/ppfqlr/p_easyopt_zerocode_hyperparameters_optimization/,poppear,MachineLearning,1970-01-01 00:00:01.631806797,Project,16,0,I got tired of writing over and over again the same boilerplate code to do hyperparameters optimization so i built [easyopt](https://github.com/galatolofederico/easyopt)It is basically an [optuna](https://optuna.org/) wrapper that does all the boring stuff for you.You just have to write a simple YAML file and run `easyopt create` to create a study and `easyopt agent <study_name>` to run the optimization agent.It supports all the optuna nice features such as:&#x200B;* Distributed Parallel Optimization* Real Time Pruning* A wide variety of sampling strategies     * Tree-structured Parzen Estimator   * CMA-ES   * Grid Search   * Random Search* A wide variety of pruning strategies     * Asynchronous Successive Halving Pruning   * Hyperband Pruning   * Median Pruning   * Threshold Pruning* A wide variety of DBMSs     * Redis   * SQLite   * PostgreSQL   * MySQL   * Oracle   * And many [more](https://docs.sqlalchemy.org/en/14/core/engines.html#sqlalchemy.create_engine)&#x200B;With some bonuses like:* YAML Configuration* Experiments Monitoring and Crash Recovering* Experiments Replicas&#x200B;You can find the project here: [https://github.com/galatolofederico/easyopt](https://github.com/galatolofederico/easyopt)&#x200B;I would love to hear some feedback!,POS
478,pph0ak,[Discussion] How to present machine learning projects to domain experts without ML background?,https://www.reddit.com/r/MachineLearning/comments/pph0ak/discussion_how_to_present_machine_learning/,skwaaaaat,MachineLearning,1970-01-01 00:00:01.631810706,Discussion,12,11,"Hey guys, I really need some suggestions on the best way to present machine learning projects to domain experts who do not have a background in ML. Here is my situation: I'm a phd student working on applying machine learning to concrete physical science problems and my committee includes some physical scientists who do not work on machine learning. I've tried my best to not use technical terms that may not familiar to these people and made my presentations as simple as possible. The issue is that some domain experts seem to find a hard time understanding the contributions and got lost in the presentation. I'm planning to include as much background as possible in my future presentations and focus on fewer projects so they won't get overwhelmed. I'm wondering if someone who has experienced similar situations could give me some suggestions on how to best convey the message during such presentations. &#x200B;Thanks!",POS
479,ppmn86,[P] TFServingCache: A distributed LRU-cache for serving TensorFlow models in Kubernetes,https://www.reddit.com/r/MachineLearning/comments/ppmn86/p_tfservingcache_a_distributed_lrucache_for/,mKaloer,MachineLearning,1970-01-01 00:00:01.631828008,Project,4,0,"The tools for deployment of machine learning models keep getting better. However, Iâ€™ve found that most tools are lacking when it comes to serving a large number of TensorFlow models, e.g. a model per user. So Iâ€™ve built my own tool, called TFServingCache, which automatically loads and unloads models in a distributed cluster of TensorFlow Serving instances.Iâ€™ve written a post about the project on medium: [TFServingCache: A distributed LRU-cache for serving TensorFlow models in Kubernetes](https://link.medium.com/lEeZEBkMBjb).The project is written in Go, and open source on GitHub: [TFServingCache](https://github.com/mKaloer/TFServingCache). Give it a try!",POS
