,id,title,url,author,subreddit,timestamp,flair,points,comments,text,sentiment
0,q821td,BERT models: how resilient are they to typos?,https://www.reddit.com/r/LanguageTechnology/comments/q821td/bert_models_how_resilient_are_they_to_typos/,wtfzambo,LanguageTechnology,1970-01-01 00:00:01.634225344,,9,7,"Hello,let me introduce the context briefly: I'm fine tuning a generic BERT model for the context of food and beverage. The final goal is a classification task.To train this model, I'm using a corpus of text gathered from blog posts, articles, magazines etc... that cover the topic.I am however facing a predicament that I don't know how to handle: specifically, there are sometimes words that either contain a typo, or maybe different accents, but that are semantically the same.Let me give you an example to briefly illustrate what I mean:The wine `Gewürztraminer` is correctly written with the `ü`, however sometimes you also find it written with just a normal `u`, or some other times even just `Gewurtz`. There are several situations like this one.Now, a human being would obviously know that we're talking exactly about the same thing, but I have absolutely no idea about how BERT would handle these situations. Would it understand that they're the same thing? Would it consider them instead to be completely different words?I am currently in the process of cleaning my training data, fixing the typos and trying to even out all these inconsistencies, but at this point I'm not even sure if I should do that at all, considering that the text that will need to be classified can potentially contain typos and situations like the one described above.What would you guys suggest?",POS
1,q7y7pn,Dense vectors for NLP (and some vision),https://www.reddit.com/r/LanguageTechnology/comments/q7y7pn/dense_vectors_for_nlp_and_some_vision/,jamescalam,LanguageTechnology,1970-01-01 00:00:01.634212770,,6,0,"Hi all, I put together an [article and video](https://www.pinecone.io/learn/dense-vector-embeddings-nlp/) on a few of the coolest (and useful of course) embeddings for NLP, and also text-image with OpenAI's CLIP at the end. Planing on diving into each area in more depth in the future!Let me know what you think, if I'm missing anything or you have any questions!Thanks!",POS
2,q87sxl,Nywspaper: comparing news using transformers,https://www.reddit.com/r/LanguageTechnology/comments/q87sxl/nywspaper_comparing_news_using_transformers/,FlatPlate,LanguageTechnology,1970-01-01 00:00:01.634242441,,1,2,"Hello everyone,I have built [nywspaper](https://nywspaper.com), a news aggregator / reader / comparison tool for my bachelor's thesis, and I am very excited to share it with you here.The goal of this tool is to make it easier for the readers to understand media bias in the news, by allowing paragraph by paragraph comparison between news articles covering the same story. When you're reading an article on nywspaper, if you click on a paragraph, you get paragraphs similar to the one you clicked from other publishers. This way you can see how a right wing news publisher delivers the same information differently than a left wing publisher.In the main page you can see articles grouped by events, and you can just navigate to the article and begin comparing. There is also a feedback button in the similar paragraph boxes, if you particularly like or dislike a paragraph that was suggested.I am really looking forward to hearing your thoughts on this tool, and if it could be used to fight media bias. I would also hugely appreciate it if you could have a chance to fill out this [survey](https://www.questionpro.com/t/AT7qPZpHzt) after you use the tool (this would help for the thesis). Thanks!",POS
3,q86ouv,We Need to Talk About Data: The Importance of Data Readiness in Natural Language Processing,https://www.reddit.com/r/LanguageTechnology/comments/q86ouv/we_need_to_talk_about_data_the_importance_of_data/,frippeo,LanguageTechnology,1970-01-01 00:00:01.634239112,,1,0,"Hey there,We've collected our experiences on teasing out the data readiness of organizations in relation to ML/NLP projects. We describe a method comprised of 15 questions that help stakeholders gauge their data readiness, along with a way to visualize the outcome of applying the method.arXiv: [https://arxiv.org/abs/2110.05464](https://arxiv.org/abs/2110.05464)Abstract: In this paper, we identify the state of data as being an important reason for failure in applied Natural Language Processing (NLP) projects. We argue that there is a gap between academic research in NLP and its application to problems outside academia, and that this gap is rooted in poor mutual understanding between academic researchers and their non-academic peers who seek to apply research results to their operations. To foster transfer of research results from academia to non-academic settings, and the corresponding influx of requirements back to academia, we propose a method for improving the communication between researchers and external stakeholders regarding the accessibility, validity, and utility of data based on Data Readiness Levels. While still in its infancy, the method has been iterated on and applied in multiple innovation and research projects carried out with stakeholders in both the private and public sectors. Finally, we invite researchers and practitioners to share their experiences, and thus contributing to a body of work aimed at raising awareness of the importance of data readiness for NLP.And the code for the visualizations is here:GitHub: [https://github.com/fredriko/draviz](https://github.com/fredriko/draviz)I'll be happy to hear any feedback! :)",POS
4,q7r5mj,I have some problems with understading how LSTM can solve Sentiment Analysis.,https://www.reddit.com/r/LanguageTechnology/comments/q7r5mj/i_have_some_problems_with_understading_how_lstm/,gorg278,LanguageTechnology,1970-01-01 00:00:01.634181134,,8,7,"I already got a grip of how to solve Sentiment Analysis problem ( pre-processing dataset, word embedding, feed word-vectors to LSTM structure and boom, I have a model that can predict a sentence is positive or negative), what I still don't understand is what LSTM layer do with word-vectors. Does it use them to understand the meaning of sentence, if so, how does it do it? Finally, when it understood the meaning, how can it know the sentence is positive or negative?",POS
5,q7zvj7,How the context is stored in context vector in encoder decoder transformer model?,https://www.reddit.com/r/LanguageTechnology/comments/q7zvj7/how_the_context_is_stored_in_context_vector_in/,Inzy01,LanguageTechnology,1970-01-01 00:00:01.634218607,,1,1,I mean I know that transformer for eg BERT can Understand the context of the paragraph but how the BERT model stored the context. I understand that word can be put into vector using on hot encoding or any other approach but storing context into vector I don't get it at all.Please help.,POS
6,q7yqws,"On Self-Service, Data Democratization and (Natural) Language",https://jpmonteiro.substack.com/p/on-self-service-data-democratization,jpmfribeiro,LanguageTechnology,1970-01-01 00:00:01.634214779,,1,0,,NEU
7,q7ueud,Sentiment analysis on software engineering texts,https://www.reddit.com/r/LanguageTechnology/comments/q7ueud/sentiment_analysis_on_software_engineering_texts/,lucifer955,LanguageTechnology,1970-01-01 00:00:01.634194803,,2,0,What are the possible ways to improve sentiment dictionaries to analyse  SE texts? There are several SE specific sentiment dictionaries but  cannot expect much accuracy when analysing open-source projects. Thank you,POS
8,q7fx3x,Microsoft and NVIDIA AI Introduces MT-NLG: The Largest and Most Powerful Monolithic Transformer Language NLP Model,https://www.reddit.com/r/LanguageTechnology/comments/q7fx3x/microsoft_and_nvidia_ai_introduces_mtnlg_the/,techsucker,LanguageTechnology,1970-01-01 00:00:01.634145444,,14,1,"Transformer-based language models have made rapid progress in many natural language processing (NLP) applications, thanks to the availability of large datasets, large computation at scale, and advanced algorithms and software to train these models.The high-performing language models need many parameters, a lot of data, and a lot of training time to develop a richer, more sophisticated understanding of language. As a result, they generalize well as effective zero– or few–shot learners on various NLP tasks and datasets with high accuracy.However, training such models is problematic for two reasons:* The parameters of these models can no longer be fit into the memory of even the most powerful GPU.* Special attention is required for optimizing the algorithms, software, and hardware stack as a whole. If proper attention is not provided, the large number of computing operations required can result in unrealistically long training times.Microsoft and NVIDIA present the Megatron-Turing Natural Language Generation model (MT-NLG), powered by [DeepSpeed ](https://github.com/microsoft/DeepSpeed)and [Megatron](https://github.com/NVIDIA/Megatron-LM), the largest and robust monolithic transformer language model trained with 530 billion parameters.# [5 Min-Quick Read](https://www.marktechpost.com/2021/10/13/microsoft-and-nvidia-ai-introduces-mt-nlg-the-largest-and-most-powerful-monolithic-transformer-language-nlp-model/) | [Microsoft Blog](https://www.microsoft.com/en-us/research/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/)&#x200B;",POS
9,q7lfap,Cambridge Quantum (CQ) Open-Sources ‘lambeq’: A Python Library For Experimental Quantum Natural Language Processing (QNLP),https://www.reddit.com/r/LanguageTechnology/comments/q7lfap/cambridge_quantum_cq_opensources_lambeq_a_python/,techsucker,LanguageTechnology,1970-01-01 00:00:01.634161498,,3,0,"[Cambridge Quantum (“CQ”)](https://cambridgequantum.com/) announced the release of the world’s first toolkit and an [open-source library ](https://github.com/CQCL/lambeq)for Quantum Natural Language Processing (QNLP), called [‘lambeq’](https://arxiv.org/abs/2110.04236).Speaking in simple words, ‘lambeq’ is the toolkit for QNLP (Quantum Natural Language Processing) to convert sentences into a quantum circuit. It can be used to accelerate development in practical, real-world applications such as automated dialogue systems and text mining, among other things.‘lambeq’ has been released on a fully [open-sourced basis](https://github.com/CQCL/lambeq) for the benefit of all quantum computing researchers and developers. Lambeq seamlessly integrates with CQ’s (Cambridge Quantum) TKET, the world’s leading and fastest-growing quantum software development platform that is also fully open-sourced. The open-sourcing of this technology provides QNLP developers with an even broader range for their work.# [Quick 3 Min Read](https://www.marktechpost.com/2021/10/13/cambridge-quantum-cq-open-sources-lambeq-a-python-library-for-experimental-quantum-natural-language-processing-qnlp/) | [Paper](https://arxiv.org/abs/2110.04236) | [Github](https://github.com/CQCL/lambeq) | [Documentation](https://cqcl.github.io/lambeq/) |[CQ Blog](https://medium.com/cambridge-quantum-computing/quantum-natural-language-processing-ii-6b6a44b319b2)",POS
10,q7ppn0,"Ways to reduce memory consumption in Q&A tasks without damage (or at least, not that much) the accuracy?",https://www.reddit.com/r/LanguageTechnology/comments/q7ppn0/ways_to_reduce_memory_consumption_in_qa_tasks/,Severe_March_88,LanguageTechnology,1970-01-01 00:00:01.634175966,,1,2,"i’m facing this problem: I’m trying to spend less  memory in my Q&A task using bert. I debugged my steps and saw that  the start\_logits and end\_logits>start\_logits, end\_logits = model(\*\*inputs)   costs more than 11gb of ram. Is there any ways to solve this? I mean,  use less memory to perform this task without harm my model accuracy? If  so, can someone share some of them? And some alternative ways in case  is not possible to do this?",POS
11,q76fa4,An illustrated tour of wav2vec 2.0,https://www.reddit.com/r/LanguageTechnology/comments/q76fa4/an_illustrated_tour_of_wav2vec_20/,jonathanbgn,LanguageTechnology,1970-01-01 00:00:01.634112147,,16,0,"When Transformers started getting popular for NLP, we saw great visualizations to understand better the internals of these models like The Illustrated BERT, GPT...I haven't seen much like that for speech processing, so I wrote this quick post to illustrate the architecture and pre-training process of wav2vec 2.0 (now part of the HuggingFace library).[https://jonathanbgn.com/2021/09/30/illustrated-wav2vec-2.html](https://jonathanbgn.com/2021/09/30/illustrated-wav2vec-2.html)Hope this is useful : )",POS
12,q7amqo,Label unstructured data using Enterprise Knowledge Graphs,https://www.reddit.com/r/LanguageTechnology/comments/q7amqo/label_unstructured_data_using_enterprise/,artreven,LanguageTechnology,1970-01-01 00:00:01.634130016,,7,0,hi. I have published a new blogpost about entity linking with domain-specific enterprise KGs: [https://revenkoartem.medium.com/label-unstructured-data-using-enterprise-knowledge-graphs-3-ca3cd1b14a36](https://revenkoartem.medium.com/label-unstructured-data-using-enterprise-knowledge-graphs-3-ca3cd1b14a36) there is also a piece of code that allows to train the model and try it out.,NEU
13,q7hq6t,Job opportunities for a fellow linguist?,https://www.reddit.com/r/LanguageTechnology/comments/q7hq6t/job_opportunities_for_a_fellow_linguist/,dalkian_,LanguageTechnology,1970-01-01 00:00:01.634150607,,2,1,"Hello folks, first time posting here, I bring a potentially different question.My girlfriend is a newly graduated linguist applying for a Master's Degree in Linguistics. She doesn't have a computer science or mathematics background.But we were looking online and some NLP job openings do seem to exist for linguists and natural language teachers/researchers.I am a computer scientist with a background in programming languages, and I have some (albeit not deep) knowledge of machine learning.We are looking for a way to get her into a more company-oriented career, rather than an academic one as a lecturer. Always good to have options.What are your thoughts on this? Could she potentially land a NLP-related job?  How much of a statistics/machine learning/computer science background would she have to develop?",POS
14,q7jr8p,"Fresh Machine Translation benchmark study: 29 MT engines, 13 language pairs, 7 domains (Aug 2021)",https://www.reddit.com/r/LanguageTechnology/comments/q7jr8p/fresh_machine_translation_benchmark_study_29_mt/,ksavenkov,LanguageTechnology,1970-01-01 00:00:01.634156389,,1,0,"Fresh Machine Translation benchmark study: 29 MT engines, 13 language pairs, 7 domains (Aug 2021)Hi folks, we've just published our new State of the Machine Translation 2021 report we have prepared together with TAUS https://hubs.la/H0ZbJhN0Every year we release an independent multi-domain evaluation of MT engines to help you choose the best-fit providers for your language pair and industry sector. In this year’s edition, we analyzed 29 commercial and open-source MT engines across 13 language pairs and 7 key domains, including Healthcare, Education, Financial, Legal, Hospitality, and General. We also explain what scores to use for MT evaluation.Happy reading, and please share your questions and ideas afterward!",POS
15,q78mna,"Hello, I'm getting into NLP and wandering if I should start with a project or normal courses.",https://www.reddit.com/r/LanguageTechnology/comments/q78mna/hello_im_getting_into_nlp_and_wandering_if_i/,icecoldmind,LanguageTechnology,1970-01-01 00:00:01.634122616,,4,4,I need to learn NLP for a position and need help on whether to take a project to learn with or start with a book/course (One that seems interesting is [this](https://web.stanford.edu/%7Ejurafsky/slp3/ed3book_sep212021.pdf).)Background: I'm already familiar with DNN's and a bit familiar with CNN's and their architectures. Already know what LSTM is.A project that I want to do is an Arabic document (mostly books) summarizer.Which should I do?,POS
16,q74ehn,Explore how winkNLP runs on the web browser,https://www.reddit.com/r/LanguageTechnology/comments/q74ehn/explore_how_winknlp_runs_on_the_web_browser/,r4chn4,LanguageTechnology,1970-01-01 00:00:01.634102883,,5,0,"We are super excited to announce that you can now use [winkNLP](https://github.com/winkjs/wink-nlp) directly on the [web browser](https://winkjs.org/wink-nlp/wink-nlp-in-browsers.html) 📷 The entire package, along with the a full-fledged model for English is under 1MB when gzipped 📷  [\#NodeJS](https://twitter.com/hashtag/NodeJS?src=hashtag_click) [\#JavaScript](https://twitter.com/hashtag/JavaScript?src=hashtag_click) [\#NLP](https://twitter.com/hashtag/NLP?src=hashtag_click)  [\#webdev](https://twitter.com/hashtag/webdev?src=hashtag_click) [\#mobiledev](https://twitter.com/hashtag/mobiledev?src=hashtag_click) [\#MachineLearning](https://twitter.com/hashtag/MachineLearning?src=hashtag_click) [\#NodeJS](https://twitter.com/hashtag/NodeJS?src=hashtag_click) [\#opensource](https://twitter.com/hashtag/opensource?src=hashtag_click) [\#js](https://twitter.com/hashtag/js?src=hashtag_click) [\#APIs](https://twitter.com/hashtag/APIs?src=hashtag_click)",POS
17,q754ts,Formal Description of Prompting: Systematic Survey of Prompting Methods in NLP (P.1),https://youtu.be/K3MasIU25Zw,prakhar21,LanguageTechnology,1970-01-01 00:00:01.634106103,,2,0,,NEU
18,q6s7xh,JAX/Flax speedup on HuggingFace,https://www.reddit.com/r/LanguageTechnology/comments/q6s7xh/jaxflax_speedup_on_huggingface/,jamescalam,LanguageTechnology,1970-01-01 00:00:01.634062528,,11,0,"A friend of mine pointed out the faster compute times of JAX/Flax vs PyTorch testing by HuggingFace [over here](https://github.com/huggingface/transformers/tree/master/examples/flax), maybe I'm just late to the party but they're pretty significant, MLM training for example is 15h32m with JAX/Flax vs 23h46m with PyTorch/XLAThought it was cool, maybe a good idea to put some time into JAX",POS
19,q6xxox,Machine Translation With Sequence To Sequence Models And Dot Attention Mechanism,https://blog.paperspace.com/nlp-machine-translation-with-keras/,athos45678,LanguageTechnology,1970-01-01 00:00:01.634079766,,3,0,,NEU
20,q6vlv4,A tutorial on how to create quick NLP Text Generation Using Gradient Workflows and GitHub,https://blog.paperspace.com/nlp-text-generation-using-gradient-workflows-and-github-integration/,athos45678,LanguageTechnology,1970-01-01 00:00:01.634072491,,2,0,,NEU
21,q6rpa3,"[Spacy and Yake] 107+ million journal articles, mined: the General Index (4.7 TiB)",/r/DataHoarder/comments/q5pcgs/107_million_journal_articles_mined_the_general/,nemobis,LanguageTechnology,1970-01-01 00:00:01.634061016,,2,0,,NEU
22,q6jai3,How do I specify a max character length per sentence for summarization using transformers (or something else!)?,https://www.reddit.com/r/LanguageTechnology/comments/q6jai3/how_do_i_specify_a_max_character_length_per/,nikotime,LanguageTechnology,1970-01-01 00:00:01.634035133,,6,2," Hi there,I am exploring different summarization models for news articles and am struggling to work out how to limit the number of characters per sentence using huggingface pipelines, or if this is even possible/a silly question to begin with!I have the following setup when being passed through the article text and model name of ‘facebook/bart-large-cnn’, ‘google/pegasus-cnn\_dailymail’ and ‘sshleifer/distilbart-cnn-6-6’:summarizer = pipeline(“summarization”, model=model\_name)  summarized = summarizer(article\_text, max\_length=118, clean\_up\_tokenization\_spaces=True, truncation = True)The articles range in length from 100 words to 1000 words.I am hoping to cap the number of characters per sentence to 118, a hard cap for my application. When I set max\_length to 118 they usually are below this limit but can be, say, 220 characters or sometimes just truncate off at the end.Alternatively, if there are different summarization methods which allow limiting if it is not possible using transformers then would love to hear.Would be wonderful if someone could let me know what I’m doing wrong!  Thanks a lot",POS
23,q6ta1d,Sentiment Analysis on Bug reports' description,https://www.reddit.com/r/LanguageTechnology/comments/q6ta1d/sentiment_analysis_on_bug_reports_description/,lucifer955,LanguageTechnology,1970-01-01 00:00:01.634065620,,1,0,"Does the sentiment analysis on bug reports' description field important to severity prediction?If it is, what things can be done to improve the process?Thank you for your kind replies.",POS
24,q68ilm,How should I engineer features for Named Entity Identification task?,https://www.reddit.com/r/LanguageTechnology/comments/q68ilm/how_should_i_engineer_features_for_named_entity/,SnooHabits4550,LanguageTechnology,1970-01-01 00:00:01.633994320,,3,7,"I was working on Named Entity Identification (not recognition) task. In this NLP task, given a sentence, the model has to predict whether each word (aka token) is named entity or not. The dataset used was CONLL2003 dataset.Initially, I included a feature `first-letter-capital` which was `1` if a token has its first letter capitalized. The model learned to predict the first word of each sentence as a named entity.So I removed this feature and added a feature `first-letter-capital-for-non-sentence-start-word`, which was `1` if a word is not the first word of the sentence and has the first letter a capital. This made the model classify the first word of each sentence as a non-named entity.When I kept neither, the model predicted no word as a named entity. Why this might have happened? Can someone share their insight?**PS:*** I am using SVM (and I have to solve this problem with SVM only as that's what the task given to me is).* I am not using any word embedding!!! Somehow it was taking a lot of execution time with SVM (may be due to 300 dimensions of embeddings). I simply formed some features by parsing sentences / surrounding tokens of the target token   (I know this simply reduces down this task to possibly non NLP simple classification task)   * `first-letter-capital-for-non-sentence-start-word` required to check if the target token was the first one in the sentence.   * Feature  `first-letter-capital`  does not need to consider surrounding tokens* There are other features too (like POS tags etc), but they are not much related here as they don't relate with a capitalization of any letter of the tokens",POS
25,q6fzfj,GEC Master's Research Proposal: English or Japanese?,https://www.reddit.com/r/LanguageTechnology/comments/q6fzfj/gec_masters_research_proposal_english_or_japanese/,Figmieser,LanguageTechnology,1970-01-01 00:00:01.634020504,,1,3,"I am applying for a Japanese NLP master's program, and I have decided that I am interested in Grammatical Error Correction. My issue is choosing a research topic to list on the application, and particularly what language to work with. Let us assume that the jobs I would apply to in the future will be working with English.If I choose to do something in English, it is clearly the largest market and has the most research being done. I could use the latest public resources immediately and there are huge and detailed corpora. However, coming up with research ideas is proving hard for me. It seems that every time I have an idea, I search and find that it has already been done by people far beyond my level, and matters are accelerating if anything. I also feel like anything I could do would be such a drop in the bucket.On the other hand, I can do something with Japanese. However, it has a learner population of just a few million, and the native population is actually shrinking. In terms of research, there are much more gap to fill and unexplored paths, but there are fewer tools and corpora available. My Japanese level is N2/B2, so I can survive in the uni and approach text, but I probably won't be the best choice to write authoritative grammatical rules or annotations or anything.I'm really wavering trying to figure this out. To employers, does it look better to work on the language with fewer resources, since it implies that I can do at least as well in the richer environment of English?       I'm hearing that the majority of Japanese NLP researchers are choosing English, and they could surely do better Japanese work than I can, so that has been worrying me as well.My core question is whether Japanese GEC is a reasonable choice for a native English speaker, but I am also open to any GEC research suggestions at all, since I am still just starting off on the proposal.",POS
26,q68xob,"Do any of you know if there is an app that based on your typing can create a list with the words of your vocabulary(and any other useful stats like spelling mistakes,...) ?",https://www.reddit.com/r/LanguageTechnology/comments/q68xob/do_any_of_you_know_if_there_is_an_app_that_based/,Super-Promise-3794,LanguageTechnology,1970-01-01 00:00:01.633995627,,3,1,,NEU
27,q5ymki,Available Filipino / Tagalog Dictionary for LIWC,https://www.reddit.com/r/LanguageTechnology/comments/q5ymki/available_filipino_tagalog_dictionary_for_liwc/,Anonymous_Dawn,LanguageTechnology,1970-01-01 00:00:01.633966907,,8,0,"Hello!I am trying to extract features from texts using the Linguistic Inquiry and Word Counter. The texts has both English and Filipino / Tagalog as its language. After checking through their documentation and asking them, they mentioned that they only have English and other certain languages except Filipino / Tagalog. But they did mention that we can use custom-made dictionaries to apply it to the text with Filipino / Tagalog language in it. So I would just like to ask if there are any available Filipino / Tagalog dictionary files that we can use for LIWC?Thanks!",POS
28,q5yfhw,Need Help With LDA Topic Modelling,https://www.reddit.com/r/LanguageTechnology/comments/q5yfhw/need_help_with_lda_topic_modelling/,Meiravulaa,LanguageTechnology,1970-01-01 00:00:01.633966343,,2,0,Hey ThereI've been playing with LDA for topic modelling recently and been wondering - how do you assess the results of this model not manually? I looked for ways to do it but didn't find many interesting leads. Also - any rule of thumb for setting the number of topics? and any other useful tips you would give to a newbie in this area?TIA,POS
29,q5q5ec,GitHub - winkjs/wink-nlp: Developer friendly Natural Language Processing ✨,https://www.reddit.com/r/LanguageTechnology/comments/q5q5ec/github_winkjswinknlp_developer_friendly_natural/,r4chn4,LanguageTechnology,1970-01-01 00:00:01.633935092,,10,0,"[**WinkNLP**](https://github.com/winkjs/wink-nlp) is a **developer friendly** JavaScript library for Natural Language Processing (NLP). It is designed specifically to make development of NLP solutions **easier** and **faster.**winkNLP is optimised for the right balance of performance and accuracy. The package can handle large amount of raw text at speeds **over 525,000 tokens/second** for the entire NLP pipeline. And with a **test coverage of \~100%**, [wink-nlp](https://github.com/winkjs/wink-nlp) is a tool for building production grade systems with confidence.",POS
30,q5xp76,How to compare speed between NLP models,https://www.reddit.com/r/LanguageTechnology/comments/q5xp76/how_to_compare_speed_between_nlp_models/,Savings_Health780,LanguageTechnology,1970-01-01 00:00:01.633964339,,2,2,"Hey everyone, How do you compare the speed between, say, two NLP models? For example comparing one that uses Glove and one that uses Word2Vec?",NEU
31,q5w7wt,Video Series on How to Create a Virtual Assistant using Python,/r/learnpython/comments/q5w5u5/video_series_on_how_to_create_a_virtual_assistant/,limapedro,LanguageTechnology,1970-01-01 00:00:01.633960021,,1,0,,NEU
32,q5ibxw,Keyphrase extraction tools for non-english languages,https://www.reddit.com/r/LanguageTechnology/comments/q5ibxw/keyphrase_extraction_tools_for_nonenglish/,pauloamed,LanguageTechnology,1970-01-01 00:00:01.633905038,,10,2,"Hey, people! Hope y'all are doing fine!**TLDR: Please share fine key phrase extraction tools for Portuguese, Spanish and English**I've been trying to find a nice key-phrase extraction tool for Portuguese, Spanish and English. However, it hasn't been a trivial task since most tools I've found require some effort for handling non-English languages and also because comparison between these tools isn't that feasible. Also some papers I've found provide no or ill maintained code, making its usage difficult. So it isn't that simple at all finding fine tools for this task.",POS
33,q5ryjv,Preparing data for training NER models,https://www.reddit.com/r/LanguageTechnology/comments/q5ryjv/preparing_data_for_training_ner_models/,arush1836,LanguageTechnology,1970-01-01 00:00:01.633943898,,1,5,"Training most of the Named Entity Recognition (NER) models for example [Flair](https://github.com/flairNLP/flair) usually needs to format data in [BOI tagging](https://en.wikipedia.org/wiki/Inside-outside-beginning_(tagging)) scheme as shown below where each sentence is separated by blank line    George N B-PER    Washington N I-PER    went V O    to P O    Washington N B-LOC        Sam N B-PER    Houston N I-PER    stayed V O    home N OBut instead of labeled text data we have entity data separated by newline in text files, so if we process the data in above format it will look something like as below which only contains entity information    George N B-PER    Washington N I-PER        Washington N B-LOC        Sam N B-PER    Houston N I-PERIs it ok if processed data looks as above",POS
34,q5epba,Findings of EMNLP 2021 Poster Presentation?,https://www.reddit.com/r/LanguageTechnology/comments/q5epba/findings_of_emnlp_2021_poster_presentation/,shanestorks,LanguageTechnology,1970-01-01 00:00:01.633893522,,9,3,"Did anyone else accepted to Findings of EMNLP 2021 receive an email from PC EMNLP-2021 asking if you wanted to present a poster at EMNLP 2021? If you filled out the attached form, have you heard any details back? Hoping I'll hear back from them soon so I can plan travel.",POS
35,q518sn,Zero-Shot Crosslingual Sentence Simplification (NLP Research Paper Walkthrough),https://youtu.be/JOB7gwufvAw,prakhar21,LanguageTechnology,1970-01-01 00:00:01.633840844,,12,2,,NEU
36,q4xdvc,using tf-idf vectorizer with JSON file,https://www.reddit.com/r/LanguageTechnology/comments/q4xdvc/using_tfidf_vectorizer_with_json_file/,jtksm,LanguageTechnology,1970-01-01 00:00:01.633825127,,0,1,"initially was using bow model instead of tf-idf, the input was a json file (dictionary). i found many online using just a corpus to do tf-idf but not a json file, does anyone know how to do it with json file? my json dataset is quite large",NEU
37,q4btiq,Google AI Introduces ‘FLAN’: An Instruction-Tuned Generalizable Language (NLP) Model To Perform Zero-Shot Tasks,https://www.reddit.com/r/LanguageTechnology/comments/q4btiq/google_ai_introduces_flan_an_instructiontuned/,techsucker,LanguageTechnology,1970-01-01 00:00:01.633744192,,31,2,"To generate meaningful text, a machine learning model needs a lot of knowledge about the world and should have the ability to abstract them. While language models that have been trained to accomplish this are becoming increasingly capable of acquiring this knowledge automatically as they grow, it is unclear how to unlock this knowledge and apply it to specific real-world activities.Fine-tuning is one well-established method for doing so. It involves training a pretrained model like BERT or T5 on a labeled dataset to adjust it to a downstream job. However, it has a large number of training instances and stored model weights for each downstream job, which is not always feasible, especially for large models.A recent Google study looks into a simple technique known as instruction fine-tuning, sometimes known as instruction tuning. This entails fine-tuning a model to make it more receptive to performing NLP (Natural language processing) tasks in general rather than a specific task. # [Google AI Blog](https://ai.googleblog.com/2021/10/introducing-flan-more-generalizable.html) | [5 Min Read](https://www.marktechpost.com/2021/10/08/google-ai-introduces-flan-an-instruction-tuned-generalizable-language-nlp-model-to-perform-zero-shot-tasks/) | [Paper](https://arxiv.org/pdf/2109.01652.pdf) | [Github](https://github.com/google-research/flan)",POS
38,q4f6qw,Training NER models for detecting custom entities,https://www.reddit.com/r/LanguageTechnology/comments/q4f6qw/training_ner_models_for_detecting_custom_entities/,arush1836,LanguageTechnology,1970-01-01 00:00:01.633758588,,7,12,"Hello everyone, we are working on a task to detect certain `custom entities` in the text, we tried training [sPacy](https://spacy.io/) but it's not convergingCan anyone suggest some other `Named Entity Recognition (NER)` models which can be trained to detect custom entities",POS
39,q4fau3,When should you train a custom tokenizer/language model?,https://www.reddit.com/r/LanguageTechnology/comments/q4fau3/when_should_you_train_a_custom_tokenizerlanguage/,LifeTimeLearner99,LanguageTechnology,1970-01-01 00:00:01.633759170,,2,4,"I am trying to better understand when you should train a custom tokenizer and language model for your dataset. My go-to is spaCy and prodigy, but I realize there are limitations. Training a RoBERTa model or something similar with HuggingFace seems like the MLM could give you some advantages over what I would get with spaCy models plus prodigy Active Learning, just given the robustness of the model learning the domain context. My primary cases are NER & text classification. Any suggestions or tips would be greatly appreciated.",POS
40,q3yrby,Introduction to Natural Language Processing (blog),https://www.reddit.com/r/LanguageTechnology/comments/q3yrby/introduction_to_natural_language_processing_blog/,IbrahimSharaf,LanguageTechnology,1970-01-01 00:00:01.633702949,,14,0,Towards Data Science: [https://towardsdatascience.com/introduction-to-natural-language-processing-nlp-323cc007df3d](https://towardsdatascience.com/introduction-to-natural-language-processing-nlp-323cc007df3d)KDnuggets: [https://www.kdnuggets.com/2019/10/introduction-natural-language-processing.html](https://www.kdnuggets.com/2019/10/introduction-natural-language-processing.html)&#x200B;Feedback is welcome!,POS
41,q43qjg,Any allennlp users in this sub?,https://www.reddit.com/r/LanguageTechnology/comments/q43qjg/any_allennlp_users_in_this_sub/,synthphreak,LanguageTechnology,1970-01-01 00:00:01.633717745,,7,3,"I have a whole host of questions that the official allennlp docs are unclear on - too many to post individually here - but no one to answer them.If there are any allennlp users in this sub who wouldn't mind discussing them with me one-on-one, I would appreciate it tremendously. Apologies for the nebulous post, but thank you in advance!",POS
42,q40avj,BART: Denoising Sequence-to-Sequence Pre-training for NLG & Translation (Explained),https://youtu.be/MxNnl_gHV1Y,deeplearningperson,LanguageTechnology,1970-01-01 00:00:01.633707625,,7,0,,NEU
43,q4a9ak,Comparative study of extractive summarization,https://www.reddit.com/r/LanguageTechnology/comments/q4a9ak/comparative_study_of_extractive_summarization/,Ok_Inspection_5208,LanguageTechnology,1970-01-01 00:00:01.633738365,,1,0,"Hello,I've been looking for a comparative study for a while that shows the characteristics of each model of State of art in a table and I didn’t find, who can help me?Example: BERT is has bi-directional encoder ✔️ multilingual ✔️ used for summarization ✔️ and other features that distinct it from other models ..Thank you",POS
44,q3xo05,NLP Conferences with a decent industry track?,https://www.reddit.com/r/LanguageTechnology/comments/q3xo05/nlp_conferences_with_a_decent_industry_track/,Captain_Flashheart,LanguageTechnology,1970-01-01 00:00:01.633699356,,4,0,"I just got back from RecSys2021 and was surprised in a good way by the industry presentations. Being mostly a NLP guy - but one who hasn't attended a NLP conference for years, I couldn't stop wondering if any of 'ours' have a similar focus. Are there any good conferences that mix academia with industry?",POS
45,q3t9hx,How to approach Jurafsky & Martin for learning NLP?,https://www.reddit.com/r/LanguageTechnology/comments/q3t9hx/how_to_approach_jurafsky_martin_for_learning_nlp/,jennkent,LanguageTechnology,1970-01-01 00:00:01.633680230,,11,2,"I'm looking to get a good overview/review of NLP in preparation for grad school. I was looking at the PhD programs I'm interested in, and quite a few of them list the Jurafsky & Martin textbook as requisite knowledge for their qualifying examinations. I've read portions of the book for classes in undergrad, but I'm not familiar with all of the topics covered, and I'd also like to review the topics I'm more familiar with. However, the book is quite long and seems tedious to read from cover to cover.If I'm more of a visual learner, do [Jurafsky's NLP lectures from Stanford](https://www.youtube.com/playlist?list=PLLssT5z_DsK8HbD2sPcUIDfQ7zmBarMYv) cover the topics from the textbook well enough? Or is there another way to approach learning from the textbook (or a better way to learn core topics in NLP altogether)?",POS
46,q3xmt6,Using CLIP to get sentence/description from image,https://www.reddit.com/r/LanguageTechnology/comments/q3xmt6/using_clip_to_get_sentencedescription_from_image/,matigekunst,LanguageTechnology,1970-01-01 00:00:01.633699244,,3,0,"I want to use CLIP to generate a sentence by inputting an image. I've worked with a lot of implementations where the opposite is done. But I'm not very acquainted with modern text generation models. I'm guessing the principle is similar: optimise the latent vector that CLIP gives you and generate text using this latent vector, convert back into CLIP's latent space again and calculate the loss using the CLIP latent of the image. Any suggestions on which model I should use for this? Preferably one that I can run on a 3090.",POS
47,q3v092,Removing whitespace between characters,https://www.reddit.com/r/LanguageTechnology/comments/q3v092/removing_whitespace_between_characters/,jtksm,LanguageTechnology,1970-01-01 00:00:01.633688964,,6,2,"Any NLP algorithm that removes extra whitespaces in between characters in a word (not in between words)?Example: ""How m uc h is it?"" should be interpreted as ""How much is it?"" instead of ""Howmuchisit""codes:    tokens = [lemmatizer.lemmatize(word.lower()) for word in nltk.word_tokenize(text) if word not in ignore_words] appreciate anyone's help!",POS
48,q3v2b6,AI & Machine Learning Toolkit Bundle from Morgan & Claypool,https://medium.com/@Humble_Bundle_Partner_Blog/ai-machine-learning-toolkit-bundle-ab1b9ff40e23?9804,HU55LEH4RD,LanguageTechnology,1970-01-01 00:00:01.633689229,,2,0,,NEU
49,q3y7nm,"Objectives of NLP, NLU & NLG",https://www.reddit.com/r/LanguageTechnology/comments/q3y7nm/objectives_of_nlp_nlu_nlg/,SiebeA,LanguageTechnology,1970-01-01 00:00:01.633701196,,0,4,"I read on a blog about NLP the following:\- NLU: reads data and converts it to structured data.\- NLP:  NLP converts unstructured data to structured data. \- NLG:   NLG writes structured data. Isn't the NLG part false? Shouldn't it be: ""Converts structured data to natural language""Source: [https://www.xenonstack.com/blog/difference-between-nlp-nlu-nlg](https://www.xenonstack.com/blog/difference-between-nlp-nlu-nlg)",POS
50,q3tf7k,LDA model returns same words in all the topics,https://www.reddit.com/r/LanguageTechnology/comments/q3tf7k/lda_model_returns_same_words_in_all_the_topics/,Senior_Time_2928,LanguageTechnology,1970-01-01 00:00:01.633681031,,1,5,I'm running an LDA model with 14k unique tokens from 33k documents. The documents are questions and answers from a technical community and are rather short and focused on the same macro topic (SAP cloud Platform).I decided to extract 25 topics as I clustered the tags assigned to the original questions in groups and it seemed logical to divide them in 25 groups.I've run the model with 100 passes and 100 iterations for 7 hours but at the end I am still returned a model in which the topics are defined mostly by the same words and don't show significant differences. What could I do to improve my results?,POS
51,q3c95t,Just finished my first proper NLP project,https://www.reddit.com/r/LanguageTechnology/comments/q3c95t/just_finished_my_first_proper_nlp_project/,keeperclone,LanguageTechnology,1970-01-01 00:00:01.633622522,,15,4,"Today I launched my first ever twitter bot  [AAPLinsight](https://twitter.com/AAPLinsights) that focus on providing sentiment scores on $AAPL. I broke down my approaches in three categories: Apple Products, Company News and Social Media. These sentiment scores come from around 20 different sources in the web. The base model that I used was BERT and I added some additional layers to create a sentiment classiifer that specialises in financial news sentiment. Although it may be quite a simple project, I think it is quite cool and thank you for the subreddit for all the advices!",POS
52,q3ihgi,Looking for a table to text codebase,https://www.reddit.com/r/LanguageTechnology/comments/q3ihgi/looking_for_a_table_to_text_codebase/,narryRG,LanguageTechnology,1970-01-01 00:00:01.633640261,,3,4,"Hi, I am trying to implement a table to text summarizer for pharma tables. I am looking for existing codebase which can help me jumpstart the project. Any suggestions? I tried looking for them (papers that use ToTTo, WebNLG etc) but most of them do not have complete code. Thanks!",POS
53,q37xf3,Styleformer performance. Or anything that turns informal to formal.,https://www.reddit.com/r/LanguageTechnology/comments/q37xf3/styleformer_performance_or_anything_that_turns/,hanoian,LanguageTechnology,1970-01-01 00:00:01.633609432,,5,3,"Hi, everyone.I have been playing around with Styleformer today and am wondering about performance. I'm unsure if this is the right place to ask.https://github.com/PrithivirajDamodaran/StyleformerI set up a basic Flask server so it would be loaded into ram and each query takes around two seconds on my laptop. What sort of server would be required to make this decently fast? Is it something I'd use DigitalOcean for, or are there better options?Sorry if this question is far too basic. It's my first day using Python and this sort of thing. I love the output of Styleformer and would rather use it than an API.Cheers.",POS
54,q3dtez,Allennlp: What in the frig is a Predictor?,https://www.reddit.com/r/LanguageTechnology/comments/q3dtez/allennlp_what_in_the_frig_is_a_predictor/,synthphreak,LanguageTechnology,1970-01-01 00:00:01.633626910,,1,4,"Title says it all. I know there is [a tutorial](https://guide.allennlp.org/training-and-prediction#4), and this description in [the docs](https://docs.allennlp.org/v2.7.0/api/predictors/predictor/):> a `Predictor` is a thin wrapper around an AllenNLP model that handles JSON -> JSON predictions that can be used for serving models through the web API or making predictions in bulk.But I dunno, I just don't get it. I had initially thought a `Predictor` was, intuitively, the ""glue"" needed on the backend to link up a `Model` and a `DatasetReader` and have them share information, but I'm able to train a model using `allennlp train` + a config without so much as (knowingly) touching a `Predictor`. This finding only heightened my confusion about what a `Predictor` is and why I should care about it.If there are any allennlp users here, can you help me understand the purpose of this component of the pipeline, and how I should use it? Thanks!",POS
55,q3b4qv,T-V Distinction Classifier,https://www.reddit.com/r/LanguageTechnology/comments/q3b4qv/tv_distinction_classifier/,TheRedSphinx,LanguageTechnology,1970-01-01 00:00:01.633619288,,1,1,"Hi all,&#x200B;A bit of a shot in the dark, but I was wondering if there were any available tools to detect if a sentence in Spanish (or any language with this distinction) is using the formal or informal form of ""you"" through the T-V distinction?While one can make a naive baseline by explicitly checking for ""tu"" or ""usted"" in Spanish, this wouldn't capture word conjugations or the likes.",POS
56,q2tpel,Is Debatepedia website/dataset non-existent?,https://www.reddit.com/r/LanguageTechnology/comments/q2tpel/is_debatepedia_websitedataset_nonexistent/,shreddedcheese893,LanguageTechnology,1970-01-01 00:00:01.633554929,,7,2,"Hi all,The other day, I was looking at a paper DDA (Diversity Driven Attention) Model.https://arxiv.org/abs/1704.08300They scraped data from the Debatepedia website for the purpose of Query-Focused Abstractive Text summarization.However the links provided (in the bash script for scraping data from Debatepedia) are not accessible. I.e. I cannot access Debatepedia.https://github.com/PrekshaNema25/DiverstiyBasedAttentionMechanismDoes anyone know how I can access Debatepedia?Thanks.",NEU
57,q2oors,Best Cleaning Models or Processes,https://www.reddit.com/r/LanguageTechnology/comments/q2oors/best_cleaning_models_or_processes/,Gyllenspetz,LanguageTechnology,1970-01-01 00:00:01.633539757,,5,0,"Hello everyone, Happy wonderful Wednesday! I wanted to quickly ask the community about their favorite cleaning model or process. Prior to running analysis, as we all know very well the data gathering phase will always result in a ton of noise, how do you reduce this in the quickest and most accurate fashion?\- Do you build a pipe of specific cleaning stages (dedup, irrelevant language, terms used, normalize, remove stop words, lemmatize etc.)\- Have you built a model to remove posts and clean the data? How did you trained said model? How big was your training dataset? What steps did you take to validate or verify it's quality? \- Other processes?I appreciate any and all comments, have an awesome day!All the best, N",POS
58,q2kuq8,NLP Project (movie reviews),https://www.reddit.com/r/LanguageTechnology/comments/q2kuq8/nlp_project_movie_reviews/,Fit-Mycologist-6951,LanguageTechnology,1970-01-01 00:00:01.633528185,,7,7,"Hello, I would like to start a project where I can web scrape data from media websites (not user-posted reviews, but rather more professional media websites where they give a rating and an article or paragraphs of movie reviews (villain genre). 1. Web scrape movie review data for villains movie genre (i will have a set list of movies and websites I want to grab from)2. Store the webscraped data somewhere and then doing sentiment analysis as well as what made a villain movie successful or unsuccessful?3. Using python to do these analysis  (not sure which libraries) and then doing a kind of small app to visualize this data.Can anyone help me with the process in terms of what I should use and how to approach it? I would ideally like for this to be able to be used for other things such as if If I wanted to maybe use it to do hero movies or something instead of villains.",POS
59,q2bzev,"What really is perplexity, and why is it important for model evaluation?",https://www.reddit.com/r/LanguageTechnology/comments/q2bzev/what_really_is_perplexity_and_why_is_it_important/,AggravatingNail7400,LanguageTechnology,1970-01-01 00:00:01.633490159,,12,3,"I know that its uncertainty, but how is it any different from entropy? Super confused and fairly new to NLP, so would love any easy-to-understand explanations! Thanks!",POS
60,q1xky0,Free 'course' on vector similarity search and Faiss!,https://www.reddit.com/r/LanguageTechnology/comments/q1xky0/free_course_on_vector_similarity_search_and_faiss/,jamescalam,LanguageTechnology,1970-01-01 00:00:01.633444267,,42,6,"Hi all, I've been working with [Pinecone](https://www.pinecone.io) for the last few months on putting together a big set of articles and videos covering many of the **essentials behind vector similarity search**, and how to apply what we learn using **Faiss** (and sometimes even plain Python). Today we released the final (for now) article on HNSW.With that, I wanted to share a *'course guide'* with you all, every link below takes you to the article, and in each article, we included one or two videos too - you can read and watch in whichever order you like, but we think this makes the most sense!# Course Guide## Part 1: Introduction1. [Semantic Search: Measuring Meaning From Jaccard to Bert](https://www.pinecone.io/learn/semantic-search/)2. [Getting Started with Faiss](https://www.pinecone.io/learn/faiss-tutorial/)3. [Nearest Neighbor Indexes for Similarity Search](https://www.pinecone.io/learn/vector-indexes/)## Part 2: Algorithm Deep Dives4. [Traditional Locality Sensitive Hashing (LSH)](https://www.pinecone.io/learn/locality-sensitive-hashing/)5. [Random Projection for LSH](https://www.pinecone.io/learn/locality-sensitive-hashing-random-projection/)6. [Compression with Product Quantization](https://www.pinecone.io/learn/product-quantization/)7. [Hierarchical Navigable Small Worlds (HNSW) Graphs](https://www.pinecone.io/learn/hnsw/)## Part 3: More Advanced Index Concepts8. [Filtering: The Missing WHERE Clause in Vector Search](https://www.pinecone.io/learn/vector-search-filtering/)9. [Composite Indexes: Facebook AI and the Index Factory](https://www.pinecone.io/learn/composite-indexes/)We've written and recorded *a lot* of content, hopefully, you'll find vector search as fascinating as I do :)",POS
61,q2108w,Your experience with referrals in the industry.,https://www.reddit.com/r/LanguageTechnology/comments/q2108w/your_experience_with_referrals_in_the_industry/,svantevid,LanguageTechnology,1970-01-01 00:00:01.633454118,,9,1,"I'm currently ending my PhD and looking into different options for a job and I'm trying to understand the role of referrals better. Some companies pay as much as 10k for successful referrals so I'm curious about the experience you have had with referrals in the past. Have you referred friends for positions? Why yes/no? Is it weird to ask out for a referral and vice versa, have you ever asked somebody if you can refer them for a position out of the blue?",POS
62,q23ife,New to Python and NLP but have to work on a basic NLP project at work (classification of text into a topic),https://www.reddit.com/r/LanguageTechnology/comments/q23ife/new_to_python_and_nlp_but_have_to_work_on_a_basic/,imposter_i_am,LanguageTechnology,1970-01-01 00:00:01.633461734,,6,3,"I do know very basic python(syntax not programming concepts) but that's about it.Can someone please help me where to begin?Should I go about learning Python first , get myself a course? I really want to do good at work and hence thought I could ask here for advice.",POS
63,q29bdq,Probing Language Model with WIKI-PSE: looking for implementation details,https://www.reddit.com/r/LanguageTechnology/comments/q29bdq/probing_language_model_with_wikipse_looking_for/,chr0nicpa1n,LanguageTechnology,1970-01-01 00:00:01.633480066,,2,0,"Hi all, I found this [https://github.com/yyaghoobzadeh/WIKI-PSE](https://github.com/yyaghoobzadeh/WIKI-PSE), related to the work [https://arxiv.org/pdf/1906.03608.pdf](https://arxiv.org/pdf/1906.03608.pdf), and I am looking for any implementation or details.   For those who want to read or are familiar with the article:I would like to try to implement the 34 MLPs (one for each as described in section 3) but I can't figure out what the input is for each MLP. Also, wanting to probe BERT, I found this other work [https://arxiv.org/abs/2004.12198](https://arxiv.org/abs/2004.12198). But I can't figure out the implementation structure.  Thanks to anyone who may be interested :D",POS
64,q28u5u,Locate handwriting in mixed text document,https://www.reddit.com/r/LanguageTechnology/comments/q28u5u/locate_handwriting_in_mixed_text_document/,Mattyjoels,LanguageTechnology,1970-01-01 00:00:01.633478365,,1,1,Hi all!I currently have a project to OCR mixed text documents. Tesseract is fine for machine text but struggles for handwriting.I am looking for a method to only recognise sections with handwriting so it can be shipped off to a Vision API. Does anyone know any low computational methods to do this?One thought is to use the confidence output from tesseract to filter out bad segments to ship.Thanks,NEG
65,q1t4ur,German POS Corpus for Commercial use,https://www.reddit.com/r/LanguageTechnology/comments/q1t4ur/german_pos_corpus_for_commercial_use/,PythonicParseltongue,LanguageTechnology,1970-01-01 00:00:01.633427630,,4,9,I'm trying to find a German corpus with POS tags that can be used for commercial purposes. I know about the TIGER corpus for which you could get a commercial license at leat in theory... however they haven't responded in months. Is there any alternative?,NEU
66,q1tmtb,Identifying medical information in text?,https://www.reddit.com/r/LanguageTechnology/comments/q1tmtb/identifying_medical_information_in_text/,MeloTheMelon,LanguageTechnology,1970-01-01 00:00:01.633429845,,2,4,"I have access to a large dataset of medical texts - notes from doctors about patients etc. - and was wondering if there is a way to take one such text and automatically create tags for it.Let's say the text describes the condition of a patient with Covid, then the algorithm would look over the text and filter out terms like ""cough"" ""covid-19"" ""high temperature"" etc. I guess what I am looking for is a dataset of medical terms I could use on the texts.If I want to train an ML model for this, focusing on one disease for the beginning, what would be a good amount of training data? I could tag a bunch of texts myself and just provide this as training data.Obviously, I'm pretty new to the whole field, so links to similar projects or papers would be great too.",POS
67,q1yyld,Hot off the press! Exploring NLP Part 2: A New Way to Measure the Quality of Synthetic Text,https://gretel.ai/blog/exploring-nlp-part-2-a-new-way-to-measure-the-quality-of-synthetic-text,Repeat-or,LanguageTechnology,1970-01-01 00:00:01.633448177,,1,0,,NEU
68,q1kfk5,Phone interview for Language Engineer job at Amazon,https://www.reddit.com/r/LanguageTechnology/comments/q1kfk5/phone_interview_for_language_engineer_job_at/,teeecue,LanguageTechnology,1970-01-01 00:00:01.633397621,,11,17,"I have one coming up soon and have no idea how to prepare for it or what kind of questions I should expect. I tried to search reddit and only found posts about onsite interviews. If anyone could share their experience I'd be very grateful.Not sure if important, but the job is not language-specific afaik. I was told earlier that I will be interviewed by 2 people but in the most recent email, the recruiter says ""interviewer"" singular, so not sure anymore.",POS
69,q1ide0,Groningen Master in Voice Technology,https://www.reddit.com/r/LanguageTechnology/comments/q1ide0/groningen_master_in_voice_technology/,Quimoxx,LanguageTechnology,1970-01-01 00:00:01.633390772,,2,2,"[https://www.rug.nl/masters/voice-technology/](https://www.rug.nl/masters/voice-technology/)Hey guys, anyone doing the new MSc in Voice Technology at the University of Groningen?It sounds quite interesting and they seem to accept students from a very diverse background, different to many CL Master's. It doesn't seem to be very NLP-focused though, which might be a bummer for many people on this sub.Anyway, apparently, the degree only started this fall for the first time ever, and there's little information on the actual contents So, if anyone's doing it, I would love to know what it's like!",POS
70,q11nr2,What's the path to computational linguistics / NLP?,https://www.reddit.com/r/LanguageTechnology/comments/q11nr2/whats_the_path_to_computational_linguistics_nlp/,garlicpowders,LanguageTechnology,1970-01-01 00:00:01.633337800,,28,12," There's quite a few posts out there detailing what a computational linguist does. But I have no grasp on the jargon, so in absolutely layman terms, what would you say a computational linguist works on day to day? And what is the distinction between comp ling and NLP, or if NLP is just the subgroup?I'm currently a junior at a US high school, and I'm beginning to consider what I'd like to study in the future. I'm fairly well-rounded, with no particular subject that I excel in so that's why I'm looking into comp ling despite having barely any experience with it in school or extracurriculars. I did the open round of NACLO last year and I didn't even qualify to the invitational round, but I thought that the logical thinking in application of language was fun, so that's why this field is one that I'm considering.Is there a way to get a taste of what the field is like? I'm self-learning python, but for ""real"" experience with comp ling there's much more advanced math required like having a solid grasp on calculus and linear algebra. I'm taking calculus this year, and likely AP stats senior year, but I could also add on an intro linear algebra course if that would make me a stronger applicant.What's a path I can take to become a computational linguist? What's recommended for undergrad, CS major, linguistics major, or comp ling major if offered? Even if undergrad comp ling was offered, would it be better to major in CS for greater flexibility in career choice, and then decide on whether or not to go for a master's in comp ling?Is there anything else I should consider before I have my heart set on computational linguistics?I would be grateful for answers to any or all questions, thank you!",POS
71,q1dain,Small-Bench NLP: Benchmark for small single GPU trained models in Natural Language Processing,https://arxiv.org/abs/2109.10847,kamalkraj,LanguageTechnology,1970-01-01 00:00:01.633376557,,2,0,,NEU
72,q19000,Question-Answering Model,https://www.reddit.com/r/LanguageTechnology/comments/q19000/questionanswering_model/,deum-vult,LanguageTechnology,1970-01-01 00:00:01.633365024,,3,2,Hey guys! I am a bit new to NLP and Question-Answering in general. How would one create a Question-Answering model on a very specific domain? I know that there are ways to train a given model (SimpleTransformers for example) but I was wondering what you guys would suggest for such a task.,POS
73,q16m99,Entity extraction from videos?,https://www.reddit.com/r/LanguageTechnology/comments/q16m99/entity_extraction_from_videos/,nikotime,LanguageTechnology,1970-01-01 00:00:01.633357993,,2,0,"Hi all,I am working on a recommendation engine which suggests the most likely related video(s) for a given news article. There is little to no metadata outside a video title so the approach that I am considering is automatically transcribing the video and then performing entity extraction on the transcript, performing the same entity extraction on the article text and comparing the two. My worry is entity extraction will be impacted negatively by noisy transcription. Does anyone have any recommendations on NER from messy data or as to whether my approach to the problem of linking relevant videos to articles is with relative merit? Thanks",NEG
74,q13oyo,creating a dataset for summerization,https://www.reddit.com/r/LanguageTechnology/comments/q13oyo/creating_a_dataset_for_summerization/,DunderSunder,LanguageTechnology,1970-01-01 00:00:01.633347516,,4,1,I'm creating a dataset for summerization and I have crawled 100k articles and summaries from 10 news sites.obviously there are some articles that are not good for the task. for example : article is  too short.what other requirements do you recommend so that I can filter out the bad ones.,NEG
75,q157ds,"I just released a ""Youtube name generator"" over the weekend by training a massive neural network",https://www.vadoo.tv/youtube-name-generator,ANil1729,LanguageTechnology,1970-01-01 00:00:01.633353287,,1,1,,NEU
76,q0cetn,NLP applications using Statistical Methods,https://www.reddit.com/r/LanguageTechnology/comments/q0cetn/nlp_applications_using_statistical_methods/,mommyzboy007,LanguageTechnology,1970-01-01 00:00:01.633243219,,7,4,"I am a novice in NLP. I have started reading HMM approach to Part of Speech Tagging and I am enjoying this ! I could really make use of some NLP techniques that invoke statistical methods to solve interesting problems.I consider that I have a pretty solid statistical and mathematical background, so I won't shy away from possibly very 'involved' approaches. Cheers !",POS
77,q059gv,Teach Computers to Understand Videos and Text without Labeled Data - VideoClip,https://youtu.be/vqMZjsIKUoQ,deeplearningperson,LanguageTechnology,1970-01-01 00:00:01.633214980,,5,0,,NEU
78,q031pb,Question about scraping unstructured texts using BERT,https://www.reddit.com/r/LanguageTechnology/comments/q031pb/question_about_scraping_unstructured_texts_using/,fartGesang,LanguageTechnology,1970-01-01 00:00:01.633207448,,4,3,"Hello,  First of all, I'm a data analyst with some data engineering background as well. I never really studied/worked with ML models...I am working on a project where I need to extract data from unstructured texts (PDF documents with multiple pages each). I assume it's possible to find the data I'm looking for in the texts. Since I know nothing about the text, and since it is unstructured, I looked into using BERT, pre-trained on the CoQA dataset to answer questions, based on:   [https://towardsdatascience.com/question-answering-with-a-fine-tuned-bert-bc4dafd45626](https://towardsdatascience.com/question-answering-with-a-fine-tuned-bert-bc4dafd45626).  I get good results from this pre-trained model if I manually locate the paragraph that contains the answer to the question, and let the model predict the answer with that paragraph as input. However, since I don't know in which paragraph the answer is hiding, this is clearly not helping me much. Some ideas I've tried:  * Splitting the text into paragraphs and asking the model to predict an answer for the same question for each paragraph. I assume I'll get the right answer, but I won't know which one it is.... So not really helpful. (I might be able to ask the model to predict again on the outputs from the  previous step, seems a bit messy but I'll try).* Extracting a list of headers from the text (meaning the title of each paragraph), and asking the model to predict which header's paragraph might contain the answer my question. This method works in some cases, but certainly not good enough.  Is there an elegant method you are familiar with? I'm sure I'm not the first person to try scraping large documents with BERT. Any inputs or ideas are welcome.   Thanks!",POS
79,pzvtyh,Suggestions on Cool NLP Projects!,https://www.reddit.com/r/LanguageTechnology/comments/pzvtyh/suggestions_on_cool_nlp_projects/,gooohjy,LanguageTechnology,1970-01-01 00:00:01.633183685,,13,10,"Hi all, receiving suggestions on any NLP projects that you may find cool in 2021! Currently brainstorming for an upcoming group project for school. It's an open-ended project where we have to build NLP models.When browsing past student's project choices I realised many of the projects were repetitive (e.g. hate speech detection, sentiment analysis, predicting stock prices). Would love to see if the community has any fresh ideas!Here are some interesting topics that I've noted down but would love to have more for me to think about. It could be anything, with existing papers or not.* Detecting personality based on social media* Automated essay scoring* Resume scoring/analysis**EDIT:** Thank you everyone for your contributions! Know that I'm looking into each and every one of them. You guys are awesome.",POS
80,pzqqq9,"Microsoft AI Unveils ‘TrOCR’, An End-To-End Transformer-Based OCR Model For Text Recognition With Pre-Trained Models",https://www.reddit.com/r/LanguageTechnology/comments/pzqqq9/microsoft_ai_unveils_trocr_an_endtoend/,techsucker,LanguageTechnology,1970-01-01 00:00:01.633160500,,19,2,"The problem of text recognition is a long-standing issue in document digitalization. Many current approaches for text recognition are usually built on top of existing convolutional neural network (CNN) models for image understanding and recurrent neural network (RNN) for char-level text generation. There are some latest progress records in text recognition by taking advantage of transformers, but this still needs the CNN as the backbone. Despite various successes by the current hybrid encoder/decoder methods, there is definitely some room to improve with pre-trained CV and NLP models.Microsoft research team unveils ‘[TrOCR](https://arxiv.org/pdf/2109.10282.pdf),’ an end-to-end Transformer-based OCR model for text recognition with pre-trained computer vision (CV) and natural language processing (NLP) models. It is a simple and effective model which is that does not use CNN as the backbone. TrOCR starts with resizing the input text image into 384 × 384, and then the image is split into a sequence of 16 × 16 patches used as the input to image Transformers. The research team used standard transformer architecture with the self-attention mechanism on both encoder and decoder parts where word piece units are generated as recognized text from an input image.# [4 Min Read](https://www.marktechpost.com/2021/10/02/microsoft-ai-unveils-trocr-an-end-to-end-transformer-based-ocr-model-for-text-recognition-with-pre-trained-models/)| [Paper](https://arxiv.org/pdf/2109.10282.pdf) | [Github](https://github.com/microsoft/unilm/tree/master/trocr)",POS
81,q06nf6,Binary classification with Bert And pytorch,https://www.reddit.com/r/LanguageTechnology/comments/q06nf6/binary_classification_with_bert_and_pytorch/,Future_Acadia_5807,LanguageTechnology,1970-01-01 00:00:01.633219833,,0,1,"If anyone could link me to any code,vids or resources I would be vvv thankful",POS
82,pzrlwl,Braifun-nlp: A free Natural Language Processing tool to help Researchers brain storm their ideas (Alpha release),https://youtu.be/h07sgvW0OyE,brainxyz,LanguageTechnology,1970-01-01 00:00:01.633165073,,2,0,,NEU
83,pzr72v,Roberta Tokenizer Query,https://www.reddit.com/r/LanguageTechnology/comments/pzr72v/roberta_tokenizer_query/,its_Soumya,LanguageTechnology,1970-01-01 00:00:01.633162850,,2,1,"I use roberta-base tokenizer     tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base',add_prefix_space=True)trained on english data to tokenize bengali just to see how it behaves . When I try to to encode a bengali character     tokenizer.encode('বা'), I get     [0, 1437, 35861, 11582, 35861, 4726, 2]which means that it finds some tokens in it vocabulary which match bengali characters even though train on english. On further exploration I find these are all special characters     ['<s>', 'Ġ', 'à¦', '¬', 'à¦', '¾', '</s>']. My question is why does it happen, isn't it supposed to output unknown tokens when applied on a new language ? Any help greatly appreciated",POS
84,pza2yi,Get list of authors for topic in gensim atmodel,https://www.reddit.com/r/LanguageTechnology/comments/pza2yi/get_list_of_authors_for_topic_in_gensim_atmodel/,Senior_Time_2928,LanguageTechnology,1970-01-01 00:00:01.633101239,,7,0,"In gensim atmodel  **get\_author\_topics(***author\_name)* returns the topic distribution for the selected author.Is there any method that given a topic, returns a list of the most probable authors?",NEU
85,pzdb72,How to get access to Wu Dao?,https://www.reddit.com/r/LanguageTechnology/comments/pzdb72/how_to_get_access_to_wu_dao/,math_is_my_religion,LanguageTechnology,1970-01-01 00:00:01.633110775,,3,0,Is there any way to get access to the Chinese language model from BAAI? Or is it proprietary?,NEU
86,pz9uy1,Training GPT-2 with HuggingFace Transformers to sound like a certain author,https://www.reddit.com/r/LanguageTechnology/comments/pz9uy1/training_gpt2_with_huggingface_transformers_to/,retepnotnaws1012,LanguageTechnology,1970-01-01 00:00:01.633100586,,7,7,"I'm training a GPT-2 model (transfer learning from a pre-trained model) on ""The Complete Works of HP Lovecraft"", and my goal is to fine tune it to look for certain relationships between words, eventually training it to use the same words and similar relationships to the original stories. The training goal would be this: let's say I break down the call of cthulhu, pt. 1: the horror in clay into what the primary subject is, who the characters are, what actions they performed, and what order they performed the actions in; I'd like for the trained model to match those milestones. what I'm *not* saying is that the story would match the original; rather, the syntax of the story would be the same. does this make sense? Is gpt-2 with huggingface transformers the best way to approach this, or is there some other library I could use? Thanks.",POS
87,pzgnu3,Text Classification - Sentiment Classifier without Training Data - Hugging Face NLP,https://www.youtube.com/watch?v=ljI7GoY5JgY,dulldata,LanguageTechnology,1970-01-01 00:00:01.633120872,,0,0,,NEU
88,pz3iwe,Please suggest some papers describing advantages of neural MT over statistical MT,https://www.reddit.com/r/LanguageTechnology/comments/pz3iwe/please_suggest_some_papers_describing_advantages/,antwash90,LanguageTechnology,1970-01-01 00:00:01.633076063,,0,1,I've seen people write about these in empirical manner - [https://www.tilde.com/about/news/316](https://www.tilde.com/about/news/316) as well as Philipp Koehn's textbooks on NLP. Are there some good research papers that summarize these findings and/or talk of this in a theoretical manner - as to what makes neural MT better than SMT?Thanks!,POS
89,pygz96,A New NLP book for Transformers!,https://www.reddit.com/r/LanguageTechnology/comments/pygz96/a_new_nlp_book_for_transformers/,According_Ad_7064,LanguageTechnology,1970-01-01 00:00:01.632997606,,40,11,"The Book Mastering Transformers is out!Our new book Mastering Transformers has been published. In this book, we discuss the transformers revolution:  Not only the introductory topics and the key aspects regarding Transformers, but also advanced topics.  You can build state-of-the-art models from scratch with advanced natural language processing techniques I'm the co-author :)[https://www.amazon.com/Mastering-Transformers-state-art-processing/dp/1801077657](https://www.amazon.com/Mastering-Transformers-state-art-processing/dp/1801077657)",POS
90,pz1be3,Download Wikipedia Text Dump?,https://www.reddit.com/r/LanguageTechnology/comments/pz1be3/download_wikipedia_text_dump/,IntraspeciesFever,LanguageTechnology,1970-01-01 00:00:01.633065706,,1,5,Does anyone know of any script which can be used to pull Wikipedia text data (preferrably the XML dump) for processing?,NEU
91,pynvzm,Data Analyst seeking to learn Text Analytics,https://www.reddit.com/r/LanguageTechnology/comments/pynvzm/data_analyst_seeking_to_learn_text_analytics/,Signal_Explorer8071,LanguageTechnology,1970-01-01 00:00:01.633020868,,9,5,"Hi Everyone, I used an off-the-shelf text and sentiment analysis tool in a previous job. I am an Analyst with SQL and Python (for Data Analysis) skills. I enjoyed text analysis and would like to apply it for use cases in my current job. \--Can you please advise if there are any free tools I may use. It looks like there are none! \--What should I learn in order to be able to use Python for text analysis. Thanks so much for your time!",POS
92,pyw8f3,Automated conversion of NL into formal logic.,https://www.reddit.com/r/LanguageTechnology/comments/pyw8f3/automated_conversion_of_nl_into_formal_logic/,Phyippa,LanguageTechnology,1970-01-01 00:00:01.633046881,,2,0,Hi. I'm wondering if anyone is familiar with any work/code that deals with translating natural language into formal logic in particular modal logic/epistemic logic. Thank you!,POS
93,pyxtes,[P]AI Biomedical Writer,/r/MachineLearning/comments/pyxjsg/pai_biomedical_writer/,wangyi_fudan,LanguageTechnology,1970-01-01 00:00:01.633052418,,1,0,,NEU
94,pyuhmk,Transformer NLP model similar to GPT-2 345M with nice up-to-date code base and multi-GPU training support?,https://www.reddit.com/r/LanguageTechnology/comments/pyuhmk/transformer_nlp_model_similar_to_gpt2_345m_with/,sabouleux,LanguageTechnology,1970-01-01 00:00:01.633040945,,1,0,"I am working on an interactive poetry project and I am searching for a model that would be easy to work with.I have worked on a previous project that involved a pre-trained version of the 345M GPT-2 model. It delivered great results for our use case. Larger models also worked great, but we opted for this smaller version since we had very limited compute available for inference — this was a personally-funded web-based application, and server time got expensive very quickly.I am working on a new project that both gives us the resources to train and fine-tune that model with our chosen datasets (cloud GPUs got really good and inexpensive in recent years!). We need to train it both in French and English. The datasets we have aren’t huge, they have respectively about 60,000 and 8,000 literary pieces, so using a gigantic model wouldn’t really be beneficial. We don’t have as much of a restriction on inference compute here, as long as it can run fine on a decent CPU at a few words per second.My initial thought was to simply train the same model, but the code base is somewhat old (not compatible past TensorFlow 1.15), which seems to cause issues with newer Ampere GPUs. It also doesn’t support multi-GPU training. I know there is a TensorFlow 2.0 fork, and I know I could spend a bit of time getting multi-GPU working by splitting batches, but time is short, and I figure there must have been a lot of NLP code written since then.So my question is: is there a nice, roughly similarly sized NLP model with a modern codebase you’d recommend for this?",POS
95,pysipu,How to customize UI,https://www.reddit.com/r/LanguageTechnology/comments/pysipu/how_to_customize_ui/,Desperate_Ad_7722,LanguageTechnology,1970-01-01 00:00:01.633034798,,1,0,"Hello, im planning on creating a naural language processing ui to help me with homework, find information on the web, make calculations, and more. My only question is, can how can I make the voice the UI responds in unique, and not the same as the first siri, or whatever default voice it uses.",POS
96,py23t7,Baidu AI Research Releases PLATO-XL: World’s First Dialogue Generation (NLP) Model Pre-Trained On 11 Billion Parameter,https://www.reddit.com/r/LanguageTechnology/comments/py23t7/baidu_ai_research_releases_platoxl_worlds_first/,techsucker,LanguageTechnology,1970-01-01 00:00:01.632941866,,19,1,"Artificial intelligence (AI) applications have a significant impact on our daily lives, making them easier. One of such applications is AI bots that are already proven effective in the automation of day-to-day tasks. These bots gather data and even imitate real-time human discussions, allowing humans to focus on more strategic activities.However, having clear, informative, and engaging conversations in the same manner that humans do is difficult for AI bots. Robots must build high-quality open-domain dialogue systems if they are to serve as emotional companions or intelligent assistants. As pre-training technology improves models’ ability to learn from vast amounts of unannotated data, mainstream research concentrates on making better use of massive data to improve open-domain discussion systems.# [4 Min Read](https://www.marktechpost.com/2021/09/29/baidu-ai-research-releases-plato-xl-worlds-first-dialogue-generation-nlp-model-pre-trained-on-11-billion-parameter/) | [Paper](https://arxiv.org/abs/2109.09519) | [BAIDU Blog](http://research.baidu.com/Blog/index-view?id=163)",POS
97,pychbp,Difference b/w Elasticsearch and Retriever,https://www.reddit.com/r/LanguageTechnology/comments/pychbp/difference_bw_elasticsearch_and_retriever/,MysticLimak,LanguageTechnology,1970-01-01 00:00:01.632976530,,2,1,"I'm in the process of documenting a build of an extractive QA pipeline using haystack and elasticsearch.  From my understanding, we first take the corpus and store the documents/contexts from the corpus into a sparse (ie. elasticsearchdocumentstore) or a dense documentstore (ie. FAISS).  Once encoded, the retriever (ie. sparse or dense passage retriever) will perform a similarity search to identity top-n of most relevant documents.  The reader will then predict where in each context the answer is located.  I'm confused where elasticsearch comes into the picture.  I read that elasticsearch is the back-end search engine but isn't the retriever doing the actual searching/similarity calculations.",NEG
98,py4zts,"Fine-tuning pre-trained word vectors to explore word ""meaning""",https://www.reddit.com/r/LanguageTechnology/comments/py4zts/finetuning_pretrained_word_vectors_to_explore/,MrPoopyLife,LanguageTechnology,1970-01-01 00:00:01.632950428,,3,3,"Hi everyone!   !Disclaimer: I am a beginner in NLP who finds word embeddings very fascinating!Say I am interested in the ""meaning"" of some word *w* within a certain corpus. I'd like to explore that meaning by training word embeddings and looking at the nearest neighbors of *w* in the vector space.   (1) First of all, would it make sense to do that?  Say, then, that I would like to see the ""meaning"" of *w* in a more general context. (2) Would it also make sense to fine-tune pre-trained word vectors on my corpus? I am wondering if then the meaning of *w* would shift towards something else, which could be an indication that the usage of *w* was sort of biased/different in my corpus.   (3) If all of the above is valid to explore, and it makes sense, could anyone point me to readings/resources to fine-tune pre-trained word vectors? I find plenty of explanations, papers and courses on what word embeddings are and how to generate them, but I can't easily find stuff for fine-tuning.  Thanks in advance :)",POS
99,pxro46,Looking for best way to do embedding search in production,https://www.reddit.com/r/LanguageTechnology/comments/pxro46/looking_for_best_way_to_do_embedding_search_in/,gtguide,LanguageTechnology,1970-01-01 00:00:01.632907159,,7,9,"Hi all,I came across with one problem of finding similar documents in a set of huge corpus. Looking for your help to figure out best possible solution.What I am looking for is, given a new document I want to retrieve similar documents based on the semantic similarities from a collection of documents (millions, billions in number)Currently I am looking at the pre-computation of all the documents in corpus and store it somehow(maybe elastic search). Now whenever a new document comes, calculate embedding and find similar documents (with some threshold). Now since documents are huge in number and for every new document I have to calculate similarity with all documents which is way too time taking. So looking for a way to reduce complexity and latency. (Results should be achieved in less than a second)Help me, if you guys know anything similar or how should I proceed with such problem.",POS
100,pxxkyd,"Release John Snow Labs Spark-NLP 3.3.0: New ALBERT, XLNet, RoBERTa, XLM-RoBERTa, and Longformer for Token Classification, 50x times faster to save models, new ways to discover pretrained models and pipelines, new state-of-the-art models, and lots more!",https://github.com/JohnSnowLabs/spark-nlp/releases/tag/3.3.0,dark-night-rises,LanguageTechnology,1970-01-01 00:00:01.632928784,,1,1,,NEU
101,pxn8zo,Google AI Introduces Translatotron 2 For Robust Direct Speech-To-Speech Translation,https://www.reddit.com/r/LanguageTechnology/comments/pxn8zo/google_ai_introduces_translatotron_2_for_robust/,techsucker,LanguageTechnology,1970-01-01 00:00:01.632887418,,2,0,"The Natural Language Processing (NLP) domain is experiencing remarkable growth in many areas, including search engines, machine translation, chatbots, home assistants and many more. One such application of S2ST (speech-to-speech translation) is breaking language barriers globally by allowing speakers of different languages to communicate. It is therefore extremely valuable to humanity in terms of science and cross-cultural exchange. Automatic S2ST systems are typically made up of a series of subsystems for speech recognition, machine translation, and speech synthesis. However, such cascade systems may experience longer latency, information loss (particularly paralinguistic and non-linguistic information), and compounding errors between subsystems.Google’s recent study presents the improved version of Translatotron, which significantly enhances performance. [Translatotron 2](https://arxiv.org/abs/2107.08661) employs a new way for transferring the voices of the source speakers to the translated speech. Even when the input speech involves numerous speakers speaking in turn, the updated technique to voice transference is successful while also decreasing the potential for misuse and better complying with our AI Principles. # [5 Min Read](https://www.marktechpost.com/2021/09/28/google-a-introduces-translatotron-2-for-robust-direct-speech-to-speech-translation/) | [Paper](https://arxiv.org/abs/2107.08661) | [Google AI Blog](https://ai.googleblog.com/2021/09/high-quality-robust-and-responsible.html)",POS
102,pxbkw1,Loss stuck. Model for speech-to-text system,https://www.reddit.com/r/LanguageTechnology/comments/pxbkw1/loss_stuck_model_for_speechtotext_system/,Abdulrahman_Adel,LanguageTechnology,1970-01-01 00:00:01.632850784,,6,12," I’m trying to build a speech-to-text system my data is (4 - 10 seconds audio wave files) and their transcription (preprocessing steps are char-level encoding to transcription and extract mel-Spectrograms from audio files). this is my model architecture is ( a 3 conv1d layers with positional encoding to the audio file - embedding and positional encoding to encoded transcription and then use those as input to transformer model and lastly a dense layer) the loss function is cross entropy and optimizer is Adam.the problem is that the loss is always stuck at some point it starts around 3.8 (I have 46 classes) and after some batches it decreases to (e.g. 2,8) and stuck their. it bounces around that value and never decrease again. I tried changing parameters of the model, I’ve changed the optimizer and learning rate always result the same problem.I don’t understand what I’m doing wrong [Training Loss](https://i.stack.imgur.com/1q8Jc.png)",NEG
103,px9ifn,Using NLP to parse and analyse cooking recipes.,https://www.reddit.com/r/LanguageTechnology/comments/px9ifn/using_nlp_to_parse_and_analyse_cooking_recipes/,JFSJoey,LanguageTechnology,1970-01-01 00:00:01.632844877,,8,3,"Hey everyone, I'm a intermediate programmer with an interest but no experience in Natural Language Processing and I was hoping to get some guidance.I'm trying to write a command-line program that takes plain text files of recipes and returns an analysis of potential typos in weight, volume, temperature, time, etc. For example, if a given recipe says to bake for 45 seconds instead of minutes.  I should also be able to query the recipe for things like ""well-cookedness"" where (given the previous example), the program would identify that the recipe produces 'uncooked' or 'undercooked' results. I was hoping to do all of the work in Python and I read that Python's default NLP library, the Natural Language Toolkit (NLTK) would be a good place to start.I am ready to learn everything as I go along but I'm hoping for guidance on the overall process of implementing such a project. Please forgive me if the following questions sound stupid 😅:* Is there an NLP library I should use instead of or in addition to Python's NLTK?* What recommended AI or NLP techniques should I research and implement for a program like this?* What would be the main stages of this program? From text analysis straight to querying data or are there some intermediate steps?Thank you for reading up to this point and for any advice!",POS
104,pwvj3s,OpenAI’s New Machine Learning Model Can Summarize Any Size Book with Human Feedback,https://www.reddit.com/r/LanguageTechnology/comments/pwvj3s/openais_new_machine_learning_model_can_summarize/,techsucker,LanguageTechnology,1970-01-01 00:00:01.632792589,,28,7,"OpenAI has developed a[ new model to study the alignment problem of machine learning](https://arxiv.org/pdf/2109.10862.pdf). This model can summarize books of any length by creating summaries of each chapter. Yes, you heard it right; OpenAI’s new machine learning model can summarize the entire book.The proposed machine learning model summarizes a small part of the book and then summarizes these summaries to obtain a higher-level overview. This research has been done as an empirical study on scaling correspondence problems which is usually tricky for AI algorithms because they require complex input text or numbers that have not yet been trained.# [3 Min Read](https://www.marktechpost.com/2021/09/27/openais-new-machine-learning-model-can-summarize-any-size-book-with-human-feedback/) | [Paper](https://arxiv.org/pdf/2109.10862.pdf) | [OpenAI Blog](https://openai.com/blog/summarizing-books/)",NEG
105,pwijwb,STS-B Glue,https://www.reddit.com/r/LanguageTechnology/comments/pwijwb/stsb_glue/,idkwhatever1337,LanguageTechnology,1970-01-01 00:00:01.632753798,,11,2,Hi guys has anyone used STS-B before (it’s one of the glue benchmark tests). I’m not really sure how to evaluate my model. The gold labels are human scores between 0-5 and correspond to how similar two sentences are. I have a model which returns vector representations of two sentences. I then compute the cosine similarity and scale the result to be between 0 and 5 by doing ((res+1)/2)*5 but that just seem wrong. Does anyone have any experience with this? Any pointers would be greatly appreciated!,POS
106,pwj8bi,BERT fine-tuning techniques,https://www.reddit.com/r/LanguageTechnology/comments/pwj8bi/bert_finetuning_techniques/,kastilyo,LanguageTechnology,1970-01-01 00:00:01.632755742,,8,2,"Hello everyone,I am currently in the process of fine-tuning BERT for a classification problem using a small dataset.I came across this article stepping through a tutorial on how to do so. https://www.analyticsvidhya.com/blog/2020/07/transfer-learning-for-nlp-fine-tuning-bert-for-text-classification/One area I was curious about in the article was the brief discussion in techniques. They discussed training the entire architecture, freeze some layers or freeze the entire architecture.Can anyone here help point me in a direction to learn more about each technique? More specifically, what the pros and cons?When to apply them in practice?And are these the only ones?Thank you!",POS
107,pwovgu,PLATO-XL: Exploring the Large-scale Pre-training of Dialogue Generation,https://www.reddit.com/r/LanguageTechnology/comments/pwovgu/platoxl_exploring_the_largescale_pretraining_of/,trcytony,LanguageTechnology,1970-01-01 00:00:01.632771883,,3,0,"Abstract: To explore the limit of dialogue generation pre-training, we present the models of PLATO-XL with up to 11 billion parameters, trained on both Chinese and English social media conversations. To train such large models, we adopt the architecture of a unified transformer with high computation and parameter efficiency. In addition, we carry out multi-party aware pre-training to better distinguish the characteristic information in social media conversations. With such designs, PLATO-XL successfully achieves superior performances as compared to other approaches in both Chinese and English chitchat. We further explore the capacity of PLATO-XL on other conversational tasks, such as knowledge grounded dialogue and task-oriented conversation. The experimental results indicate that PLATO-XL obtains state-of-the-art results across multiple conversational tasks, verifying its potential as a foundation model of conversational AI.Paper link: [https://arxiv.org/abs/2109.09519](https://arxiv.org/abs/2109.09519)",POS
108,pw9c7a,[R] Compressing Large-Scale Transformer-Based Models: A Case Study on BERT,https://www.reddit.com/r/LanguageTechnology/comments/pw9c7a/r_compressing_largescale_transformerbased_models/,prakharg24,LanguageTechnology,1970-01-01 00:00:01.632715497,,13,3,"Hi all,We have released a survey on current SOTA in BERT model compression. We do a thorough study of various components of BERT-like Transformer models, collect various compression methods in literature and finally provide our insights on future research directions. The paper was recently **published by TACL.**You can find the paper at ->[https://direct.mit.edu/tacl/article/doi/10.1162/tacl\_a\_00413/107387/Compressing-Large-Scale-Transformer-Based-Models-A](https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00413/107387/Compressing-Large-Scale-Transformer-Based-Models-A)Hopefully, this can help new NLP researchers get a better understanding of the field. We welcome your feedback.",POS
109,pwc0ao,Classify short sentences into 6 different classes using BERT pretrained model,https://www.reddit.com/r/LanguageTechnology/comments/pwc0ao/classify_short_sentences_into_6_different_classes/,gattaloukik321,LanguageTechnology,1970-01-01 00:00:01.632726856,,4,2,"How can I train the bert pretrained model with a custom dataset that I have in the .xlsx format? The training data has 2 columns, an input column and a class column.",NEU
110,pwdjuc,Information theory – why is the geometric distribution the best one for optimal communicative efficiency?,https://www.reddit.com/r/LanguageTechnology/comments/pwdjuc/information_theory_why_is_the_geometric/,il_signor_hyde,LanguageTechnology,1970-01-01 00:00:01.632735230,,2,2,"Hi there, I've recently stumbled upon a number of papers framing linguistic research in an information-theoretic way and have a few doubts I hope you can help me clear up. I tried to ask this same question over [math stackexchange](https://math.stackexchange.com/questions/4260182/information-theory-what-is-the-best-code-words-distribution-for-optimal-commun) and [r/statistics](https://www.reddit.com/r/statistics/comments/pvqspm/q_information_theory_best_codewords_distribution/), but the answers I got didn't really help me.My question has to do with something I came across in a few papers – [\[1\]](http://www.sfs.uni-tuebingen.de/~mramscar/papers/How-children-learn-to-communicate-ramscar.pdf)   [\[2\]](https://arxiv.org/pdf/2001.05292.pdf)   [\[3\]](https://arxiv.org/pdf/1904.03991.pdf) – namely the claim that the communicative efficiency of a code is maximized when code-words are exponentially distributed. Or better, when they follow a geometric distribution, which is the discrete form of the exponential (see [\[1\]](http://www.sfs.uni-tuebingen.de/~mramscar/papers/How-children-learn-to-communicate-ramscar.pdf) top of page 18; [\[2\]](https://arxiv.org/pdf/2001.05292.pdf) top of page 11). In one of the paper the author also mention that ""*geometric (exponential) distributions enable the average information in a set of code-words to be minimized (such that we ought to expect an optimal communicative code to employ geometric rather than Zipfian distributions)*"" (see [\[3\]](https://arxiv.org/pdf/1904.03991.pdf) top of page 20).As a linguistics student (with a poor stats background, shame on me) I struggle to come up with an intuitive explanation as to why this is the case.According to Wikipedia, the [exponential distribution](https://en.wikipedia.org/wiki/Maximum_entropy_probability_distribution#Positive_and_specified_mean:_the_exponential_distribution) and the [geometric distribution](https://en.wikipedia.org/wiki/Maximum_entropy_probability_distribution#Discrete_distributions_with_specified_mean) are the maximum entropy distributions among all continuous and discrete ones, respectively. However, if both distributions result in maximum entropy, that is maximum  uncertainty, how can they possibly be the best choice in terms of communicative efficiency of a code?Some of the contributions I got so far argue that maximum entropy allows you to ""express more"". To me, intuitively, this makes some sense but I cannot reconcile this with the claim that geometric distributions minimize the average information in a set of code-words.Any help or nudge in the right direction would be greatly appreciated!",POS
111,pw6plw,Best open source solution to automatically shorten product titles to 60 characters or fewer,https://www.reddit.com/r/LanguageTechnology/comments/pw6plw/best_open_source_solution_to_automatically/,JoZeHgS,LanguageTechnology,1970-01-01 00:00:01.632705548,,3,2,Hi everyone!I need an open source solution that could help me automatically shorten tens of thousands of product titles from 100-200 characters to 60 characters or fewer. Is such a miraculous solution available to the poor and uneducated like myself (or even to others more fortunate)?Thanks a lot!,POS
112,pw5k9l,"Struggling with understanding pytorch model code. I need to train this model, but I literally don’t understand how it works (haven’t worked with pytorch previously). Any tips or resources I can get on where I could start from the basics?",https://www.reddit.com/r/LanguageTechnology/comments/pw5k9l/struggling_with_understanding_pytorch_model_code/,hiworld12333,LanguageTechnology,1970-01-01 00:00:01.632701339,,3,6,"Title. I’ve just gotten a new position and my group threw a bunch of code (like, literally) and told me to train this model. Have no idea where to start. I’d like to start from the basics and learn more on pytorch training. First time doing this sort of work, used TF and keras before. Any resources on where I should start?Btw: people I am working with are PhDs and adults who have a ton of experience in NLP, and I’m a high schooler who has to learn all of this by myself in the next 3 days.",POS
113,pw2z4x,Gothenburg vs Uppsala Masters,https://www.reddit.com/r/LanguageTechnology/comments/pw2z4x/gothenburg_vs_uppsala_masters/,dmoses815,LanguageTechnology,1970-01-01 00:00:01.632692311,,3,5,"What’re the reputations like for these programs? I’m currently doing my undergrad in Linguistics and a minor in CS. I’m a junior so I’m trying to figure out some good options. I know Edinburg is good, what about some other schools?",POS
114,pvwdxw,Need a mentor for his/her guidance in my NLP project,https://www.reddit.com/r/LanguageTechnology/comments/pvwdxw/need_a_mentor_for_hisher_guidance_in_my_nlp/,_-CoffeE_,LanguageTechnology,1970-01-01 00:00:01.632671176,,6,9,"Hi community! I am in search for a mentor who can guide me on how to approach for a project I want to build. My project is aimed to build a NLP model which can take information about a certain topic/query from various sources and summarises the text in a more understandable manner. The key task is the model takes a query from user and uses Google's search results to extract text from the webpages, understand the semantics and provide a more summarised and understandable output for the searched topic. As I am new to this there might be some assumptions I am making wrong or arbitrary. I don't know how should I approach this problem neither I have worked upon an NLP project before but I can learn and work for it. If anyone can mentor me for this, it'll be great.Thanks in advance!",NEG
115,pvvor5,Indox - text summarization engine,https://www.reddit.com/r/LanguageTechnology/comments/pvvor5/indox_text_summarization_engine/,alexadar,LanguageTechnology,1970-01-01 00:00:01.632668984,,3,6,"Hi all!  I’ve developed a cutting-edge summarization engine and want to start a company that will provide AI services to customers. I dropped an article on medium [https://medium.com/@OlexanderKorenyuk/indox-summarizaton-engine-b2fc49864ddf](https://medium.com/@OlexanderKorenyuk/indox-summarizaton-engine-b2fc49864ddf) .   If you like, please, look at it, demo area on a website will be very appreciated for a feedback  Thanks!",POS
116,pvvyjc,Training or fine-tuning transformers on weighted sample data,https://www.reddit.com/r/LanguageTechnology/comments/pvvyjc/training_or_finetuning_transformers_on_weighted/,mm_maybe,LanguageTechnology,1970-01-01 00:00:01.632669816,,3,5,"Hi there, I am wondering if there is a way to use weights (e.g. upvotes/downvotes) into the fine-tuning of GPT-2 or a different NLP algorithm.  In other words, the higher the human rating given to a sample in the corpus, the more influence it should have on the fine-tuned model.  I apologize in advance if this is very basic functionality that I'm just not aware of!",POS
117,pvlf10,[Hiring] Looking for data scientists with NLP experience in USA,https://www.reddit.com/r/LanguageTechnology/comments/pvlf10/hiring_looking_for_data_scientists_with_nlp/,No_Formal_806,LanguageTechnology,1970-01-01 00:00:01.632625053,,7,0,"Hi all, My team is currently looking for data scientists with NLP experience. The role could potentially be remote from anywhere in the USA. Although the role would involve the usual data science suspects like EDA and ad hoc analysis, there would be a heavy NLP element to the role including custom NER modeling. Ideal candidate - have industrial data science experience and comfort with messy data. If anyone is interested, pls reach out to me.",POS
118,pveedg,"Would you say that creating data for relation extraction (RE) is ""harder"" than creating data for named entity recognition (NER)?",https://www.reddit.com/r/LanguageTechnology/comments/pveedg/would_you_say_that_creating_data_for_relation/,Seankala,LanguageTechnology,1970-01-01 00:00:01.632600125,,6,6,"Title is the question. Creating labeled data is expensive for _any_ subtask of machine learning, but I'm focused particularly on the two information extraction subtasks of RE and NER.I'm wondering if it's legitimate to say that ""creating data for RE is harder than that for NER"" since, well, I don't really have any concrete way to prove the difficulty.I came to wonder this because NER is largely seen by many as a task that's achieved a lot of progress and SoTA NER tools can be used out of the box without any horrendous error cases. Therefore it seems that creating silver standard data for NER is fairly simple (i.e., just use these tools or a SoTA neural model on unlabeled text), but for RE we have to go an extra step. What I mean is that we have to perform NER and then additionally annotate the relation between two entities.Could you say that creating data for RE is more difficult in this regard? Also, is there any research work out there that touches upon this subject? Thanks!",POS
119,putyjx,We are now publishing some downloadable NLP datasets from reddit posts and comments. First subreddits covered are /r/wallstreetbets (25K posts and 1 million comments) and /r/NoNewNormal (120k posts 2.5 million comments) for Aug 2021,https://socialgrep.com/datasets,xdotcommer,LanguageTechnology,1970-01-01 00:00:01.632522237,,29,10,,NEU
120,pujbzg,FAISS and the Index Factory - an intro to composite indexes for similarity search,https://www.reddit.com/r/LanguageTechnology/comments/pujbzg/faiss_and_the_index_factory_an_intro_to_composite/,jamescalam,LanguageTechnology,1970-01-01 00:00:01.632488682,,22,6,"Hi all - I put together [an article and videos](https://www.pinecone.io/learn/composite-indexes/) covering the composite indexes for vector similarity search and how we can implement them in Faiss.I've done a lot of articles/videos on faiss + vector similarity search recently and I think this has to be the most useful for building good indexes imo!I hope some of you find it useful, and let me know what you think/if you have questions!",POS
121,puv5nd,"Facebook AI Unveils Dynatask, A New Paradigm For Benchmarking AI, Enabling Custom NLP Tasks For AI Community",https://www.reddit.com/r/LanguageTechnology/comments/puv5nd/facebook_ai_unveils_dynatask_a_new_paradigm_for/,techsucker,LanguageTechnology,1970-01-01 00:00:01.632526646,,2,0,"Last year, Facebook AI launched [Dynabench ](https://ai.facebook.com/blog/dynabench-rethinking-ai-benchmarking/)as a first-of-its-kind platform that rethinks benchmarking in artificial intelligence. Now, they are introducing ‘Dynatask’, a new feature unlocking Dynabench’s full capabilities for the AI community.[Dynatask](https://ai.facebook.com/blog/dynatask-a-new-paradigm-of-ai-benchmarking-is-now-available-for-the-ai-community/) helps researchers identify weaknesses in NLP models by having human annotators interact with them naturally. Dynatask has developed a new artificial intelligence model benchmarking system that is more accurate and fair than traditional methods. Researchers will be able to utilize the strong capabilities of the Dynatask platform and can compare models on the dynamic leaderboard. This is not limited to just accuracy but includes a measurement approach of fairness, robustness, compute, and memory.When Dynabench was launched, it had four tasks: natural language inference, question answering, sentiment analysis, and hate speech detection. The Facebook AI research team has powered the multilingual translation challenge at Workshop for Machine Translations with its latest advances. Cumulatively these dynamic data collection efforts resulted in eight published papers and over 400K raw examples.# [5 Min Read](https://www.marktechpost.com/2021/09/24/facebook-ai-unveils-dynatask-a-new-paradigm-for-benchmarking-ai-enabling-custom-nlp-tasks-for-ai-community/) | [Facebook Blog](https://ai.facebook.com/blog/dynatask-a-new-paradigm-of-ai-benchmarking-is-now-available-for-the-ai-community/)&#x200B;https://reddit.com/link/puv5nd/video/m3lfbzn9ejp71/player",POS
122,puxtcr,How will machines understand people? That's how! The Folks’Talks understanding test.,https://www.reddit.com/r/LanguageTechnology/comments/puxtcr/how_will_machines_understand_people_thats_how_the/,FolksTalksGame,LanguageTechnology,1970-01-01 00:00:01.632536910,,0,0,[https://youtube.com/watch?v=mlJakDX\_93g&feature=share](https://youtube.com/watch?v=mlJakDX_93g&feature=share),NEU
123,puo2hu,A Guide to Building Your First NLP Application to Detect SPAM,https://blog.paperspace.com/nlp-spam-detection-application-with-scikitlearn-xgboost/,hellopaperspace,LanguageTechnology,1970-01-01 00:00:01.632503020,,7,2,,NEU
124,pumxao,Zero or Few Shot NER on Custom Entity,https://www.reddit.com/r/LanguageTechnology/comments/pumxao/zero_or_few_shot_ner_on_custom_entity/,cvkumar,LanguageTechnology,1970-01-01 00:00:01.632499661,,3,0,"Hey ya'll, I'm try to get a baseline for how good a zero or few shot approach would be on recognizing a custom entity (in this case job titles in german). I've been skimming through a few papers and see that it's certainly possible to do this, but I haven't seen any out-of-box type code that I could use to get a baseline on how effective it'll be. Anyone have any thought or ideas on how to approach this?",POS
125,pu66z3,"Fine-tuning BERT models, alternatives for the last layers?",https://www.reddit.com/r/LanguageTechnology/comments/pu66z3/finetuning_bert_models_alternatives_for_the_last/,nattmorker,LanguageTechnology,1970-01-01 00:00:01.632435749,,13,2,"I'm relatively new to the field of NLP, so excuse me if this is a trivial question.I'm fine-tuning a BERT model to do sentiment analysis, I have already succeeded. However, I find interesting that all tutorials and notebooks I found use the same layers after the BERT encoder, namely a dropout (sometimes) and a dense layer with the appropriate size for the task.It is common to use different architectures for the layers after the encoder, for example, two (or more) dense layers, etc.Thanks for any insight.",POS
126,pugio5,UBIAI,https://www.reddit.com/r/LanguageTechnology/comments/pugio5/ubiai/,UBIAI,LanguageTechnology,1970-01-01 00:00:01.632477362,,0,0,"Today, text annotation tools are one of the most prominent parts of machine learning. Research areas such as search engines, chatbots, sentiment analysis, and virtual assistants require text annotation tools for better training of machine learning models.The machine learning industry and AI research require a large amount of annotated data. High-quality annotated data is like a goldmine for them. However, finding and creating this enormous amount of annotated data can be an arduous task, and most of the time, expensive.Fortunately, text annotation tools can help annotate this enormous amount of data in a matter of time. These annotation tools help with named entity recognition annotation, entity extraction, sentiment analysis, relation annotation, document classification, and more.Find out more here: [https://ubiai.tools/](https://ubiai.tools/)",POS
127,pttzvk,Fine-tuning GPT-J: key takeaways,https://www.reddit.com/r/LanguageTechnology/comments/pttzvk/finetuning_gptj_key_takeaways/,juliensalinas,LanguageTechnology,1970-01-01 00:00:01.632399661,,25,0,"Hello all,We've spent quite some time benchmarking the best fine-tuning techniques for GPT-J at [NLP Cloud](https://nlpcloud.io?utm_source=reddit&utm_campaign=j431103c-ed8e-11eb-ba80-2242ac130007). Finding the best solution was not straightforward and we had to look at things like speed, server costs, ease of development, accuracy of the fine-tuned model... It took time but we ended up with a nice setup (and we are now officially proposing GPT-J fine-tuning + automatic deployment on our platform).Here are our key takeaways:* The best methodology seems to be the one from the Mesh Transformer Jax team: [https://github.com/kingoflolz/mesh-transformer-jax/blob/master/howto\_finetune.md](https://github.com/kingoflolz/mesh-transformer-jax/blob/master/howto_finetune.md)* Fine-tuning on GPU is not ideal. Even several GPUs used in parallel with Deepspeed can be very slow. We used 4 GPUs Tesla T4 in parallel, and it took 1h30 to only compute our first checkpoint (+ 80GB of RAM used...), for a training dataset made up of 20k examples. Maybe a GPU A100 would be worth a try.* Fine-tuning on TPU is very efficient but it takes a TPU v3 because TPUs v2 are running out of memory. It takes around 15mns, for a training dataset made up of 20k examples, which is really awesome.* The overall process is not straightforward as it takes several kind of conversions (converting the datasets to the right format, making a slim version of the model, converting the weights to Transformers...)In the end this is worth the effort, because combining fine-tuning and few-shot learning makes GPT-J very impressive and suited for all sorts of use cases. If you guys have different feedbacks about GPT-J fine-tuning, please don't hesitate to comment, I would love to have your opinion.Hope you found the above useful!",POS
128,ptv48z,Summarizing multiple documents into one summary,https://www.reddit.com/r/LanguageTechnology/comments/ptv48z/summarizing_multiple_documents_into_one_summary/,jman-007,LanguageTechnology,1970-01-01 00:00:01.632403196,,6,4,I have found lots of info on summarizing single documents. But what I am looking for is being able to take multiple documents on the same subject and generate one summary that encompasses several different source documents.  The next level of this for me would be to highlight the outlier info in the different documents.Has this been done? Maybe I am searching using the wrong terms to find the info...Any help is appreciated,POS
129,pty2mo,Currently looking for a research internship for my masters thesis,https://www.reddit.com/r/LanguageTechnology/comments/pty2mo/currently_looking_for_a_research_internship_for/,duffpaddy,LanguageTechnology,1970-01-01 00:00:01.632412025,,2,5,"I'm currently writing a letter to companies who will hopefully take me on and give me a project to work on.The problem is that I have no idea what I'm interested in because I'm interested in most things to do with NLP / machine learning. I feel like I should just say ""something something transformers, algorithms"". I feel like it's hard to be specific when I'm asking them to give me a project? Does anyone else have this issue?",POS
130,ptgfvf,Concatenate to LSTM models,https://www.reddit.com/r/LanguageTechnology/comments/ptgfvf/concatenate_to_lstm_models/,MarurSri,LanguageTechnology,1970-01-01 00:00:01.632345128,,5,6,"I'm fairly new to NLP and building a model that takes two sub-models and concatenates them. The dataset has two text input columns and the predictor variable has 3 classes. Below is the code I wrote:model1 = Sequential() model1.add(Embedding(MAX_NB_WORDS,EMBEDDING_DIM,input_length=X1.shape[1])) model1.add(SpatialDropout1D(0.2)) model1.add(LSTM(100,dropout=0.2,recurrent_dropout=0.2))# Shape <KerasTensor: shape=(None, 100) dtype=float32 (created by layer 'lstm_3')>model2 = Sequential() model2.add(Embedding(MAX_NB_WORDS,EMBEDDING_DIM,input_length=X2.shape[1])) model2.add(SpatialDropout1D(0.2)) model2.add(LSTM(100,dropout=0.2,recurrent_dropout=0.2))# Shape <KerasTensor: shape=(None, 100) dtype=float32 (created by layer 'lstm_4')>concat_layer = Concatenate()([model1.output, model2.output]) dense_layer = Dense(10, activation='relu')(concat_layer) output = Dense(3, activation='softmax')(dense_layer)input_1 = Input(shape=(MAX_LEN,)) input_2 = Input(shape=(MAX_LEN,))# I have set Max_LEN=250 # Both input_1 and input_2 are of shape TensorShape([None, 250])model = Model(inputs=[input_1, input_2], outputs=output)# When I run the model I get the below error:ValueError: Graph disconnected: cannot obtain value for tensor KerasTensor(type_spec=TensorSpec(shape=(None, 250), dtype=tf.float32, name='embedding_3_input'), name='embedding_3_input', description=""created by layer 'embedding_3_input'"") at layer ""embedding_3"". The following previous layers were accessed without issue: []What mistake am I making?",NEU
131,ptc0g4,Interpret 3d/2d shape from its text description,https://www.reddit.com/r/LanguageTechnology/comments/ptc0g4/interpret_3d2d_shape_from_its_text_description/,gattaloukik321,LanguageTechnology,1970-01-01 00:00:01.632330357,,3,0,"I want to make a model that takes a text input such  as ""Make a round ball and a pyramid for me please"" and gives an output ""sphere and cone"" since they are the 3d shapes that are refereed to in the sentence. Any idea I can achieve something like this? Any links that can help me with this task?",POS
132,pt2on7,Is there any white paper or research paper explaining the architecture of any NLP engine like Dialogflow or LUIS?,https://www.reddit.com/r/LanguageTechnology/comments/pt2on7/is_there_any_white_paper_or_research_paper/,sid8491,LanguageTechnology,1970-01-01 00:00:01.632297274,,13,6,"I tried to find on Google but couldn't find any research paper related to i design implementation of any NLP engine like Dialogflow,  LUIS etc.I would be really thankful if someone could provide.Basically I need to complete a POC for designing an NLP engine from scratch.",POS
133,pt6fnm,Pre processing text,https://www.reddit.com/r/LanguageTechnology/comments/pt6fnm/pre_processing_text/,Senior_Time_2928,LanguageTechnology,1970-01-01 00:00:01.632313508,,3,0,"I am trying to clean some text from html tags however I cannot manage to remove new lines and slashes. What am I missing?raw text:'Is there an easy way to get a list of my blogs that require re-tagging?**\[\\'<div class=""dm-section-hero--question\\\\\\\\\\\\\\\_\\\\\\\\\\\\\\\_body"">\\\\n                                <p>**Most of my blogs have migrated without a primary tag. I can work through them using the list from my profile page, but the further through the list I get the harder it is to keep track of those  I\*\*\\\\\\**'ve done and those I haven**\\\\\\\*\*'t . Is there an easy way to get a list of my blogs that need re-tagging? That would make the job a whole lot easier...**</p><p>**Steve.**</p>\\\\n                            </div>\\'\]**'what I do:    soup = BeautifulSoup(raw_text)    text = soup.get_text()    text = re.sub(r'[\ \n]{2,}', ' ', text)    text = re.sub(r'[\t\r\n]', '', text)    text = re.sub(r'\n', ' ', text)    text.replace(""\\n"", """")What I get:""Is there an easy way to get a list of my blogs that require re-tagging?**\['\\\\n** Most of my blogs have migrated without a primary tag. I can work through them using the list from my profile page, but the further through the list I get the harder it is to keep track of those  **I\\\\\\'ve** done and those I **haven\\\\\\'t** . Is there an easy way to get a list of my blogs that need re-tagging? That would make the job a whole lot easier...Steve.**\\\\n '\]**""What I want:""Is there an easy way to get a list of my blogs that require re-tagging? Most of my blogs have migrated without a primary tag. I can work through them using the list from my profile page, but the further through the list I get the harder it is to keep track of those I 've done and those I haven't . Is there an easy way to get a list of my blogs that need re-tagging? That would make the job a whole lot easier...Steve.""",POS
134,ptbqlk,Asking for Some Help Regarding a System to Help Facilitate Communication between a Deaf/Hard of Hearing Professor and Students in a Classroom Environment,/r/ECE/comments/ptbjas/asking_for_some_help_regarding_a_system_to_help/,JustATemporaryReddit,LanguageTechnology,1970-01-01 00:00:01.632329593,,1,0,,NEU
135,pssqci,Natural language processing course - Looking for feedback,https://www.reddit.com/r/LanguageTechnology/comments/pssqci/natural_language_processing_course_looking_for/,sb2nov,LanguageTechnology,1970-01-01 00:00:01.632260151,,9,20,"I’m Sourabh, I lead one of the core Tensorflow teams at Google Brain and worked on data products at Coursera with Andrew Ng. Kaushik Rangadurai, ML Engineer at Facebook and I are leading a live, cohort based course on NLP starting November 1st. [https://corise.com/course/natural-language-processing](https://corise.com/course/natural-language-processing).We wanted to share what we’ve learned in machine learning over the years. You can join the first run of the course (capped at about 30 students) below. If you’re open to giving feedback on the class on how we can do better, happy to give a discount.",POS
136,psilbc,NLP Bachelor's Thesis idea,https://www.reddit.com/r/LanguageTechnology/comments/psilbc/nlp_bachelors_thesis_idea/,caclo,LanguageTechnology,1970-01-01 00:00:01.632230412,,10,8,"Hi everyone, I want to write my bachelor's thesis about Natural Language Processing in the context of cyber security. Is there anyone familiar with the combination of these fields (Evaluation of Logs for example)? One idea coming to mind would be using NLP on nMap-Output.",POS
137,psvb0k,Recognition of Resume and onvoice documents,https://www.reddit.com/r/LanguageTechnology/comments/psvb0k/recognition_of_resume_and_onvoice_documents/,Beautiful-Ad-1928,LanguageTechnology,1970-01-01 00:00:01.632268331,,1,1,"Hello, i need helpI am asked in my internship to detect only invoice and resume documents from large amount of documents that contains numerous types.I am asked to build a model with NLP, so i should extract text from image or PDF than i begin the process of detection/classificationTo be honest, i don't know from where i can start, i find it difficult taskCan any one help me and put me in the road",POS
138,psm07i,Catogorize the Data- Topic Modelling algorithm,https://www.reddit.com/r/LanguageTechnology/comments/psm07i/catogorize_the_data_topic_modelling_algorithm/,Shiva_NLP,LanguageTechnology,1970-01-01 00:00:01.632240839,,0,1,"Team,I am new to NLP , there is a requirement asked for me to categorize the data, Data which i have is just one column data in excel and these are values are user daily search criteria on google browser.simply the search text done on google browser.&#x200B;I need to run a LDA (topic mapping algorithm ) on this data , so that the algorithm will classify them into some meaningful categories.Thanks,",POS
139,ps737m,NLP Research Ideas,https://www.reddit.com/r/LanguageTechnology/comments/ps737m/nlp_research_ideas/,Savings_Health780,LanguageTechnology,1970-01-01 00:00:01.632182730,,5,4,"Hey everyone, I'm currently very interested in NLP and think that it'd be great to do a research project about it. Is there any possible cool NLP research idea that has not been done before (because I think current papers in NLP have really done all the good stuff and ideas)? Probably will be doing on the project for 3-4 months.",POS
140,ps9rr2,Voice Cloning Software not limited to Text-to-Speech,https://www.reddit.com/r/LanguageTechnology/comments/ps9rr2/voice_cloning_software_not_limited_to_texttospeech/,xhannyah,LanguageTechnology,1970-01-01 00:00:01.632192239,,2,0,"Hello everyone,I have been searching for a voice cloning software or process that isn't limited to Text-To-Speech output; something that allows me to clone, for example, my neighbor's voice then use that to either edit pre-recorded audio or change the voice live.I have come across a lot of software that clones your voice and uses text-to-speech, but this is limited by the language that the text-to-speech supports (plus it sounds a bit robotic). The other side of the coin is software that allows you to edit out audio or mask it live, but this one is limited to pre-synthesized voices included in the software.I have been exploring some ideas about new content for my youtube/twitch channel, and was wondering if this was possible.",POS
141,prvt1r,Is huggingface pre-trained models on their site can be used for commercial use?,https://www.reddit.com/r/LanguageTechnology/comments/prvt1r/is_huggingface_pretrained_models_on_their_site/,meitar-qfwd,LanguageTechnology,1970-01-01 00:00:01.632147425,,12,4,Hi Everyone! new to this community.I've stumbbled upon huggingface.co pre-trained models and I'm not sure If I can freely use them.Can someone elaborate on the subject? Thanks!,POS
142,prx2de,A visual guide to filters in vector similarity search + latest advances in efficient filtering,https://www.reddit.com/r/LanguageTechnology/comments/prx2de/a_visual_guide_to_filters_in_vector_similarity/,jamescalam,LanguageTechnology,1970-01-01 00:00:01.632151370,,7,0,"I put together [an article and video covering metadata filtering](https://www.pinecone.io/learn/vector-search-filtering/) for vector similarity search - it had lots of very insightful input from some of the industry's leading experts at [Pinecone](https://www.pinecone.io/), so I think this should hopefully be very useful! Pinecone also let me write a little bit about a new approach to filtering they've developed, and also test it - and from what I've seen it is super impressive, I was a little blown away haha :)Let me know what you think and if you have any questions!",POS
143,ps78ng,Clinical trials LM?,https://www.reddit.com/r/LanguageTechnology/comments/ps78ng/clinical_trials_lm/,ms9696,LanguageTechnology,1970-01-01 00:00:01.632183237,,1,2,"Which pre-trained LMs will best suit applications for data from clinicaltrials.gov? (Like biobert, clinicalBert, UMLSBert, etc.)?",POS
144,prsnve,Low Resource Language Q&A model.,https://www.reddit.com/r/LanguageTechnology/comments/prsnve/low_resource_language_qa_model/,throwmeawaypls098,LanguageTechnology,1970-01-01 00:00:01.632136338,,4,4,"Hi all, I'm trying to implement a Q&A model for a low resource language ( Konkani), where can I begin? I'm using the AraBERT paper for some idea on how to go about it due to Konkani having a different script(Devnagri) as well. Will I have to first create a Q&A dataset? Or would scraped corpus of data be enough? I don't have too much experience in the field but I'm willing to put in the time and effort to try make this work. Any insights/advice would be greatly appreciated. Thank you!",POS
145,prsi83,After training a transformer for speech recognition task how to use it for inference if you have an untranscribed audio file?,https://www.reddit.com/r/LanguageTechnology/comments/prsi83/after_training_a_transformer_for_speech/,Abdulrahman_Adel,LanguageTechnology,1970-01-01 00:00:01.632135683,,3,2, I'm trying to train a model for speech-to-text system. but as I understand a Transformer takes as input the audio file and also the target transcription (shifted). so for prediction how could I transcribe if I only have an audio file(not transcribed)? [Transformer architecture](https://i.stack.imgur.com/hN1SQ.png),NEU
146,prw2jq,Which library/framework/model would you use to do this: fine-tune a pre-trained language model using a custom loss function that is computed using a second pre-trained language model?,https://www.reddit.com/r/LanguageTechnology/comments/prw2jq/which_libraryframeworkmodel_would_you_use_to_do/,NoRexTreX,LanguageTechnology,1970-01-01 00:00:01.632148283,,1,0,What would you use to do this:  Input text > \[model1\] > changed text > \[model2\] > reconstructed text. Then use the similarity between the input text and reconstructed text as the training signal for both models.,NEU
147,prg2yi,STraTA: Self Training with Task Augmentation for Better Few shot Learning (Paper Explained),https://youtu.be/0yriOQbNWmo,deeplearningperson,LanguageTechnology,1970-01-01 00:00:01.632084195,,10,0,,NEU
148,prhprb,Get multiple translation candidates,https://www.reddit.com/r/LanguageTechnology/comments/prhprb/get_multiple_translation_candidates/,LargeBrick7,LanguageTechnology,1970-01-01 00:00:01.632089503,,4,1,Does anyone know a (Statistical) Machine Translation model which is able to give me (for example) the best five translation candidates and not only one?,POS
149,pr8opz,AI-modified short story experiment,https://www.reddit.com/r/LanguageTechnology/comments/pr8opz/aimodified_short_story_experiment/,wnybom,LanguageTechnology,1970-01-01 00:00:01.632060775,,5,2,"Hi all, I am doing a PhD on personalisation and narratives, and for this, I created a user study where a short story of about 4000-5000 words has been modified by AI. The participants should also do a very short personality test and answer a few questions on what they thought of the story. I hope everyone who has a few minutes could try! It's at [https://cci.arts.ac.uk/\~wnybom/cloak.html](https://cci.arts.ac.uk/~wnybom/cloak.html)",POS
150,pr3u65,Textless NLP explainer,https://www.reddit.com/r/LanguageTechnology/comments/pr3u65/textless_nlp_explainer/,gauravc2796,LanguageTechnology,1970-01-01 00:00:01.632038605,,12,0," Facebook AI has just released Textless NLP architecture, where speech to speech translation, generation, and many tasks can be done without any dependency on text data.  Video explainer: [https://youtu.be/zw\_QjUptr5o](https://youtu.be/zw_QjUptr5o)",NEU
151,pr2k3x,Controllable Generation from Pre-trained Language Models via Inverse Prompting (Paper Summary),https://www.reddit.com/r/LanguageTechnology/comments/pr2k3x/controllable_generation_from_pretrained_language/,prakhar21,LanguageTechnology,1970-01-01 00:00:01.632031836,,9,1,"Open-ended Text Generation systems usually suffer from problems like Low relevance and Out-of-context generations. 👾This paper proposes an easy, intuitive yet effective method for Controllable Text Generation using Pre-trained Transformers Language Models. 🔥 Paper Summary: https://youtu.be/6RRdXnNd6XMPaper details in the comments!",POS
152,pr6axw,Efficient Documentation and Cataloguing,https://www.reddit.com/r/LanguageTechnology/comments/pr6axw/efficient_documentation_and_cataloguing/,bepnc13,LanguageTechnology,1970-01-01 00:00:01.632051316,,1,0,"Efficient Recording and Documentation MethodsHow can I simultaneously record and catalogue audio in the most efficient way? I work with an endangered language and need to be able to be able to record words or phrases and simultaneously organize the sound files and embed them into some kind of database (excel?).I'm also wanting to do the same thing with pdfs in which a single line of text can be given an audio files which the viewer can click on.I hope I'm being specific enough, let me know if I need to clarify. Thanks!",POS
153,pr610z,Any idea where I can find Spanish reasearch paper abstracts??,https://www.reddit.com/r/LanguageTechnology/comments/pr610z/any_idea_where_i_can_find_spanish_reasearch_paper/,Sensitive-Loss1522,LanguageTechnology,1970-01-01 00:00:01.632049979,,1,1,,NEU
154,pr0yjj,Is there any sorce code implemented for twitter trending topic detection?,https://www.reddit.com/r/LanguageTechnology/comments/pr0yjj/is_there_any_sorce_code_implemented_for_twitter/,NarrowTree,LanguageTechnology,1970-01-01 00:00:01.632024295,,2,5,,NEU
155,pqf58b,Will no code low code make entry level NLP jobs obsolete?,https://www.reddit.com/r/LanguageTechnology/comments/pqf58b/will_no_code_low_code_make_entry_level_nlp_jobs/,serratus_magnus,LanguageTechnology,1970-01-01 00:00:01.631936683,,13,9,"I've been in the process of learning NLP (learning Python and other NLP related topics deeper). A friend of mine, who knows that I want to change careers, has been nagging me on the topic for a while. He is more of an  entrepreneurial - start-up type.  He got into this low code no code scene and being the sales oriented person he is, he and a friend of him developed a web app. They sold some copies and made a couple of thousand dollars. This relative success made him bolder and he implied that I've been wasting my time on learning how to code when there are automated solutions to the problems I want to solve. Two of the articles he sent me are here:[Automated Sentiment Analysis via Zapier and Watson](https://tummala.medium.com/automate-twitter-sentiment-analysis-using-zapier-and-watson-no-coding-reqd-406aabd8ee66)[Effortless Cloud Based Natural Language Understanding](https://tummala.medium.com/effortless-cloud-based-natural-language-understanding-for-business-8342ee9cd1b2)I was aware of automated systems but his claim is platforms like Zapier, Bubble etc. will make many tasks easily done by non-professionals and only people with deeper understanding of computer science will do some deep level work. The implication is that I've been investing my time and resources in vain. I know the shortcoming of no code solutions yet I cannot foresee if they will replace certain hand coded NLP tasks in near future to the extent that any person can apply them to their businesses. I like the intellectual challenge of the computational handling of natural language, but my main aim is to get a job. Is he right? Will  it not be possible for a person with a proper CS degree to get a job as the tasks usually done by such people will be fully automated soon? I don't want to end up like that guy who invested a lot in learning Morse code and telegraph operation only to wake up to a telephone call.",POS
156,pqh0zf,[Need help/advise] Phrase/Token labeling,https://www.reddit.com/r/LanguageTechnology/comments/pqh0zf/need_helpadvise_phrasetoken_labeling/,hel00w0r1d,LanguageTechnology,1970-01-01 00:00:01.631945371,,3,8,"Looking for suggestions on how to define the following NLP problem and different ways in which it can be modeled to leverage machine learning. I believe there are multiple ways to model this problem.  Deep-learning-based suggestions also work as there is a good amount of data is available for training.Will evaluate different approaches for the given dataset. Please share relevant papers, blogs, or GitHub repos. Thanks!**Input**: Given a sentence S having words W1 to W10.S = W1 W2 W3 W4 W5 W6 W7 W8 W9 W10The sentence has some syntactic and semantic patterns, but it is not exactly freely written natural language but it's in English. These are words, can be punctuation&#x200B;**Output:** should be something like this.Label1 - W4Label2 - W3Label3 - \[W2 W1\] continuous // semantically related. Means words \[W2 W1\] in-order are assigned a Label3. **Also okay with solutions that don't output in-order.**Label4 - \[W6 W8\]Label5- W10Noise- W7, W9. Means words W7 and W9 independently are assigned a Label3.Label7- W5&#x200B;Need to solve the problem. Looking for research/thoughts on how this problem can be defined in different ways to exploit different patterns in the structure of sentences. Looking for similar tasks which are already defined in NLP such as token labeling, parsing which can be used.Would be really helpful to get the suggestions to the latest research on solving/defining this problem.",POS
157,pq69cn,Document IDs in the author2doc dictionary in gensim atmodel,https://www.reddit.com/r/LanguageTechnology/comments/pq69cn/document_ids_in_the_author2doc_dictionary_in/,Senior_Time_2928,LanguageTechnology,1970-01-01 00:00:01.631904763,,2,0," From the documentation:* **author2doc** (*dict of* *(str,* *list of int),* *optional*) – A dictionary where keys are the names of authors and values are lists of **document IDs** that the author contributes to.Do the document IDs have to be the order in which the document appears in the corpus? Although it is not explicitly written in the documentation (hence why I am asking), I assume so because otherwise I wouldn't understand how the model would link authors and documents.",POS
158,pq31r5,Poor man's IBM Watson Concepts Model,https://www.reddit.com/r/LanguageTechnology/comments/pq31r5/poor_mans_ibm_watson_concepts_model/,ydennisy,LanguageTechnology,1970-01-01 00:00:01.631894746,,4,2,"I am interested in how to replicate the IBM watson ""Concepts"" model, this from the outside seems a little similar to a topic model, however with all the topics labelled nicely!This seems a great way to ""tag"" articles, I would be interested in suggestions of both the model itself and also any datasets which could be leveraged to train such a model.My initial thoughts of a topic model or neural net for such a problem, but the fact that the output space is huge (60k+ concepts) I am not sure if that is the correct approach!Any ideas welcome :)[https://cloud.ibm.com/apidocs/natural-language-understanding#concepts](https://cloud.ibm.com/apidocs/natural-language-understanding#concepts)",POS
159,pq5kpk,What does quantum computing have to do with natural language processing? More than you might think.,https://www.youtube.com/watch?v=DjLrTZGN-mY,tmf1988,LanguageTechnology,1970-01-01 00:00:01.631902583,,0,0,,NEU
160,ppqzub,Is there some way to only apply the AI21 tokenizer to a bunch of texts? Have they released the tokenizer anywhere?,https://www.reddit.com/r/LanguageTechnology/comments/ppqzub/is_there_some_way_to_only_apply_the_ai21/,AggravatingNail7400,LanguageTechnology,1970-01-01 00:00:01.631843873,,4,0,,NEU
161,ppl134,Search-and-replace with correct grammatical case - does it exist?,https://www.reddit.com/r/LanguageTechnology/comments/ppl134/searchandreplace_with_correct_grammatical_case/,Coloneljesus,LanguageTechnology,1970-01-01 00:00:01.631822961,,10,8,"First off: I'm a professional software engineer but complete noob regarding NLP and linguistics, so please excuse if I explain myself badly or misuse any terms.Is there a tool, or even just a research effort, to enable a search-and-replace mechanism for single root words that results in gramatically correct results for (a) language(s) with grammatical cases? Or, to narrow it down and maybe make more clear what I mean: Is there a tool that can, in a German text replace all occurences of one noun with another noun in such a way that the grammatical case is still correct everywhere?For example, let's say I have this text:    ""Ich machte die Spende nur der Spende wegen.""Then I replace all occurences of ""Spende"" (or rather its root) with ""Beitrag"" and would like to get as output    ""Ich machte den Beitrag nur des Beitrags wegen.""For non-German speakers: The result is different from a normal search an replace because the genitive case of ""Beitrag"" is constructed differently than that of ""Spende"".Further questions: - Are there any specific or technical terms I should use to research this?- How hard of a problem is this? Is it something an NLP newbie like me could take on as a side project? (I imagine it being easy, as it's syntax-only and doesn't require semantic understanding of the text.)",POS
162,ppby4i,Want to reduce Data Annotation cost? GPT-3 can help (Research Paper Walkthrough),https://www.reddit.com/r/LanguageTechnology/comments/ppby4i/want_to_reduce_data_annotation_cost_gpt3_can_help/,prakhar21,LanguageTechnology,1970-01-01 00:00:01.631793704,,12,4,[Edited URL] This research paper from Microsoft proposes GPT-3 Language Model for Data Annotation in NLP. 🔥 The authors perform extensive experimentation to evaluate the quality of labels produced by GPT-3 and its cost-effectiveness when compared to human annotators. 🔥Watch paper summary at - https://youtu.be/CYD7HRIjhps,NEU
163,ppfr3k,MLM vs CLM for actual language modeling,https://www.reddit.com/r/LanguageTechnology/comments/ppfr3k/mlm_vs_clm_for_actual_language_modeling/,fasttosmile,LanguageTechnology,1970-01-01 00:00:01.631806838,,2,2,"I've got a situation where I need to just evaluate the probability of a sentence.I've started using a transformer LM and only recently realized that it was trained causally and I must also use it in that way for evaluation.But since I'm not interested in doing text generation, and I really just want to evaluate how ""real"" a sentence is, it seems to me that not being able to use future context is a real hindrance (for example the first word has no context then).So my question is does it make sense to do training with a MLM criterion, and then for evaluation to run the model for each word in the sentence while I masking that word (and then sum up the logprobs)? Obviously this is wrong from the perspective of traditional LMing where one decomposes P(X) = P(x_i|x_i-1 ...) etc.But I feel this would work a lot better.",NEG
164,pp3aqk,Any advanced NLP tutorial to refresh a veteran's memory?,https://www.reddit.com/r/LanguageTechnology/comments/pp3aqk/any_advanced_nlp_tutorial_to_refresh_a_veterans/,sim_inf,LanguageTechnology,1970-01-01 00:00:01.631754735,,11,14,"I am looking for an intermediate to advanced NLP tutorial (or crash course) to prepare for interviews. I have the background, I need to refresh my memory. I prefer to read though, rather than watching videos.I searched on the web but what I found was not useful. I found tutorials that cover things like: ""what is NLP"", ""what is stemming"", ""what is stopwords"", or ""what is parsing"".These are not useful for me.I am looking for a perhaps 50 to 150-page tutorial that rapidly goes through intermediate to advanced concepts and common methods. Things like: common methods for NER, common methods of POS tagging, types of parsing and their common methods, co-reference resolution, machine translation, types of embeddings, summarization, etc.Thank you!",POS
165,ppbe6h,The Role of Text Annotation Tools in Machine Learning,https://www.reddit.com/r/LanguageTechnology/comments/ppbe6h/the_role_of_text_annotation_tools_in_machine/,UBIAI,LanguageTechnology,1970-01-01 00:00:01.631791409,,1,0,"Today, text annotation tools are one of the most prominent parts of machine learning. Research areas such as search engines, chatbots, sentiment analysis, and virtual assistants require text annotation tools for better training of machine learning models.The machine learning industry and AI research require a large amount of annotated data. High-quality annotated data is like a goldmine for them. However, finding and creating this enormous amount of annotated data can be an arduous task, and most of the time, expensive.Fortunately, text annotation tools can help annotate this enormous amount of data in a matter of time. These annotation tools help with named entity recognition annotation, entity extraction, sentiment analysis, relation annotation, document classification, and more.Find out more here: [https://ubiai.tools/blog/article/introducing-ubiai-easy-to-use-text-annotation-for-nlp-applications](https://ubiai.tools/blog/article/introducing-ubiai-easy-to-use-text-annotation-for-nlp-applications)",POS
166,ppa3y9,"Can someone please explain to me the differences between train, dev and test datasets?",https://www.reddit.com/r/LanguageTechnology/comments/ppa3y9/can_someone_please_explain_to_me_the_differences/,DyNaCored,LanguageTechnology,1970-01-01 00:00:01.631785412,,2,6," Hey guys,The topic is Hate Speech Detection and I'm using this Kaggle competition:  [https://kaggle.com/c/detecting-insults-in-social-commentary](https://kaggle.com/c/detecting-insults-in-social-commentary)I'm also trying to solve this task in a python notebook (.ipynb) using the FARM  framework [https://farm.deepset.ai/](https://farm.deepset.ai/) and BERT model of huggingface [https://huggingface.co/bert-base-uncased](https://huggingface.co/bert-base-uncased)From what I'm understanding is that there are 3 relevant datasets for the purpose of binary text classification:* train.csv:  which consists of the columns ""Insult"", ""Date"" and ""Comment"", due to  the ""Insult"" column this dataset is labeled (0 = OTHER; 1 = TOXIC)* test\_with\_solutions.csv:  which consists of the columns ""Insult"", ""Date"", ""Comment"" and ""Usage"";  due to the ""Insult"" column this dataset is labeled as well* test.csv: which consists of the columns ""ID"", ""Date"" and ""Comment"";  this dataset is not labeledI'm  somewhat familiar with train\_and\_test\_split to create ""unseen"" data for  the classifier but what confuses me is that I'm thinking this dataset  is already splitted into train, dev and test datasets.My  assumption is that train.csv is the train dataset (such wow :p),  test\_with solutions.csv is my dev dataset and test.csv is my test  dataset, but how can I evaluate my classifier if there isn't a column  ""Insult"" with the labels? Is that due to the Kaggle Competition? If it helps, I can also post my code :)Can someone help me?Best regards,Cored",POS
167,pp37fq,rake-nltk 1.0.6 released. Comes with the flexibility to choose your own sentence and word tokenizers.,https://github.com/csurfer/rake-nltk,c5urf3r,LanguageTechnology,1970-01-01 00:00:01.631754396,,3,0,,NEU
168,pp4hvt,German language sentiment classification - NLP Deep Learning,https://www.reddit.com/r/LanguageTechnology/comments/pp4hvt/german_language_sentiment_classification_nlp_deep/,grid_world,LanguageTechnology,1970-01-01 00:00:01.631759138,,3,2,I  am trying to build a sentiment classification (hate speech) for German  language using NLP + Deep Learning. Any code tutorial? I found lots of  research papers but few code implementations.,NEU
169,powhqf,"AI21 Offers Instant Access To Its NLP Models, Jurassic-1 Large and Jurassic-1 Jumbo via AI21 Studio",https://www.reddit.com/r/LanguageTechnology/comments/powhqf/ai21_offers_instant_access_to_its_nlp_models/,techsucker,LanguageTechnology,1970-01-01 00:00:01.631732263,,10,0,"[AI21](https://www.ai21.com/), An Israeli AI company specializing in Natural Language Processing (NLP), has recently launched two big NLP models, [Jurassic-1 Large ](https://www.businesswire.com/news/home/20210811005033/en/AI21-Labs-Makes-Language-AI-Applications-Accessible-to-Broader-Audience)and [Jurassic-1 Jumbo](https://www.businesswire.com/news/home/20210811005033/en/AI21-Labs-Makes-Language-AI-Applications-Accessible-to-Broader-Audience), through an interactive web UI dubbed AI21 Studio. Unlike OpenAI, which has a limited beta, AI21 makes its models available for everyone to try out – there is no waiting list.[As per the researcher](https://www.businesswire.com/news/home/20210811005033/en/AI21-Labs-Makes-Language-AI-Applications-Accessible-to-Broader-Audience), “Using AI21 Studio, businesses can take advantage of text-based AI in the same way that Amazon Web Services makes cloud computing available”NLP is an area of computer science that aims to design algorithms that can process and generate written natural language. Language Models are systems that can intake text and generate likely continuations. Language models are almost exclusively produced at big tech AI labs, out of reach of a wider developer audience, because training and deploying them into production generally requires expensive computational resources and highly sought-after AI engineers. As a result, language models are almost exclusively produced at big tech AI labs, out of reach of a wider developer audience.# [4 Min Read](https://www.marktechpost.com/2021/09/15/ai21-offers-instant-access-to-its-nlp-models-jurassic-1-large-and-jurassic-1-jumbo-via-ai21-studio/) | [Paper](https://uploads-ssl.webflow.com/60fd4503684b466578c0d307/61138924626a6981ee09caf6_jurassic_tech_paper.pdf) | [Get Instant Access](https://www.ai21.com/studio)&#x200B;https://reddit.com/link/powhqf/video/vyoejsy8spn71/player",POS
170,pp5hgn,What is the difference between weak supervision and distant supervision? Is it just me or is there no clear-cut definition?,/r/MachineLearning/comments/pp5h4b/d_what_is_the_difference_between_weak_supervision/,Seankala,LanguageTechnology,1970-01-01 00:00:01.631762934,,0,0,,NEU
171,pomrxd,An NLP Approach to Exaggeration Detection in Science Journalism,https://www.unite.ai/an-nlp-approach-to-exaggeration-detection-in-science-journalism/,Symbiot10000,LanguageTechnology,1970-01-01 00:00:01.631698335,,11,1,,NEU
172,pp57t2,Sentiment Analysis using Python and Deep Learning (DistilBERT) in 3 lines of code,https://www.youtube.com/watch?v=GRH78O2FLHc,Pragyanbo,LanguageTechnology,1970-01-01 00:00:01.631761888,,0,0,,NEU
173,poyqk9,Fine-tune MT5,https://www.reddit.com/r/LanguageTechnology/comments/poyqk9/finetune_mt5/,LargeBrick7,LanguageTechnology,1970-01-01 00:00:01.631739190,,1,1,How do I fine-tune a MT5 model for generating german paraphrases? I have enough datasets but I can't find a working script to fine-tune a MT5 model :(,NEG
174,pot30f,NLP Career prospects?,https://www.reddit.com/r/LanguageTechnology/comments/pot30f/nlp_career_prospects/,apkswift,LanguageTechnology,1970-01-01 00:00:01.631721906,,1,5,"Anyone here work in NLP and can speak to its career prospects in the US (California/Seattle to be specific)? I vaguely remember reading a comment to a post here that was discouraging and explained that most of NLP was an academic field as it had been absorbed into automated language tech and AI in industry (I can't find the post but if I do, I'll link it here).I'm a mobile/compiler engineer who will be starting a CSMS (machine learning) next fall and have been looking around at the various niches.",NEG
175,pohbvb,"My group is looking into tokenizers. Which tokenizers are definitely worth looking into? Looking to find the most effective ones. Right now have done BERT, GPT-2, XLNet. Also, is there any difference between DistilBERT Tokenizer and BERT Tokenizer?",https://www.reddit.com/r/LanguageTechnology/comments/pohbvb/my_group_is_looking_into_tokenizers_which/,AggravatingNail7400,LanguageTechnology,1970-01-01 00:00:01.631673454,,8,1,Thanks!,POS
176,po3joj,Bert WordPiece tokenizer tutorial,https://www.reddit.com/r/LanguageTechnology/comments/po3joj/bert_wordpiece_tokenizer_tutorial/,jamescalam,LanguageTechnology,1970-01-01 00:00:01.631628622,,6,2,"Hi, I put together an [article](https://towardsdatascience.com/how-to-build-a-wordpiece-tokenizer-for-bert-f505d97dddbb) and [video](https://youtu.be/cR4qMSIvX28) covering the build steps for a Bert WordPiece tokenizer - I wasn't able to find a guide on this anywhere (the best I could find was BPE tokenizers for Roberta), so I figured it could be useful!Let me know what you think/ if you have Qs - thanks all! :)*(If the article link shows the Medium paywall you can use [this link](https://towardsdatascience.com/how-to-build-a-wordpiece-tokenizer-for-bert-f505d97dddbb?sk=eea06e01c9faecd939e10589e9de1291) for free access)*",POS
177,po6mei,Are Zero-Shot Text Classification Transformer Models the Key to Better Chatbots?,https://www.vennify.ai/zero-shot-transformer-models-chatbots/,VennifyAI,LanguageTechnology,1970-01-01 00:00:01.631638140,,3,0,,NEU
178,pnwg3s,"Anyone heard of A21I? It's a tokenizer trained on a larger set of data, but I can't find it publicly available.",https://www.reddit.com/r/LanguageTechnology/comments/pnwg3s/anyone_heard_of_a21i_its_a_tokenizer_trained_on_a/,AggravatingNail7400,LanguageTechnology,1970-01-01 00:00:01.631596436,,5,7,"Feel free to say you haven't heard of it. Just wanted to know if this is available somewhere, thanks!",POS
179,pnt8kk,Text Classification with Naive Bayes using Julia,https://www.youtube.com/watch?v=oHaeVuIhYoc&list=PLe1T0uBrDrfOLQlomF_4AxHa4LX0wsCXa&index=49,mindaslab,LanguageTechnology,1970-01-01 00:00:01.631583844,,6,0,,NEU
180,pneq54,"[P] Rubrix: Open-source Python framework for NLP data annotation, exploration, and monitoring",/r/MachineLearning/comments/pnel9y/p_rubrix_opensource_python_framework_for_nlp_data/,dvilasuero,LanguageTechnology,1970-01-01 00:00:01.631537317,,11,0,,NEU
181,pntk2s,amazon uk 'language engineer' : compensation?,https://www.reddit.com/r/LanguageTechnology/comments/pntk2s/amazon_uk_language_engineer_compensation/,Wild_Air_9451,LanguageTechnology,1970-01-01 00:00:01.631584999,,1,5,what is a realistic salary range for amazon uk 'language engineer' position ?how does it compare to software engineers?(i have a phd in releated field and 2.5 years of experience.),POS
182,pn8u2b,Help needed NLP learning,https://www.reddit.com/r/LanguageTechnology/comments/pn8u2b/help_needed_nlp_learning/,d0r1h,LanguageTechnology,1970-01-01 00:00:01.631509252,,4,3,"Hello everyone, I am putting this query here to ask a favor from you guys, I want practical understanding in NLP, so if there is any NLP book/doc/lectures that covers Practical NLP and also with the updated techniques, please suggest me.note: Please suggest only if you done that or having a good understanding of that (don't just google and put it here, I need a human perspective)Thanx",POS
183,pn6au1,NLP Data Reduction With Similarity Detection,https://www.reddit.com/r/LanguageTechnology/comments/pn6au1/nlp_data_reduction_with_similarity_detection/,pyMad,LanguageTechnology,1970-01-01 00:00:01.631499195,,5,14,"TLDR: I'm trying to use natural language processing to label transaction data (topic modeling).   The problem is I have way too much data, a lot which are very similar.For Example:1. ""Wells Fargo MTG PYMT #2434""2. ""Wells Fargo MTg PYMT #4235435""What are some efficient techniques to group like texts/transactions?The long and to expand on this:Say I have a myriad of transaction descriptions; below is an example of what I want the final output to look like.|**Transaction Description**|Label||:-|:-||McDonalds Debit #2343|Restaurant||Wells Fargo Pymt|Bank||Nordstrom San Francisco #54525|Retail||Macy's  In-Store Purchase|Retail|&#x200B;The problem (a good problem) is I have a lot of data.In an attempt to reduce the data, I'm wondering if it's common to apply some sort of training sample reduction techniques. My thoughts are using string similarity algorithms (Jaro, Jaccard, Levenshtein, etc...) - if the strings are say 90% a match to one another, assume it's the same transaction. But this would be computationally expensive.&#x200B;What are efficient methods to group like texts and only take a fraction of them? Concretely, let's assume we have the following transactions:&#x200B;**Transaction**Amazon Txn #1234Amazon Txn #434343Amazon Txn #445454Chase Bank Loan Pymnt #12Chase Bank Loan Pymnt #1243Wells Fargo Bank Debit -50Wells Fargo Bank Debit -3131SStarbucks Main StStarbucks Second StStarbucks San Francisco&#x200B;Albeit we have \~10 total transactions, in reality, we have 4 distinct transactions in the above example; Amazon, Wells Fargo, Chase, Starbucks. So instead of bringing in all transactions maybe only bring in 1,000 for each type?Maybe I'm thinking about this completely wrong and naively, but any help would be greatly appreciated.",POS
184,pmrcgk,NLP in etymology,https://www.reddit.com/r/LanguageTechnology/comments/pmrcgk/nlp_in_etymology/,Hefty_Raisin_1473,LanguageTechnology,1970-01-01 00:00:01.631447719,,10,13,Are you familiar with any work that aimed to study etymology by means of NLP?,NEU
185,pmtbqp,Sentiment analysis and studying content maturity?,https://www.reddit.com/r/LanguageTechnology/comments/pmtbqp/sentiment_analysis_and_studying_content_maturity/,funkykookaburra,LanguageTechnology,1970-01-01 00:00:01.631455712,,1,0,"Let's say I had two texts written by the same author, set in the same universe. But one is commonly seen as written for children, and the other is seen as a mature story. I.e., the second text deals with more violence, warfare, and has women-men relationships (I wish to include all women-men relationships, not just sexual ones. The first text itself has no women, only men.).So is there some solid way to computationally count these instances up? Is there some way of telling R (I like R alot) to look for instances of violence, warfare, and relationships, and compare the statistics between the two texts?Edit: should clarify, I am comfortable working with R and quanteda at the moment",NEG
186,pmpxwj,Language Translation using Hugging Face and Python in 3 lines of code,https://www.youtube.com/watch?v=724I8e66pTc,Pragyanbo,LanguageTechnology,1970-01-01 00:00:01.631440664,,0,0,,NEU
187,pmdtud,Pre-Training Bert on a Laptop,https://www.reddit.com/r/LanguageTechnology/comments/pmdtud/pretraining_bert_on_a_laptop/,idkwhatever1337,LanguageTechnology,1970-01-01 00:00:01.631388735,,2,2,"Hey guys so I’m working on a model that can hopefully achieve comparable (though somewhat worse) GLUE scores to BERT, but it’s readily trainable on a laptop cpu as it has far far fewer parameters. As a comparison how long would it take to pre-train BERT on e.g. an MacBook Air as I’d like to include that as a motivation for the research. If anyone has any resources or papers that might indicate this it would be greatly appreciated!",POS
188,plr5xr,Classify if an input is a command or greet,https://www.reddit.com/r/LanguageTechnology/comments/plr5xr/classify_if_an_input_is_a_command_or_greet/,gattaloukik321,LanguageTechnology,1970-01-01 00:00:01.631299868,,5,3,"I am working on a bot, this bot should converse with the user as well as execute some software level activities. How can i identify if the input is a greet or a command? Like how does ok google identify a difference in input such as “hey” and “ play a song on spotify”",POS
189,pln2q3,"Anyone have experience with semantic role labeling annotation, especially in PropBank framework?",https://www.reddit.com/r/LanguageTechnology/comments/pln2q3/anyone_have_experience_with_semantic_role/,squirreltalk,LanguageTechnology,1970-01-01 00:00:01.631287013,,5,2,"My lab is trying to develop and evaluate some domain-specific SRL systems, and we're wrestling with exactly how we want to approach evaluating them. One approach is to generate gold standard PropBank annotations, but this seems very, very arduous to me. Another is to just take systems' predictions and judge them as right or wrong (which sort of implicitly generates gold standards when the predictions are right), but this is unconventional and so might not go down well with reviewers, and also isn't so useful for supervising a new SRL system.If anyone has experience with this and could chat (here, DM's, email, zoom, whatever) I'd be extremely grateful!",NEG
190,plak3i,"Facebook AI Introduces GSLM (Generative Spoken Language Model), A Textless NLP Model That Breaks Free Completely of The Dependence on Text for Training",https://www.reddit.com/r/LanguageTechnology/comments/plak3i/facebook_ai_introduces_gslm_generative_spoken/,techsucker,LanguageTechnology,1970-01-01 00:00:01.631234978,,49,3,"The recent advancements in text-based language models, such as BERT, RoBERTa, and GPT-3, have been extremely impressive. Because they can generate realistically written words from a given input, these models can be utilized for various natural language processing applications, including sentiment analysis translation information retrieval inferences summarization, among others using only a few labels or examples (e.g., BART and XLM R). However, these applications have a major limitation: the models are only suitable for languages with very large text data sets.Facebook AI has introduced the first high-performance NLP model, called Generative Spoken Language Model (GSLM), which leverages state-of-the-art representation learning to work with raw audio signals without labels or text. This can lead to a new era of textless applications for any language spoken on earth, even those without significant text data sets. By using GSLM, you can develop NLP models that incorporate the full range of expressivity found in spoken language.# [4 Min Read](https://www.marktechpost.com/2021/09/09/facebook-ai-introduces-gslm-generative-spoken-language-model-a-textless-nlp-model-that-breaks-free-completely-of-the-dependence-on-text-for-training/) | [GLSM Paper](https://arxiv.org/abs/2102.01192?) | [Expressive Resynthesis Paper](https://arxiv.org/abs/2104.00355) | [Prosody-Aware GSLM Paper](https://arxiv.org/abs/2109.03264?) | [Code and Pretrained Models](https://github.com/pytorch/fairseq/tree/master/examples/textless_nlp) | [Facebook Blog](https://ai.facebook.com/blog/textless-nlp-generating-expressive-speech-from-raw-audio)",POS
191,plij7e,Getting NLP to Challenge Misinformed Questions,https://www.unite.ai/getting-nlp-to-challenge-misinformed-questions/,Symbiot10000,LanguageTechnology,1970-01-01 00:00:01.631270426,,2,0,,NEU
192,plig1x,Where can we find problems related to English language phenomena?,https://www.reddit.com/r/LanguageTechnology/comments/plig1x/where_can_we_find_problems_related_to_english/,SnooHabits4550,LanguageTechnology,1970-01-01 00:00:01.631270014,,1,0,"Where do I find questions such as below:1. ""The number of the morphemes in the word “uncommunicativeness” is \_\_\_\_\_""2. Consider the grammatically correct sentence ""Buffalo buffalo Buffalo buffalo buffalo buffalo Buffalo buffalo"". How many NN tags are there for this sentence?I there any university course / online site which discusses such questions and their answers? I want to learn my concepts of English language phenomena/linguistics.",POS
193,pl0byt,Sentiment Analysis in Python with NLTK. 10 Videos ~ 1hour,https://www.youtube.com/playlist?list=PLhTjy8cBISEoOtB5_nwykvB9wfEDscuEo,attreya12,LanguageTechnology,1970-01-01 00:00:01.631202253,,9,0,,NEU
194,pklq2g,"Salesforce Open-Sources ‘CodeT5’, A Machine Learning Model That Understands and Generates Code in Real Time",https://www.reddit.com/r/LanguageTechnology/comments/pklq2g/salesforce_opensources_codet5_a_machine_learning/,techsucker,LanguageTechnology,1970-01-01 00:00:01.631143440,,16,0,"AI-powered coding tools, which use machine learning algorithms to generate code based on input data, have attracted increasing attention. In theory, these systems can reduce the time spent writing codes as well as computational and operational costs with minimal errors in output. However, current coding pre-training systems have many challenges. These methods heavily rely on either an encoder-only model similar to BERT or a decoder-only model like GPT. Either way, it is suboptimal for generation and understanding tasks. As an example, CodeBERT needs an additional decoder when used for tasks like code summarization. Apart from the above issue, most current methods adopt the conventional NLP pre-training techniques on source code by considering it a sequence of tokens like in natural language (NL). This largely ignores the rich structural information present in programming languages, which is vital to comprehend its semantics fully.# [3 Min Read](https://www.marktechpost.com/2021/09/08/salesforce-open-sources-codet5-a-machine-learning-model-that-understands-and-generates-code-in-real-time/) | [Paper](https://arxiv.org/pdf/2109.00859.pdf) | [Code](https://github.com/salesforce/CodeT5)| [Salesforce Blog](https://blog.einstein.ai/codet5/)",POS
195,pkl55x,"Named-Entity Recognition of Long Texts Using HuggingFace's ""ner"" Pipeline",https://www.reddit.com/r/LanguageTechnology/comments/pkl55x/namedentity_recognition_of_long_texts_using/,Revolutionary-Ad-65,LanguageTechnology,1970-01-01 00:00:01.631141440,,6,8,"I'm trying to fine-tune BERT to do named-entity recognition (i.e. token classification with some extra steps). Most of my documents are longer than BERT's 512-token max length, so I can't evaluate the whole doc in one go.In theory, I think what I want to do is have a sliding window that averages the logits for the overlapping sections. I am not sure how to accomplish this using [TokenClassificationPipeline](https://huggingface.co/transformers/main_classes/pipelines.html#transformers.TokenClassificationPipeline) ([source](https://huggingface.co/transformers/_modules/transformers/pipelines/token_classification.html#TokenClassificationPipeline)), which seems to automatically [truncate](https://github.com/huggingface/transformers/blob/c37573806ab3526dd805c49cbe2489ad4d68a9d7/src/transformers/pipelines/token_classification.py#L213) the input text to the model's max length.Anyone know an easy way to accomplish this? Or should I make a feature request to HuggingFace? 3rd option?",POS
196,pkjtvy,Webinar: Search Query Expansion Using NLP and Machine Learning,https://www.reddit.com/r/LanguageTechnology/comments/pkjtvy/webinar_search_query_expansion_using_nlp_and/,thefirstsage,LanguageTechnology,1970-01-01 00:00:01.631137198,,7,0,Webinar about addressing data specific search engine query expansion using client side knowledge graph creation and semantic similarity: [https://www.youtube.com/watch?v=2GR5WlWz\_pQ](https://www.youtube.com/watch?v=2GR5WlWz_pQ),POS
197,pkkud3,Project Help: Abstractive Text Summarisation,https://www.reddit.com/r/LanguageTechnology/comments/pkkud3/project_help_abstractive_text_summarisation/,kav_yay,LanguageTechnology,1970-01-01 00:00:01.631140431,,3,2,How should one approach abstractive text summarisation using NLG ? I’m just starting out in the field of NLP and this topic seemed interesting to me. Could you please suggest some resources and the topics I need to be through with before starting out with the problem statement ?,POS
198,pk7j1k,BERTopic: Topic Modeling using Transformers in NLP,https://www.reddit.com/r/LanguageTechnology/comments/pk7j1k/bertopic_topic_modeling_using_transformers_in_nlp/,prakhar21,LanguageTechnology,1970-01-01 00:00:01.631095405,,25,1,BERTopic is a topic modeling technique that leverages BERT embeddings and a class-based TF-IDF to create dense clusters allowing for easily interpretable topics whilst keeping important words in the topic descriptions. 🔥 Watch the algorithm/library quickie here https://youtu.be/Lg7ZQj4dYKs,POS
199,pkmbu5,Learn word embeddings using AWS Sagemaker BlazingText,https://www.shrikar.com/blog/aws-blazingtext-word-embeddings,canhelp,LanguageTechnology,1970-01-01 00:00:01.631145588,,1,0,,NEU
200,pkl29c,Suggest Research papers for Abstractive Text Summarisation on multi-corpus data,https://www.reddit.com/r/LanguageTechnology/comments/pkl29c/suggest_research_papers_for_abstractive_text/,kav_yay,LanguageTechnology,1970-01-01 00:00:01.631141168,,1,0,,NEU
201,pkdyk1,Is there a preexisting TensorFlow model that can parse the text of an image that shows a list of ingredients?,https://www.reddit.com/r/LanguageTechnology/comments/pkdyk1/is_there_a_preexisting_tensorflow_model_that_can/,techsavvynerd91,LanguageTechnology,1970-01-01 00:00:01.631118641,,1,1,So there's a feature for my Android/iOS application where the user can take a picture of a list of ingredients from a recipe and it will parse the text where the app can read the contents of each line from the picture. Any preexisting models that I should use to accomplish this? I'm looking at the [TensforFlow Hub](https://tfhub.dev/) and I'm not sure which model to use for this.,POS
202,pk8pka,Detect words based of context,https://www.reddit.com/r/LanguageTechnology/comments/pk8pka/detect_words_based_of_context/,LouTr0n,LanguageTechnology,1970-01-01 00:00:01.631100885,,1,5,"HiI am sort of familiar with typical NLP classifiers (for example a classic Twitter sentiment classifier). However I am working on a project where I want to detect programming languages and framework names in a text. I can label some examples myself for training, but I would like to detect way more with ML.An example of text could be: ""In order to develop this you could use Javascript"". A combination of type of word (noun, random capital letter, verbs used before etc) should be some giveaways of identifying the word. I don't mind for some false positives or negatives.Now my question is, how can I feed a model training data where I just label one or more words in a block.Hope my question is clear.Thanks in advance.",POS
203,pk4dl7,"COLING 2022 will be held in Gyeongju, South Korea.",https://www.reddit.com/r/LanguageTechnology/comments/pk4dl7/coling_2022_will_be_held_in_gyeongju_south_korea/,Chanjun_Park,LanguageTechnology,1970-01-01 00:00:01.631079111,,3,0,"COLING 2022 will be held in Gyeongju, South Korea.&#x200B;The conference will be held from October 12 to 17, 2022.An Official SNS page is currently being opened, and we share LinkedIn and Reddit pages.Facebook page and Twitter will be added in the future.&#x200B;If you are interested, please follow the page.&#x200B;COLING 2022 Official LinkedIn: [https://www.linkedin.com/company/coling2022/](https://www.linkedin.com/company/coling2022/)&#x200B;COLING 2022 Official Reddit: [https://www.reddit.com/r/COLING2022/](https://www.reddit.com/r/COLING2022/)",POS
204,pjrqbn,Chrome summarizer extension (powered by DistilBERT),https://chrome.google.com/webstore/detail/tldr-chrome/khkpnmmnkenbelkljphmpbjgbmobgonn,ENOTwhynoT,LanguageTechnology,1970-01-01 00:00:01.631035812,,9,3,,NEU
205,pjro0y,"How to quantify ""public support"" versus ""public sentiment""?",https://www.reddit.com/r/LanguageTechnology/comments/pjro0y/how_to_quantify_public_support_versus_public/,Kaamelott,LanguageTechnology,1970-01-01 00:00:01.631035614,,7,5,"Say I have a bunch of tweets with the keywords ""artificial intelligence"". I want to assess how many tweets support/like artificial intelligence, and how many tweets fear/dislike it.I trained a model on categorized Twitter data (positive/negative) based on more than 1 million tweets.So, I can identify, based on this (78% accuracy) model, which tweets are positive and which are negative.However, this gives me the ""polarity of the discussions"", but not per se the actual opinion on the matter (i.e., one can be happy that the project is a failure, or unhappy that it worked, or happy that the project is a success, or unhappy that it fails).For example:""Artificial intelligence will doom humanity"" --> model see ""negative"" sentiment --> correct for ""support""""Another artificial intelligence project shut down, awesome, there is hope for humankind"" --> model see ""positive"" sentiment --> incorrect for ""support""So, my question is:Do i have to train the data myself on exactly what I want (that is, instead of sentiment, categorize tweets based on ""support for"") and create a new model? Or is that actually too complex with (non-expert) regular models due to sarcasm, double negatives, etc, and the accuracy will basically never be > 50% (unless I'm a genius at creating data and applying my models)?Categorizing enough (and ""enough"" is difficult to define too) tweets  is very time consuming so I want to be as sure as possible that it's not an obvious waste of time.",POS
206,pjrm65,Automatically disambiguating medical acronyms with ontology-aware deep learning,https://www.nature.com/articles/s41467-021-25578-4,theawkwardmegladon,LanguageTechnology,1970-01-01 00:00:01.631035458,,5,0,,NEU
207,pjq18l,Reducing dimensionality in Bag of Words. Is grouping words using Word2Vec a known practice?,https://www.reddit.com/r/LanguageTechnology/comments/pjq18l/reducing_dimensionality_in_bag_of_words_is/,x11ry0,LanguageTechnology,1970-01-01 00:00:01.631030692,,6,4,"Dear,I'm playing with a bag of words currently on a dataset where I need to detect the toxicity in the discussions. It is somewhat similar to what has been done in the toxic comments classification challenge. However I need explainability because this is an educative app.I'm cutting the documents sentence per sentence and run inference on each sentence independently. Then I shall detect the role of each word in the prediction.I'm testing BERT and SHAPE but I also exploring more lightweight methods.One issue using Bag of Words is that the vocabulary is very rich and this is definitely not useful to keep such a rich vocabulary because many words have same use type. Insults, reproaches, judgements...One of my ideas was to keep the most common words and nGrams, like ""you are"", ""such a"" but to reduce the extremely dimensionality due to the rares adjectives related to insults, reproaches, judgements... by using Word2Vec vector similarity to group them together.I tend to have a wide imagination that can lead me to explore silly things. I wonder if some of you have some experience or opinions about that?Thanks",POS
208,pjt09l,Implementing Transformer Paper (Google T5 Transformer from Scratch and using it to create a…,https://medium.com/analytics-vidhya/googles-t5-transformer-theory-ffd0acc738d2,Diligent-Pepper5166,LanguageTechnology,1970-01-01 00:00:01.631039721,,2,0,,NEU
209,pjh1lk,Is there something like person-specific sentiment analysis?,https://www.reddit.com/r/LanguageTechnology/comments/pjh1lk/is_there_something_like_personspecific_sentiment/,massimosclaw2,LanguageTechnology,1970-01-01 00:00:01.630994102,,11,3,"Sentiment analysis, as we know, measures ""Cake sucks"" as say -0.4, and ""Cake is great"" as 0.7.What I'm looking for is something a bit different like so:1. Given input text data written by 1 person (say a blog)2. Predict how they (the person who wrote all the text on the blog) might react to a certain piece of text----------What might something like this look like?- Let's suppose that Person A with a blog has written in his blog post thousands of times about how much cake is the best thing to happen to humanity.- The system should probably infer that if that person read something like ""Cake is the WORST food ever"", they would react negatively to it, if say, they also believe that there is such a thing as 'objective taste' somehow ([aesthetic absolutism](https://en.wikipedia.org/wiki/Aesthetic_absolutism))- Or if Person A has made anti-racist statements, that racist statements would be strongly negative.- If Person A reads the statement ""I hate lawyers"" and in their blog they have written about how they don't care either way about law, it should probably be 0 (indifferent). - Finally, if Person A reads the statement ""iPhones are better than Android"" and there is *zero* data about either iPhones or Androids, or even related data about Apple or Google, then it should probably be 0, with an additional ""confidence"" metric at 0 (since there is no data, this confidence metric will let us know whether there is any data to support the measurement or not). ----------This model would need to be able to somehow inductively 'infer' a value system of some kind, and assign intensities of probable reactions based on the frequency of an expressed view, as well as pick up on nuances (such as philosophical assumptions, (for example in the above cake example: aesthetic absolutism) etc.) that may inform that measurement.In other words, I'd like to create a model (or find a pre-trained model to fine-tune), that would be able to, given text data from that 1 person, predict their sentiment in response to a new piece of text.Would love any help whatsoever regarding:1. What types of pre-trained models I should look at2. Any ideas of any kind whatsoever you might have on how to achieve this3. What sorts of architectures/resources/concepts may be relevant to look at",POS
210,pjrrba,how do you keep track of translations for terminologies when writing a technical book,/r/writing/comments/pjrr02/how_do_you_keep_track_of_translations_for/,paarulakan,LanguageTechnology,1970-01-01 00:00:01.631035891,,1,1,,NEU
211,pjjqy4,Any method to generate text using only keywords in GPT2?,https://www.reddit.com/r/LanguageTechnology/comments/pjjqy4/any_method_to_generate_text_using_only_keywords/,GustaMusto,LanguageTechnology,1970-01-01 00:00:01.631007397,,3,3,I am fine-tuning a GPT2 model in order to generate text. One approach I've found would work is to append the nouns at the start of the sentence and then pass it to the model for fine-tuning.The text would get shortened even more due to the max\_len feature. Are there any alternative approaches I can use for this task?,POS
212,pjjl5w,NLP Model which extracts key points,https://www.reddit.com/r/LanguageTechnology/comments/pjjl5w/nlp_model_which_extracts_key_points/,Lychee7,LanguageTechnology,1970-01-01 00:00:01.631006569,,1,2,"I'm sorry if it isn't the right place.I'm looking for a NLP Model which extracts like --Eg.-"" I ate small bowl of oatmeal for breakfast then had a pizza for lunch. In the evening played basketball for 2 hours.Finally had rice curry for dinner ""The model should extract anything which added the calories or burn it. Is there any trained model like this or similar to it ?I'm familiar with ML but not much with NLP. Regards.",POS
213,pj1whc,Comparison of python libraries for language identification 🐍,https://modelpredict.com/language-identification-survey,derivablefunc,LanguageTechnology,1970-01-01 00:00:01.630943057,,20,6,,NEU
214,pjcpcy,What is the best solution to automatically preprocess and correct a LOT of English text?,https://www.reddit.com/r/LanguageTechnology/comments/pjcpcy/what_is_the_best_solution_to_automatically/,JoZeHgS,LanguageTechnology,1970-01-01 00:00:01.630976781,,2,3,Hi everyone!I am looking for the best automated solution to go through a **LOT** of text in the English language and correct all sorts of problems from misspellings to improper capitalization and grammar. Think [Grammarly](https://www.grammarly.com/) on **crack**.Does such a solution (or set of solutions) exist? What would you recommend?Thank you very much!,POS
215,pj4jhd,Manual Data Annotation Tool for Relation Extraction,https://www.reddit.com/r/LanguageTechnology/comments/pj4jhd/manual_data_annotation_tool_for_relation/,MarurSri,LanguageTechnology,1970-01-01 00:00:01.630950700,,2,2,"Over the past week I’ve been looking for a manual annotation tool for relation extraction like this (e1,relation,e2). I have found UBIAI and Prodigy but they offer paid services.I’m looking for open source tools for my project. BRAT and Inception are used for RE but I’m not considering them because of their limitations.Looking for open source tools for manual annotationThanks",POS
216,piunte,Ancient Texts for non-English texts - which library?,https://www.reddit.com/r/LanguageTechnology/comments/piunte/ancient_texts_for_nonenglish_texts_which_library/,Premshay,LanguageTechnology,1970-01-01 00:00:01.630913178,,5,5,"Hi all,I'm looking into existing libraries to analyze ancient texts in Greek and in Arabic.Stanza?SpaCY?CLTK?I believe in any case I'll have to add to the library code or models, since none has robust (ancient) Greek.Arabic also has challenges, like second person.Which would you recommend to start exploring?",POS
217,pisaek,"What is the best free NLP or similar solution to tell whether two product descriptions are of the same product, of similar but different products or completely different products?",https://www.reddit.com/r/LanguageTechnology/comments/pisaek/what_is_the_best_free_nlp_or_similar_solution_to/,JoZeHgS,LanguageTechnology,1970-01-01 00:00:01.630902552,,7,8,"Hi everyone!I need to compare product descriptions and titles from e-commerce websites to tell whether they are describing the same product, similar products from the same category or completely different products. Can this be relatively easily accomplished using already existing and free solutions? Also, does language matter? I wanted it for Portuguese and English at the moment, maybe more languages later.Thanks a lot!",POS
218,piughc,Building a Aspect based sentiment classification,https://www.reddit.com/r/LanguageTechnology/comments/piughc/building_a_aspect_based_sentiment_classification/,upsilonbeta,LanguageTechnology,1970-01-01 00:00:01.630912242,,4,1,"Here is the task -We are given a text and a phrase. We need to find the sentiment associated with the phraseExample1-text - ""The quality of food was good but service is bad""phrase - ""food""Sentiment - PositiveExample2-text - ""the milk was delivery late""phrase - ""milk""Sentiment - NegativeTotal sentiment classes = [Positive, Negative, Nuetral]How can I go about achieving this task, Any suggestions would be helpfulFor now-1. I tried using Bert sentence pair classification by concatenating the text and phrase tokens with \[SEP\] token and passed it into BERT. The model was overfitting and had no good results.Im thinking of trying-1. Trying to use the contextualized vectors of the phrase in the sentence (BERT or ELMO) and then directly apply the feed-forward layer to it.Could you recommend some other strategies to solve this problem?",NEG
219,pifzdb,What does NLP work outside of the FAANG and research look like?,https://www.reddit.com/r/LanguageTechnology/comments/pifzdb/what_does_nlp_work_outside_of_the_faang_and/,compost_embedding,LanguageTechnology,1970-01-01 00:00:01.630858557,,38,21,"In the Stanford CS224N course, there's a lot of discussion about different architectures for solving different NLP tasks and how those have evolved over the years. I imagine in research and at FAANG companies, some of the work is based around developing those systems. What I'm curious about is what types of applications does NLP find in the less-techy firms, and what is the nature of the data scientist's work there? In other words, is it often just applying the pre-built models one can find online to privately held corpora, adding some custom business logic around the results, etc? Or is there more space and need for more nuts and bolts development of these systems? I get the impression that one could potentially do quite a bit with simply using some of the pre-built models from a library like HuggingFace and that needing to think about different architecture structures wouldn't really come up in day-to-day tasks, but I'm unsure whether this is correct.",POS
220,piwyb1,Project Help : Semi supervised text classifier,https://www.reddit.com/r/LanguageTechnology/comments/piwyb1/project_help_semi_supervised_text_classifier/,d0r1h,LanguageTechnology,1970-01-01 00:00:01.630924654,,1,1,"Hi,I am planning to work on a semi supervised text classifier (NLP. Project), though I'm a master student.I have never worked before on semi supervised learning.So if you have thought, Idea, resources, or this seems interesting to you and wanna Collab I'm happy to welcome you. Any kind of help will be very grateful.",POS
221,pigmc3,What tech do I need to learn to programmatically parse ingredients from a recipe?,https://www.reddit.com/r/LanguageTechnology/comments/pigmc3/what_tech_do_i_need_to_learn_to_programmatically/,robskrob,LanguageTechnology,1970-01-01 00:00:01.630860651,,3,2,"I’m looking to create an app that can parse ingredients from recipes written in English.I have a general understanding that the above application is a form of NLP, but nothing more than this basic understanding. My questions are as follows:1. I think this falls in the realm of Natural Language Processing (NLP). Is that correct?2. What do I need to learn to write a program that can parse ingredients from recipes?3. Given the above, does anyone have any suggested courses that I might want to take to learn what I need to learn?4. I’m currently learning Kotlin – can I write an app in Kotlin to parse ingredients from a recipe text?5. If using Kotlin is a suitable approach, what technologies in addition to Kotlin should I look into for parsing ingredients from a recipe?",POS
222,picsar,Deep Natural Language Processing for LinkedIn Search Systems (Research Paper Walkthrough),https://youtu.be/l3O7bCn1JI0,prakhar21,LanguageTechnology,1970-01-01 00:00:01.630847260,,5,0,,NEU
223,pikuvm,There can be no True Scottish Spoken Language System - Journal of Astrological Big Data Ecology,https://www.reddit.com/gallery/p541qg,Squester,LanguageTechnology,1970-01-01 00:00:01.630874307,,1,0,,NEU
224,phwiep,Comparing the similarity of webpage text,https://www.reddit.com/r/LanguageTechnology/comments/phwiep/comparing_the_similarity_of_webpage_text/,Jack7heRapper,LanguageTechnology,1970-01-01 00:00:01.630778831,,9,3,"Newbie question here. I have a large dataset of web pages related to a particular domain. I would like to develop a model that can ""learn"" the type of language written on the web pages in this domain. Since all of them come under the same domain, the text in those web pages is similar. I think you could train some sort of encoder to understand the language in those web pages. I am not sure but I think Word2Vec is similar to what I want. After that, given any webpage, I would like to calculate the similarity between the text in that webpage and the type of text that the model has learned. Is this sort of thing possible?",POS
225,ph8zc5,"Difference between libraries like NLTK, SpaCy and models like GPT-3",https://www.reddit.com/r/LanguageTechnology/comments/ph8zc5/difference_between_libraries_like_nltk_spacy_and/,InvincibleKnigght,LanguageTechnology,1970-01-01 00:00:01.630687638,,13,5,I am an ultra newbie at Natural Language processing and I've done a basic course in it. I've used SpaCy and NLTK in projects and I've read about GPT-3/GPT-2 BERT and the likes. However I have not seen an intersection of these. Are SpaCy and NLTK related to GPT-3 or are these two completely different entities.Do they overlap or they are completely disjoint?Please Explain like I'm five! Thank you :D,POS
226,ph50pm,Tutorial: Faster and smaller Hugging Face BERT on CPUs via “compound sparsification”,https://www.reddit.com/r/LanguageTechnology/comments/ph50pm/tutorial_faster_and_smaller_hugging_face_bert_on/,markurtz,LanguageTechnology,1970-01-01 00:00:01.630675215,,23,2,"Hi r/LanguageTechnology,I want to share our latest open-source research on combining multiple sparsification methods to improve the Hugging Face BERT base model (uncased) performance on CPUs. We combine distillation with both unstructured pruning and structured layer dropping. This “compound sparsification” technique enables up to 14x faster and 4.1x smaller BERT on CPUs depending on accuracy constraints.We’ve been working hard to make it easy for you to apply our research to your own private data: [sparsezoo.neuralmagic.com/getting-started/bert](https://sparsezoo.neuralmagic.com/getting-started/bert)If you’d like to learn more about “compound sparsification” and its impact on BERT across different CPU deployments, check out our recent blog: [neuralmagic.com/blog/pruning-hugging-face-bert-compound-sparsification/](https://neuralmagic.com/blog/pruning-hugging-face-bert-compound-sparsification/)Let us know what you think!",POS
227,ph9umk,"Numbers, how do you deal with them in NLP",https://www.reddit.com/r/LanguageTechnology/comments/ph9umk/numbers_how_do_you_deal_with_them_in_nlp/,gevorgter,LanguageTechnology,1970-01-01 00:00:01.630690267,,6,2,"So far, everything i read deals with words. And makes sense.How do you deal with numbers, Do you have embeddings digit by digit?I see how word ""red"" can be predicted on the phrase ""He was hit by a red bus"". But doubt that any one can predict numbers in a phrase ""His loan amount was 1346.00""",NEG
228,pha029,"Sentiment Analysis and ""Polite Conditionals""",https://www.reddit.com/r/LanguageTechnology/comments/pha029/sentiment_analysis_and_polite_conditionals/,eephus2,LanguageTechnology,1970-01-01 00:00:01.630690756,,5,2,"I'm using a transformer model based on distilBERT for sentiment analysis for customer service data, and the model is scoring text as positives when there are polite conditionals, such as ""It would be nice if"", or ""I'd like to see this"". Ideally such phrases should probably be viewed as neutral.  A full example is ""I think it would be a great opportunity if you could improve your app's performance. We need to enhance response times."" This is coming back positive, but probably should be negative or neutral. I was hoping with BERT based model, we'd pick up such nuance.  Should I go with a bigger model than DistilBERT, add more labeled data targeting this issue, or might there be another option? My dataset has these polite conditionals all over the place. &#x200B;Thanks!!!!",POS
229,pha6ef,Run ONNX Transformers pipelines,https://www.reddit.com/r/LanguageTechnology/comments/pha6ef/run_onnx_transformers_pipelines/,davidmezzetti,LanguageTechnology,1970-01-01 00:00:01.630691322,,2,0,"&#x200B;https://reddit.com/link/pha6ef/video/4qjcbr5tsbl71/playerThe example above shows how [txtai](https://github.com/neuml/txtai) enables exporting models to [ONNX](https://github.com/onnx/onnx). The example loads an existing Transformers model, exports it to ONNX and then runs the sentiment analysis model with ONNX (scores are from 0-1 with 1 being most positive and 0 most negative).Exported ONNX models can be directly loaded either with txtai or Hugging Face pipelines. Check it out![Link to Gist](https://gist.github.com/davidmezzetti/9a1f75cfe96be1c1b043a42fe9082dc5)",POS
230,pgu89q,"Is anyone familiar with the task ""Opinion Unit Extraction""?",https://www.reddit.com/r/LanguageTechnology/comments/pgu89q/is_anyone_familiar_with_the_task_opinion_unit/,Revlong57,LanguageTechnology,1970-01-01 00:00:01.630628804,,11,4,"So, I was looking through this [blog post](https://monkeylearn.com/blog/aspect-based-sentiment-analysis/)  on Aspect-Based Sentiment Analysis, and they brought up something called ""Opinion Unit Extraction"" in the preprocessing section. Here is a demo for the model they  used to do this. [https://app.monkeylearn.com/main/extractors/ex\_N4aFcea3/tab/demo/](https://app.monkeylearn.com/main/extractors/ex_N4aFcea3/tab/demo/) However, I can't seem to find anything else on the internet related to concept of ""Opinion Units"", but I'd like to learn more about how this model/task works. Does anyone have any ideas where I could read more about this?",POS
231,pgz9xa,Podcasts for NLP in German?,https://www.reddit.com/r/LanguageTechnology/comments/pgz9xa/podcasts_for_nlp_in_german/,reqarfar,LanguageTechnology,1970-01-01 00:00:01.630648891,,2,1,Do you know of any good podcasts in German about NLP?,POS
232,pghq96,How to use the biocreative corpus in NLTK?,https://www.reddit.com/r/LanguageTechnology/comments/pghq96/how_to_use_the_biocreative_corpus_in_nltk/,Adam_24061,LanguageTechnology,1970-01-01 00:00:01.630588717,,7,2,"Does anyone know how to load the ""biocreative"" corpus in NLTK? I've downloaded it (all the corpora, in fact) but    >>> import nltk    >>> from nltk.corpus import biocreative    Traceback (most recent call last):      File ""<pyshell#22>"", line 1, in <module>        from nltk.corpus import biocreative    ImportError: cannot import name 'biocreative' from 'nltk.corpus'and I get the same error if I try `biocreative_ppi`, `bc`, or `bc_ppi` (ideas from looking at the directory and file names). (Other corpora such as `brown`, `reuters`, `treebank`, and so on work.)",NEG
233,pfuaen,Software / Library for Translating Audio Files to Transcripts,https://www.reddit.com/r/LanguageTechnology/comments/pfuaen/software_library_for_translating_audio_files_to/,Anonymous_Dawn,LanguageTechnology,1970-01-01 00:00:01.630504193,,12,6,What are the suggested and best software / libraries / APIs for providing transcripts of audio file (speech recognition) that can either be free or have subscription charges?,POS
234,pfexm6,I want to start this project but I am unsure what to do,https://www.reddit.com/r/LanguageTechnology/comments/pfexm6/i_want_to_start_this_project_but_i_am_unsure_what/,Stylografo,LanguageTechnology,1970-01-01 00:00:01.630444140,,5,4,"For a bit of background, I have no experience in coding except very basic knowledge in Python but I am super willing to learn anything necessary.I have this idea for a competitor for Grammarly. Not a direct competitor, but one targeted at poets. I envision it being able to count syllables per line, rhymes for words, synonyms, antonyms, detect clichés and be able to guide a user to use a form of their choice. With all these different components, I know it will take time, probably years to complete however I will just pick out a few to release a beta to customers to gauge interest and then carry on from there. So now you know what I want to do, how would you start this in my position? Would you dive right into the NLP part or fully learn Python then start?",POS
235,pfb5im,Encoder Decoder model train on custom dataset,https://www.reddit.com/r/LanguageTechnology/comments/pfb5im/encoder_decoder_model_train_on_custom_dataset/,gattaloukik321,LanguageTechnology,1970-01-01 00:00:01.630432657,,5,9,"I am working on a model that can give me hex codes of a certain color based on a description of it. I have a dataset that maps the description of a color to its respective hex codes. Now which model should I finetune inorder to make this work? I tried working using some of the hugging face models but I couldnt find any, can any one share some links that might help me?",POS
236,pfjh2y,[Projects] Ideas on a project for grad school,https://www.reddit.com/r/LanguageTechnology/comments/pfjh2y/projects_ideas_on_a_project_for_grad_school/,urban_fantast,LanguageTechnology,1970-01-01 00:00:01.630459436,,0,4,"Hey all,I just started a graduate(masters) program in computer science and I am supposed to submit an idea for a final term project for an intro to NLP class. Even though I have done projects in the past, I am feeling overwhelmed now. Earlier, I used to find research papers and tried to implement them on my own or by using the code as a reference. Are my professors looking for something similar?Should I use an existing paper, or should I create something original on my own? I am not sure how to do that without the required datasets. Please help me on this, and any guidance on where to get such a std paper would be much appreciated too.Ps: By now you might have guessed that I don’t have any research experience. I am here to dip my toes into the ocean.",POS
237,pf3lx2,Looking for text highlighting / annotation library like displacy that can handle overlapping entities / texts,https://www.reddit.com/r/LanguageTechnology/comments/pf3lx2/looking_for_text_highlighting_annotation_library/,ratatouille_artist,LanguageTechnology,1970-01-01 00:00:01.630407892,,9,5,I am looking for a  text highlighting library like displacy that can handle overlapping entities / matches. I am currently using displacy to highlight matched texts and named entities. One challenge with it is that it can't handle overlaps.I looked at [https://github.com/tvst/st-annotated-text](https://github.com/tvst/st-annotated-text) which doesn't handle overlaps either :(  Searching for annotations excluding matplotlib results I don't get too many results.  Looking for text highlighting libraries doesn't seem fruitful either as most of them are pdf based highlighting solutions.  Prodigy's span manual annotation interface is an exmaple of what I am looking for [https://prodi.gy/docs/api-interfaces#spans\_manual](https://prodi.gy/docs/api-interfaces#spans_manual) but I am looking for a library where I can just display text like this though ideally using boxes rather than just lines,POS
238,petq6l,"I scrapped Amazon product reviews, I was wondering what analysis can I do on it",https://www.reddit.com/r/LanguageTechnology/comments/petq6l/i_scrapped_amazon_product_reviews_i_was_wondering/,maxcaliburx,LanguageTechnology,1970-01-01 00:00:01.630365655,,9,15,"I wrote a Selenium script that scrape Amazon product reviews and I want to get an insight from it, and to impress my potential employer - job interview is on Wednesday (the company buys Amazon brands). What insights and tools, l can I get from product reviews? I hope this this is the right place to ask. Thank you for the help!",POS
239,pex5fe,CtrlGen Workshop at NeurIPS 2021 (Controllable Generative Modeling in Language and Vision),https://www.reddit.com/r/LanguageTechnology/comments/pex5fe/ctrlgen_workshop_at_neurips_2021_controllable/,CtrlGenWorkshop,LanguageTechnology,1970-01-01 00:00:01.630377895,,4,0,"We are holding a controllable generation workshop at NeurIPS 2021! It aims to explore disentanglement, controllability, and manipulation for the generative vision and language modalities. We feature an exciting lineup of speakers, a live QA and panel session, interactive activities, and networking opportunities. See our website below for more! We are also inviting both paper and demo submissions related to controllable generation (read further for details).**Workshop Website:** [https://ctrlgenworkshop.github.io/](https://ctrlgenworkshop.github.io/)**Contact:** [ctrlgenworkshop@gmail.com](mailto:ctrlgenworkshop@gmail.com)**Important Dates*** Paper Submission Deadline: ***September 30, 2021 (UPDATED)**** Paper Acceptance Notification: October 22, 2021* Paper Camera-Ready Deadline: November 1, 2021* Demo Submission Deadline: ***October 29, 2021**** Demo Acceptance Notification: November 19, 2021* Workshop Date: ***December 13, 2021*****Submission Portal (Papers + Demos):**  [https://cmt3.research.microsoft.com/CtrlGen2021/Submission/Index](https://cmt3.research.microsoft.com/CtrlGen2021/Submission/Index)&#x200B;**Full Call for Papers:** [h](https://ctrlgenworkshop.github.io/CFP.html)[ttps://ctrlgenworkshop.github.io/CFP.html](https://ctrlgenworkshop.github.io/CFP.html)Paper submission deadline: ***September 30, 2021 (UPDATED)***. Topics of interest include:**Methodology and Algorithms:*** New methods and algorithms for controllability.* Improvements of language and vision model architectures for controllability.* Novel loss functions, decoding methods, and prompt design methods for controllability.**Applications and Ethics:*** Applications of controllability including creative AI, machine co-creativity, entertainment, data augmentation (for [text](https://arxiv.org/abs/2105.03075) and [vision](https://journalofbigdata.springeropen.com/articles/10.1186/s40537-019-0197-0)), ethics (e.g. bias and toxicity reduction), enhanced training for self-driving vehicles, and improving conversational agents.* Ethical issues and challenges related to controllable generation including the risks and dangers of deepfake and fake news.**Tasks (a few examples):*** [Semantic text exchange](https://aclanthology.org/D19-1272/)* [Syntactically-controlled paraphrase generation](https://arxiv.org/abs/1804.06059)* [Persona-based text generation](https://aclanthology.org/W19-3402/)* Style-sensitive generation or style transfer (for [text](https://arxiv.org/abs/2011.00416) and [vision](https://github.com/ycjing/Neural-Style-Transfer-Papers))* Image synthesis and scene representation in both 2D and 3D* Cross-modal tasks such as controllable image or video captioning and generation from text* New and previously unexplored controllable generation tasks!**Evaluation and Benchmarks*** New and improved evaluation methods and metrics for controllability* Standard and unified metrics and benchmark tasks for controllability**Cross-Domain and Other Areas*** Work in interpretability, disentanglement, robustness, representation learning, etc.**Position and Survey Papers*** For example, exploring problems and lacunae in current controllability formulations, neglected areas in controllability, and the unclear and non-standardized definition of controllability&#x200B;**Full Call for Demonstrations:** [https://ctrlgenworkshop.github.io/demos.html](https://ctrlgenworkshop.github.io/demos.html)Submission deadline: ***October 29, 2021***. Demos of all forms: research-related, demos of products, interesting and creative projects, etc. Looking for creative, well-presented, and attention-grabbing demos. Examples include:* Creative AI such as controllable poetry, music, image, and video generation models.* Style transfer for both text and vision.* Interactive chatbots and assistants that involve controllability.* Controllable language generation systems, e.g. using GPT-2 or GPT-3.* Controllable multimodal systems such as image and video captioning or generation from text.* Controllable image and video/graphics enhancement systems.* Systems for controlling scenes/environments and applications for self-driving vehicles.* Controllability in the form of deepfake and fake news, specifically methods to combat them.* And much, much more…&#x200B;**Organizing Team:*** [Steven Feng](https://styfeng.github.io/) (CMU)* [Anusha Balakrishnan](https://www.microsoft.com/en-us/research/people/anbalak/) (Microsoft Semantic Machines)* [Drew Hudson](https://cs.stanford.edu/people/dorarad/) (Stanford)* [Tatsunori Hashimoto](https://thashim.github.io/) (Stanford)* [Dongyeop Kang](https://dykang.github.io/) (UMN)* [Varun Gangal](https://vgtomahawk.github.io/) (CMU)* [Joel Tetreault](https://www.cs.rochester.edu/~tetreaul/academic.html) (Dataminr)",POS
240,pekvik,Article + videos explaining product quantization for vector similarity search,https://www.reddit.com/r/LanguageTechnology/comments/pekvik/article_videos_explaining_product_quantization/,jamescalam,LanguageTechnology,1970-01-01 00:00:01.630338902,,13,2,Hi all! I put together [this article](https://www.pinecone.io/learn/product-quantization/) (including two videos) covering the essentials of product quantization (PQ) for vector compression in vector similarity search - also includes examples for IndexPQ and IndexIVFPQ in faiss with python - let me know if you have questions/suggestions - I hope it's useful! :),POS
241,pem9io,Automated answering questions based on semantic similarity and ontology,https://www.reddit.com/r/LanguageTechnology/comments/pem9io/automated_answering_questions_based_on_semantic/,DragonKhaled,LanguageTechnology,1970-01-01 00:00:01.630342908,,3,2,"Hi guys, I'm working on this project what is the essential technics that i should be aware of.",NEU
242,pedmrt,How to extract item name from a given sentence?,https://www.reddit.com/r/LanguageTechnology/comments/pedmrt/how_to_extract_item_name_from_a_given_sentence/,vijhhh2,LanguageTechnology,1970-01-01 00:00:01.630310562,,9,16,"Hi,I am new to NLP.I have sentence like**4-5 bone-in skin-on chicken thighs** in this I need to extract  **Chicken thighs** as Ingredient.one more example**2 cloves of garlic minced** in this I need to extract **garlic** as Ingredientone final example**2 serrano chiles minced (remove the seeds and membranes if you want it less spicy)** in this I need to extract **chiles** as IngredientPlease explain me a method on how to solve this using NLP.",POS
243,peng3g,ML Podcast!,https://www.reddit.com/r/LanguageTechnology/comments/peng3g/ml_podcast/,ieeevitvellore,LanguageTechnology,1970-01-01 00:00:01.630346413,,1,0," Hey guys! My friends and I have been working on a tech podcast and our latest episode on Machine Learning is out now!This week’s episode is on Machine Learning, Artificial Intelligence and Data Science with our speaker Vaidheeswaran Archana who is an Artificial intelligence engineer at Continental and Leadership Fellow at Women Who Code.It’ll be great if y’all could check it out and get some amazing insights into the world of MLNow streaming on Spotify, Apple Music and all platforms that you love! Listen now: [https://open.spotify.com/show/7550NpVvaE4pgaOvYo6xCp?si=OG9-FfENQxWjJ9Qd9KAJWw&nd=1](https://open.spotify.com/show/7550NpVvaE4pgaOvYo6xCp?si=OG9-FfENQxWjJ9Qd9KAJWw&nd=1)[https://podcasts.apple.com/in/podcast/the-techloop-podcast/id1528881215](https://podcasts.apple.com/in/podcast/the-techloop-podcast/id1528881215)&#x200B;https://reddit.com/link/peng3g/video/j6t4cjlcbjk71/player",POS
244,pegiuj,Ratio Between Amount of Training Data and Vocab Size,https://www.reddit.com/r/LanguageTechnology/comments/pegiuj/ratio_between_amount_of_training_data_and_vocab/,idkwhatever1337,LanguageTechnology,1970-01-01 00:00:01.630324274,,3,3,"Hey guys, so I’m currently working on a language modelling task and I have roughly 6 million sentences for training data. What would a suitable vocab size be? Maybe the 100k tokens or is that too large given the data?",POS
245,pedfli,How can I identify flagged keywords from text?,https://www.reddit.com/r/LanguageTechnology/comments/pedfli/how_can_i_identify_flagged_keywords_from_text/,Current_Dark6603,LanguageTechnology,1970-01-01 00:00:01.630309513,,2,5,"I have text data from expense receipts. I need to identify few items like alcohol, so I can mark those receipts.Data format: 1 text file for each receipt text with trimmed spaces.In future I might be supposed to find jewellery and cosmetics receipt types as well from their raw text.For beginners I have a config file with related string / regex patterns which I am using to identifying few items.I need to improve performance of the system. Is there anything  I can refer for further enhancements, like a dataset for related regex patterns or list of alcohol items.I cannot use ML models to classify them as it will take my team some time to request for further resources.Programming language: python",POS
246,pecn8h,"Analyzing screenplay- NLP models for more than one ""author""?",https://www.reddit.com/r/LanguageTechnology/comments/pecn8h/analyzing_screenplay_nlp_models_for_more_than_one/,reddithurc,LanguageTechnology,1970-01-01 00:00:01.630305506,,2,1,I want to conduct a research on movie screenplay (e.g. punchline/ argument) but not sure what models I should be looking into....For not the BERT models/ RNNs work towards one author. Is there any language model architecture that caters to multiple authors specifically (e.g. it can classify whether two people are having an argument based on a movie script)?,NEG
247,peawfb,[paper suggestion] Long text generation,https://www.reddit.com/r/LanguageTechnology/comments/peawfb/paper_suggestion_long_text_generation/,nlpkar,LanguageTechnology,1970-01-01 00:00:01.630297580,,2,1,"Hi there,I want to work on long text generation. I want to have your experiences : )Which papers do you think are best/must to read? and which approaches do you think are the best?Also if you know of any paper on long text generation with VAE, that would be great.&#x200B;Thanks in advance.",POS
248,pdkuji,Automated Narrative Content from Data,https://www.reddit.com/r/LanguageTechnology/comments/pdkuji/automated_narrative_content_from_data/,petahpablo,LanguageTechnology,1970-01-01 00:00:01.630195408,,6,0,"I am looking to automate some report writing based on tables of simple statistics such as percent change over time, ranking, share of total etc. I am aware of vendors that provide such services such as WordSmith, Arria and Narrative Science but am hoping to find an open source solution in Python. Is anyone aware of this NLG capability in Simple NLG, spaCy, a Markov Generator or any other option?Thank you",POS
249,pdhz2e,non-standard english dataset,https://www.reddit.com/r/LanguageTechnology/comments/pdhz2e/nonstandard_english_dataset/,horniestvegan,LanguageTechnology,1970-01-01 00:00:01.630184839,,5,1,Hi!I am looking for a text dataset with any sort of non-standard english like Appalachian English or AAVE. Do you know where I could find something like this?Thanks!,POS
250,pdk9s1,Best Machine Learning (open source) libraries for C# .NET5 (cross platform?),https://www.reddit.com/r/LanguageTechnology/comments/pdk9s1/best_machine_learning_open_source_libraries_for_c/,BlackCat550,LanguageTechnology,1970-01-01 00:00:01.630193196,,2,0,"I've been trying to create a model with KERAS.NET and TENSORFLOW.NET, but have been running into problems:No module named 'tensorflow'.I've tried resolving this with Python.Included with no luck.So I'm now considering a (less frustrating) alternative that I can hopefully use cross platform.(I'm currently using Catalyst NLP for pre-processing, so pre-processing capabilities arent important.)Any suggestions?",POS
251,pcygyp,"Run Transformers models directly in JavaScript, Java and Rust with ONNX",https://www.reddit.com/r/LanguageTechnology/comments/pcygyp/run_transformers_models_directly_in_javascript/,davidmezzetti,LanguageTechnology,1970-01-01 00:00:01.630106194,,16,0,"The ONNX runtime provides a common serialization format for machine learning models. ONNX enables direct inference on a number of different platforms/languages. For example, a model could be run directly on Android to limit data sent to a third party service. ONNX is an exciting development with a lot of promise.The referenced notebook below covers how to export models to ONNX using txtai. These models will then be directly run in Python, JavaScript, Java and Rust. Currently, txtai supports all these languages through it's API but this notebook runs everything direct within each language!The following is a non-exhaustive list of use cases.* Build locally executed models for mobile/edge devices* Run models with Java/JavaScript/Rust development stacks when teams prefer not to add Python to the mix* Export models to ONNX for Python inference to improve CPU performance and/or reduce number of software dependenciesNotebook: [https://colab.research.google.com/github/neuml/txtai/blob/master/examples/18\_Export\_and\_run\_models\_with\_ONNX.ipynb](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/18_Export_and_run_models_with_ONNX.ipynb)GitHub: [https://github.com/neuml/txtai](https://github.com/neuml/txtai)",POS
252,pcz8ig,Using char n grams as input to a neural network,https://www.reddit.com/r/LanguageTechnology/comments/pcz8ig/using_char_n_grams_as_input_to_a_neural_network/,carusGOAT,LanguageTechnology,1970-01-01 00:00:01.630108922,,3,5,Do any of you have any experience with using char n grams as features for a neural network? If so are there any concerns considering the sparsity of features and how exactly should they be preprocessed for a neural network?,NEU
253,pcn4ma,Register for Truth and Trust Online 2021 Conference,https://www.reddit.com/r/LanguageTechnology/comments/pcn4ma/register_for_truth_and_trust_online_2021/,kochkinael,LanguageTechnology,1970-01-01 00:00:01.630070105,,5,0,"The **registration for Truth and Trust Online 2021** is now open!The annual Conference for Truth and Trust Online is organised as a unique collaboration between practitioners, technologists, academics and platforms, to share, discuss, and collaborate on useful technical innovations and research in the space of online communications.Register to hear from our keynote speakers **Kalina Bontcheva (Sheffield University), Gianluca Stringhini (Boston University) and Lyric Jain (LogicallyAI)**, and join the discussions on the future of online communications! [https://truthandtrustonline.com/register/](https://truthandtrustonline.com/register/)  Also, we have a f**ee waiver program** in place for those who may not have the financial resources that permit registration. Find the form here (the deadline to apply is October 4th, 2021): [https://docs.google.com/forms/d/e/1FAIpQLSdfKlquWRahjRPY0kyoinH8L1\_irZXu712XLd6oaxnZwn73MQ/viewform](https://docs.google.com/forms/d/e/1FAIpQLSdfKlquWRahjRPY0kyoinH8L1_irZXu712XLd6oaxnZwn73MQ/viewform)   Looking forward to seeing you in October!",POS
254,pcepoa,Join us this weekend for TALC (the Teen Academic Linguistics Conference)!!,/r/linguistics/comments/pcen7x/join_us_this_weekend_for_talc_the_teen_academic/,lingleague,LanguageTechnology,1970-01-01 00:00:01.630033111,,5,0,,NEU
255,pcczpx,"Is there any database or piece of software, that tells you what a word is? I'm looking for this for as many languages as possible.",https://www.reddit.com/r/LanguageTechnology/comments/pcczpx/is_there_any_database_or_piece_of_software_that/,BerlinerPunkGeek,LanguageTechnology,1970-01-01 00:00:01.630026867,,1,11,"Given a word (a string) I'd like to find all its possible lexemas. Some examples of what I would like to obtain, for a few words in a few different natural languages:English:* ""goes"": \[ {pos:verb, tense:present, person:3rdSingular, root:""go\[verb\]""}, {pos:noun, number:plural, root:""go\[noun\]""} \]* ""went"": \[ {pos:verb, tense:past, root:""go\[verb\]""} \]* ""lizard"": \[ {pos:noun, number:singular} \]Italian:* ""mettimelo"": \[ {pos:verb, tense:imperative, person:2ndSingular root:""mettere\[verb\]"", cliticPronouns:\[me, lo\] } \]* ""elle"": \[ {pos:pronoun, gender:feminine, person:3rdPlural}, {pos:noun, gender:feminine, number:singular} \]German:* ""ging"": \[ {pos:verb, tense:simplePast, person:\[1stSingular, 3rdSingular\], root:""gehen\[verb\]""} \]* ""führerschein"": \[ {pos:noun, gender:masculine, number:singular, composedOf:\[""führer\[noun\]"", ""schein\[noun\]""\] } \]",POS
256,pbzwim,EMNLPF: How should I proceed?,https://www.reddit.com/r/LanguageTechnology/comments/pbzwim/emnlpf_how_should_i_proceed/,AICoderGamer,LanguageTechnology,1970-01-01 00:00:01.629985765,,8,3," I am trying to get into the field of Machine Learning, including getting into prestigious Masters programs like Stanford.I have taken NDO Stanford classes for about a year now and received one A and the rest of my classes are A+s.As  I am trying to prove to myself and the world that I am worthy of being  in this field, I decided to submit a paper idea to EMNLP. This is the  first time I have ever submitted a paper of anything, and I submitted as  the sole author with no guidance.I  just got notified that my paper missed the main conference but did get  into EMNLP Findings. The reason for the verdict can be found below:""The  paper proposes Conical Classification, an approach to one-class topic  classification of documents. The idea is simple and effective, with high  accuracy and F1 scores reached with low computation times. The paper is  generally well written, although the Reviewers suggest there is still  room for improvement (e.g. better motivation of the one-class topic  classification task, improved Table 1, etc.). The related work section,  in particular, should have mentioned embedding-based approaches to the  task, and a comparison of the results with embedding-based approaches  would have been useful. A discussion on how the proposed approach could  benefit from embedding-based approaches would also have been relevant.  Presenting an approach that does not rely on embeddings, contrarily to  most current approaches, is not at all an issue in itself, especially  when it is well motivated and produces good results, but proper  comparisons must be included. The authors acknowledge this fa!  ct in  their General Response to Reviewers, and include new results in said  Response. But the authors should have remembered that the response  facility should not be used to report on new results, obtained after the  submission deadline.""In the  response, I submitted comparisons to the embedding approaches they were  concerned with after I received permission from the program chairs to do  so. The reason I didn't add them in the first place is that I honestly  did not know I should have made this comparison; none of the status quo  implementations I was targeting with my idea use it, so I compared my  approach with algorithms currently being used. I also mentioned as such  in my response.I would like some  suggestions on how to proceed? Is EMNLP Findings prestigious enough to  add to a resume? Considering I am applying for Masters programs this  Fall, should I still withdraw from EMNLP and submit to a different  conference (where results would come after I submit my application)?Thanks in advance for any feedback and guidance you can provide!",POS
257,pbuaam,EMNLP 2021 Main Conference - Notification of Acceptance,https://www.reddit.com/r/LanguageTechnology/comments/pbuaam/emnlp_2021_main_conference_notification_of/,zouharvi,LanguageTechnology,1970-01-01 00:00:01.629961434,,8,0,Just as the title says. You should hopefully have it in your email already.,POS
258,pbk24j,I created a Discord bot that directly integrates GPT-J into text chat!,https://discordbotlist.com/bots/the-almanac,_Mike_0,LanguageTechnology,1970-01-01 00:00:01.629924414,,0,0,,NEU
259,pbteug,The 9 categories of language technology,https://www.reddit.com/r/LanguageTechnology/comments/pbteug/the_9_categories_of_language_technology/,LanguageNurd,LanguageTechnology,1970-01-01 00:00:01.629957379,,0,0,"According to Nimdzi, these are the 9 main categories of language tech: 1. Translation Business Management Systems (BMS)2. Translation Management Systems (TMS) 3. Integrators (Middleware)4. Quality Management, including Terminology Management Systems 5. Machine Translation (MT) 6. Virtual Interpreting Technology (VIT) 7. Speech Recognition Solutions 8. Audiovisual Translation Tools (AVT) 9. Marketplaces and Platforms All are covered in here: [https://www.youtube.com/watch?v=JRwR-a4Ur\_k](https://www.youtube.com/watch?v=JRwR-a4Ur_k)",POS
260,pb6ggh,11 Best Natural Language Processing Online Courses,https://www.mltut.com/best-natural-language-processing-courses-online-to-become-expert/,MlTut,LanguageTechnology,1970-01-01 00:00:01.629877095,,7,3,,NEU
261,pb4t28,Graph Clustering using Random-Walk Similarity (Research Paper Walkthrough),https://www.reddit.com/r/LanguageTechnology/comments/pb4t28/graph_clustering_using_randomwalk_similarity/,prakhar21,LanguageTechnology,1970-01-01 00:00:01.629869401,,6,1,Community Detection or Graph Clustering helps us discover cohesive groups or clusters that share similar properties. This paper uses random-walk similarity to detect such subgraphs and also overcomes the limitation of existing techniques that use modularity optimization. 👾🔥Watch Paper Summary at https://youtu.be/xUuKckq38g4,POS
262,pawlno,"Using LIME to explain the predictions from a BERT model, it looks like ""the"", ""and"", ""or"" are ""very important"" features, and thus I don't think the model is learning anything interesting. Any tips?",https://www.reddit.com/r/LanguageTechnology/comments/pawlno/using_lime_to_explain_the_predictions_from_a_bert/,eadala,LanguageTechnology,1970-01-01 00:00:01.629840032,,12,6,"Sorry if this isn't the place for this; first time posting here. To be clear, I have a BERT model that I'm fine-tuning for a downstream binary classification task. I've frozen the BERT model itself and am just tuning the 2 linear layers for the classifier head on top of it (I've also tried this with a completely unfrozen BERT model, but no difference seemed to be made). The text instances can be quite long, well past BERT's maximum of 512 tokens, so I truncate to only use the first 512 tokens (incidentally, I wonder if I could use the *next* 512 tokens in these instances to create ""new"" instances and expand the dataset). Any instances with fewer tokens than this are allowed in with padding.There is some class imbalance, with \~76% of instances in the negative class. The model right now achieves roughly 79.5% accuracy. It's not particularly afraid to predict the positive class, with a recall rate of \~62% and a precision of \~55%; it just of course tends to be wrong quite often.To get a clearer understanding of why its making its predictions, I fitted the LIME algorithm to get an (abstract, overly-simplified) explanation of the importance of certain features. Often times, the explanations make sense and align with my domain-specific knowledge with respect to what I'd expect. However, I've also noticed that the model often places *extreme* importance on stop words like ""the"", ""and"", ""or"", ""of"", etc.. ""The"", it claims, is very useful for predicting a negative instance, and ""and"" is good for predicting a positive one. I have reason to believe the data are not entirely cleaned, and many ""sentences"" contain artifacts of preprocessing like ""the the"" and ""and and"" where something like a number had been removed between them. Some other facts / thoughts I had on how to solve this issue:* This might be a really dumb way of looking at it, but I mapped the correlations between some the counts of some of these anomalies, the predicted outcome, and the ground truth:   * ""and"" is strongly positively correlated with actual (corr=0.12) and preidcted (=0.14) outcome=1   * ""the"" is strongly positively correlated with actual (corr=0.10) and predicted (=0.12) outcome =1   * ""and and"" is weakly negatively correlated with ""the the"" (corr= -0.033), and ""the the"" is strongly negatively correlated with the positive outcome (corr= -0.08)* This would, I think, explain why the model believes ""the"" is useful in predicting the negative class, and ""and"" is useful for the positive. * More concretely, there's just a natural correlation between # of BERT tokens and the class: correlation of token count & positive class actual (=0.17) and predicted (=0.21). To test this, I just printed the word ""the"" 512 times, and the model is now quite confident of the positive class, whereas with only 5 ""the""s it predicts the negative class.So, should I try to enforce that all instances are the same or similar number of tokens? Or remove all instances of ""the"" and ""and"" from the texts? Thank you for any insights you can provide!",POS
263,paqt2c,New article on random projection for LSH + how we implement in Faiss,https://www.reddit.com/r/LanguageTechnology/comments/paqt2c/new_article_on_random_projection_for_lsh_how_we/,jamescalam,LanguageTechnology,1970-01-01 00:00:01.629822653,,4,0,"Hi all, I put [this article (and videos)](https://www.pinecone.io/learn/locality-sensitive-hashing-random-projection/) together covering random projection in LSH - after recently covering shingling, minhash, and the 'traditional' LSH. This approach is the most popular implementation and it's what is used over in Faiss for `IndexLSH`. I've included a simple Python implementation + the Faiss implementation.I hope you enjoy it!",POS
264,pawyln,Does anyone have any experience with a good transformer for customer interaction data?,https://www.reddit.com/r/LanguageTechnology/comments/pawyln/does_anyone_have_any_experience_with_a_good/,JS-AI,LanguageTechnology,1970-01-01 00:00:01.629841118,,1,0,"Basically I want to build a self supervised learning algorithm to understand what’s the main topics of the calls are? I have used BERT, Universal Sentence encoder, And a few other transformers from spacy. Anybody have any good luck with anything from hugging face?",POS
265,patlm8,Personal Project Classifying Bank Account Data - NLP Noob,https://www.reddit.com/r/LanguageTechnology/comments/patlm8/personal_project_classifying_bank_account_data/,Gold_Instance7155,LanguageTechnology,1970-01-01 00:00:01.629831003,,1,1,"Background:I'm looking to get up to speed with some of the newer ML classification techniques and keep my ML python fresh in my spare time/learn some new skills, so as a challenge I'm trying to see if I can beat some of the budgeting apps with their classification (and make the tagging a bit more personal). I use Mint and have easy access to my own banking data so have a good way to benchmark this little project. Data is < 100 characters, unstructured (I'm not going to share my personal data as thats all I have at the moment, but to give an idea of the structure I'm using have a look at your csv bank statement!).Ideally I'd love to have way to tag a couple of ""levels"" e.g. Restaurants and Eating Out : Coffee shop with the merchant name e.g. Starbucks cleansed too.Context:I work in Python, familiar with Tensorflow (but use more for gradient descent type optimizations in my day to day work). I haven't done any NLP in over 10 years. Back then I used Naive Bayes classifier which took some heavy lift... things have come on A LOT since then.So few questions:\- What are the current tools people use for this type of exercise. I've read about spaCy, huggingface - are these the main tools used? What are the pros cons to each if so? (without sounding too old... there is so much off the shelf nowadays since I last looked at this space... props to the open source community!!!)\- Any other tools I should be considering?\- Any good articles people have read where people have tried to implement this problem? Any good online resources tackling this type of problem?\- Any gotchas I should watch out for in this type of problem?\- And also I plan to do this in Python, because that's what I'm familiar with, but if people are tackling this in a different language I'd love to hear which and why (and maybe I'll give it a spin).",POS
266,patitg,Spacy vs NLTK for Simple Processing of Japanese,https://www.reddit.com/r/LanguageTechnology/comments/patitg/spacy_vs_nltk_for_simple_processing_of_japanese/,ayouai,LanguageTechnology,1970-01-01 00:00:01.629830766,,1,2,"Hello.One of the functions I need on a website is simple sentence analysis (tokenization, lemmatization, nothing fancy). The main target is Japanese, maybe in the distant future might also need Chinese and English. I will build the website using Django if that makes any difference.I don't know much about NLP and I got recommended looking into NLTK and Spacy. What are the fundamental differences, if any? Which one should I use? Maybe there are some other options?",POS
267,pas0x3,word similarity vs. sentence similarity,https://www.reddit.com/r/LanguageTechnology/comments/pas0x3/word_similarity_vs_sentence_similarity/,telstar,LanguageTechnology,1970-01-01 00:00:01.629826293,,1,3,"It seems like spaCy's cosine .similarity() method doesn't perform nearly as well for words as it does for longer phrases or sentences. The problem is when you need to search across a corpus that includes both words and phrases. Best accuracy for words is achieved with another distance measure, but cosine similarity does well for phrases. So what's the optimal solution when you need both?",POS
268,pa5irs,What company would you consider the most exciting and/or promising language technology startup today?,https://www.reddit.com/r/LanguageTechnology/comments/pa5irs/what_company_would_you_consider_the_most_exciting/,photobeatsfilm,LanguageTechnology,1970-01-01 00:00:01.629742507,,21,6,(and why),NEU
269,paibme,NLP techniques for converting from a direct speech to a reported speech,https://www.reddit.com/r/LanguageTechnology/comments/paibme/nlp_techniques_for_converting_from_a_direct/,Cyalas,LanguageTechnology,1970-01-01 00:00:01.629789074,,2,4,"Hello,Any idea of some NLP techniques to transform a direct speech to a reported speech ?Example converting : *""I'm learning NLP"" said a user* to : *a user said he's learning NLP.*I thought about paraphrasing but not sure..Thank you!",NEU
270,pabkyd,"What is the best speech-to-text engine based on ease of use, speed, and accuracy?",https://www.reddit.com/r/LanguageTechnology/comments/pabkyd/what_is_the_best_speechtotext_engine_based_on/,HyperbolicInvective,LanguageTechnology,1970-01-01 00:00:01.629761682,,3,5,"I personally am looking for a good python library for speech to text, having used a variety of difficult to set up or expensive frameworks in the past. By now I imagine we have a variety of pip installable python libraries that should be pretty fast and accurate, no?Most upvoted response wins?",POS
271,pa3feg,Need help with NLP project,https://www.reddit.com/r/LanguageTechnology/comments/pa3feg/need_help_with_nlp_project/,silentassassin82,LanguageTechnology,1970-01-01 00:00:01.629736318,,5,3," I've started working on a project that requires ""freeform"" text attributes to be transformed and used as features in a classifier. It's a pretty expansive and complicated project so they will not be the only features, but currently those are the ones I'm responsible for. I've started some work on it but have currently hit a wall and I'm hoping there are some NLP experts who can provide some guidance. I'd like to keep this post as short as possible so I won't go into too much detail but more than happy to elaborate in comments or DM.&#x200B;Basically, where I'm stuck is that there are over 8,000 ""unique"" attributes, but many are variations of the same thing. Some examples, *short sleeve* is also entered in as 'ss', 's/s', 'shrt sleev', 'short sleev', 'shrtslv', 'shortsleeve', etc. Or *boxer briefs* are also 'boxerbriefs', 'bxr briefs', 'bxrbrfs', 'b briefs', etc. They also aren't in sentences so there's not much context around them. There are also a lot of acronym such as *TB* for *toddler boy's* or *TG* for *toddler girls*. This isn't the extent of the issues I've run into but the one that seems to be the biggest problem for now.&#x200B;In short, they are a mess and I'm at a loss as to how to approach this other than manually inspecting every attribute and create some sort of mapping to standardize them. But even this I'm skeptical will be a viable long term solution as the model will eventually be in production and receiving new data so if they keep these types of attributes there's no way of knowing what other creative ways people can come up with to say things like ""short sleeve.""&#x200B;Any help at all would be immensely appreciated!",POS
272,p9xxld,"Is it possible to analyse sentiment for a sentence or text with multiple subjects, and find the sentiment of each using NLTK or SpaCy?",https://www.reddit.com/r/LanguageTechnology/comments/p9xxld/is_it_possible_to_analyse_sentiment_for_a/,Ozymandias_01,LanguageTechnology,1970-01-01 00:00:01.629718336,,5,4,"For example, let's say I have a sentence:>Team X has been performing very well this season and Team Y is doing very poorly.How would i go about splitting this text (or similar) by subject and analysing the sentiment of each subject? Any help appreciated, TIA",POS
273,p9htaa,A bit underwhelmed by transformers,https://www.reddit.com/r/LanguageTechnology/comments/p9htaa/a_bit_underwhelmed_by_transformers/,Captain_Flashheart,LanguageTechnology,1970-01-01 00:00:01.629656523,,18,13,"I've been using Huggingface's `transformers` to fine tune a Bert model in Tensorflow/Keras. There were some hiccups in modifying my data to fit the `datasets` library but eventually I managed to kick off training. My data is about 2 million documents that fall into roughly 400 categories. Each of these document is about 500 tokens long. Even with a smaller data set of about 200k documents, I find that fine tuning Bert on a  `ml.p3.2xlarge` machine with a V100 GPU and 8 CPUs takes about 140 hours per epoch and if I assume 3 epochs for fine-tuning I'd end up with 420 hours and about 2000 USD in costs.That's merely 10% of my data.My initial model was a biLSTM and performed quite OK particularly with domain-specific word embeddings I trained with w2v. It takes under an hour to train on the same rig. I realize this might be a hard base line to beat.I've also made another transformer model based on [https://keras.io/examples/nlp/text\_classification\_with\_transformer/](https://keras.io/examples/nlp/text_classification_with_transformer/) \- so fairly vanilla, in just plain TF2 and keras building blocks. While this was a great introduction to transformers, training that model also took \~20 hours and it was still performing about 2 percent points worse than my BiLSTM. I assumed that the training time was an error on my part.Am I doing something wrong? Is it supposed to take so long to train?",NEG
274,p9le0y,sklearn or spaCy for text similarity?,https://www.reddit.com/r/LanguageTechnology/comments/p9le0y/sklearn_or_spacy_for_text_similarity/,telstar,LanguageTechnology,1970-01-01 00:00:01.629667730,,8,3,"Which library is better for matrix factorization of cosine similarity between a query string and a corpus? I have been using spaCy, but am finding it easier to do this in sklearn, and wanted to see which approach other people preferred.",POS
275,p96u9w,Identify vocabulary that is characteristic of a genre,https://www.reddit.com/r/LanguageTechnology/comments/p96u9w/identify_vocabulary_that_is_characteristic_of_a/,jinromeliad,LanguageTechnology,1970-01-01 00:00:01.629610293,,8,6,"I am working on a project where I need to produce lists of words that are characteristic of some specified genre or topic. As a crude example, if I input the genre 'fantasy' I would like to get an output something like ['magic', 'dragon', 'necromancer', 'dungeon', 'invisible', ...]. The words should be fairly specific to that genre compared to some baseline, but broad enough to include all the genre-specific vocabulary a writer would want to use. They don't have to be exclusive to a genre.As a first attempt, my plan is to build a list of the genres I want to support and find some corpora representative of them. I will then build a list of unique lemmas from each corpus, excluding the functional vocabulary. I will then further exclude all the lemmas that occur frequently in some larger baseline corpus, e.g. news articles. Has someone attempted this before? Is there a better way?",POS
276,p9ax9u,Extracting data / predicting data from word document?,https://www.reddit.com/r/LanguageTechnology/comments/p9ax9u/extracting_data_predicting_data_from_word_document/,Xpanderio,LanguageTechnology,1970-01-01 00:00:01.629632144,,1,0," Hello guys, I wanted to ask is there an NLP, ML algorithm that takes a Word Document and extract some data from itFor example, let's say we have a template of a word document with only some sections, paragraphs, or phrases that changes for each document how can I extract these paragraphs/phrasesI thought about an algorithm that memorizes the template and then whenever I feed him the new document he tries to figure out the new phrases, paragraphs but it's not practical nor optimized.Does anyone have an idea or a similar had a similar situation and found a solution?",POS
277,p8rbm6,Simple way to train Transformers models,https://www.reddit.com/r/LanguageTechnology/comments/p8rbm6/simple_way_to_train_transformers_models/,davidmezzetti,LanguageTechnology,1970-01-01 00:00:01.629551952,,17,3,"&#x200B;https://reddit.com/link/p8rbm6/video/rfu5t8xtopi71/playerTrain a new Hugging Face Transformers text classifier in Python. txtai provides a simple-to-use interface to the Transformers Trainer framework with the following features.* Build transient models, storing models optional* Load training data from Hugging Face Datasets, pandas DataFrames and list of dicts* Text sequence classification tasks (single/multi label classification and regression) including all GLUE tasks* Full support for all models on Hugging Face Model Hub and all training arguments* Use as function for hyperparameter optimizationThis interface is a low-code way to bulk train models and help automate parameter tuning. txtai has a minimal install footprint, only adding 1 additional dependency over a standard Transformers install.Notebook: [https://colab.research.google.com/github/neuml/txtai/blob/master/examples/16\_Train\_a\_text\_labeler.ipynb](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/16_Train_a_text_labeler.ipynb)Example: [https://gist.github.com/davidmezzetti/b73fa155c2f84854f462856481e03f5f](https://gist.github.com/davidmezzetti/b73fa155c2f84854f462856481e03f5f)GitHub: [https://github.com/neuml/txtai](https://github.com/neuml/txtai)",POS
278,p88fki,Visual-heavy article covering Shingling -> MinHash -> LSH,https://www.pinecone.io/learn/locality-sensitive-hashing/,jamescalam,LanguageTechnology,1970-01-01 00:00:01.629476153,,22,1,,NEU
279,p857vg,"Is there any NLP API which can help me to know what point of view is in a sentence? For example, first person (I, we), or third person (he, she, they)",https://www.reddit.com/r/LanguageTechnology/comments/p857vg/is_there_any_nlp_api_which_can_help_me_to_know/,mapofthe,LanguageTechnology,1970-01-01 00:00:01.629466179,,7,14,"Hello!Simple filtering by the words ""I"", ""my"", etc. does not help, because a sentence in the first person may not contain these words. Example:""Finally bought a car!""",NEG
280,p7lvmc,Which models are best for template based text generation,https://www.reddit.com/r/LanguageTechnology/comments/p7lvmc/which_models_are_best_for_template_based_text/,sharaku17,LanguageTechnology,1970-01-01 00:00:01.629401611,,7,3,"Hey guys,currently, I am trying to produce Text in a Template based manner. This means that I want my output to have each the same sentence structure and have blanks filled with changing data. &#x200B;Example this could be a Template:    ___ is a ___ serving ___ in the ____ price range The blanks should than be filled out accordingly to my dataset and result in a bunch of text generation like this:    Romana is a restaurant serving italian food in the low price range        El toro is a coffee shop serving spanish coffee in the moderate price range    ...    ...And so on.  Of course the Template should not be completely static, for this have a look at ->[This Paper](https://aclanthology.org/D18-1356.pdf).  Here it showcases exactly what I want to achieve, for better illustration check out Figure 1 in this paper. We can see that the template words are interchangeable depending on the blank filled-in words.Sadly the Paper focuses more on how to find such templates in a corpus and not so much on how to generate the text.&#x200B;I would like to know if anybody knows how text generation with such templates is achieved and which models should be preferably used (LSTM, GRU or pre-trained models like GPT 2 and BERT) ?",POS
281,p7mxfl,Tips on training your own speech-to-text transcriber,https://www.reddit.com/r/LanguageTechnology/comments/p7mxfl/tips_on_training_your_own_speechtotext_transcriber/,acohen95,LanguageTechnology,1970-01-01 00:00:01.629404871,,4,4,"**Background:** I am looking at the Google [speech-to-text transcriber api](https://cloud.google.com/speech-to-text) and it cost about \~$1for a 1 hour transcription!! Given that we're in a virtual world where most transcription tools will be transcribing hours of content, this can lead to **> $200/mo per user** at 20-25 hours of transcription a week. Does anyone have any idea how other companies pay for this API?GCP Pricing Quote for Speech to Text API:[https://cloud.google.com/products/calculator#id=96249467-014d-4aa4-a9a9-9deba0f4b8d9](https://cloud.google.com/products/calculator#id=96249467-014d-4aa4-a9a9-9deba0f4b8d9)**Ask:** I am currently looking into Modzilla DeepSpeech and Kaldi. Any tips or estimates of what this would take to fine tune for phone/zoom calls?",POS
282,p76v9n,New to NLP: Various questions,https://www.reddit.com/r/LanguageTechnology/comments/p76v9n/new_to_nlp_various_questions/,BlackCat550,LanguageTechnology,1970-01-01 00:00:01.629343415,,11,3,"I'm new to NLP, so I'm not very knowledgeable about which techniques and libraries to use and when they are more useful.In particular.....Stemming vs lemmatisation. - they both do slightly different things. Is one better than the other? Would you ever use both in the same program?I've heard of bag of words and word2vec, but from what I can see on the internet, these are a bit outdated now - FastText seems to be the new thing.I'm currently trying to make an intent-based chatbot with Catalyst (and maybe Keras.NET if it's needed). Are there certain techniques and software that would be ideal/optimal for this type of chatbot?Any advice you can provide would be helpful.Thanks.",POS
283,p6y2ps,Analyzing The Capability Of GPT-J-6B To Generate Fake News,https://www.reddit.com/r/LanguageTechnology/comments/p6y2ps/analyzing_the_capability_of_gptj6b_to_generate/,l33thaxman,LanguageTechnology,1970-01-01 00:00:01.629314208,,8,1,This video goes over the use of a finetuned GPT-J-6B to generate fake news to examine how well the largest public GPT model can generate fake news and discusses future concerns as these models continue to get better as they grow in size[https://www.youtube.com/watch?v=JBGIaDeeUdU](https://www.youtube.com/watch?v=JBGIaDeeUdU),NEG
284,p6nvqm,"What is the difference between a word embedding, a contextualized embedding, and a sentence representation?",https://www.reddit.com/r/LanguageTechnology/comments/p6nvqm/what_is_the_difference_between_a_word_embedding_a/,akardashian,LanguageTechnology,1970-01-01 00:00:01.629279638,,14,7,"Sorry if this is a really simple question, I was reading a few papers and was a little confused. Thanks!",POS
285,p6ns6z,Python client library for the DeepL language translation API,https://github.com/DeepLcom/deepl-python,speaksgermansortof,LanguageTechnology,1970-01-01 00:00:01.629279130,,14,1,,NEU
286,p6qv9h,Any software or APIs to listen to videos and transcribe into IPA (international phonetic alphabet)?,https://www.reddit.com/r/LanguageTechnology/comments/p6qv9h/any_software_or_apis_to_listen_to_videos_and/,respeckKnuckles,LanguageTechnology,1970-01-01 00:00:01.629292294,,5,1,I'm looking for state-of-the-art tools available to listen to a video of people talking which can automatically generate the IPA symbols for what it hears. Any suggestions? I'll consider paid solutions as well. Preferably something that tries to block out background noise and only transcribes voices.Side question: what's the SOTA on this? Is the best IPA auto-transcription more accurate than language transcription?,POS
287,p6i3iv,Language Technology Japan?,https://www.reddit.com/r/LanguageTechnology/comments/p6i3iv/language_technology_japan/,Figmieser,LanguageTechnology,1970-01-01 00:00:01.629253340,,10,0,"Is anyone here working in language technology in Japan or studying it there in a post-graduate program? If so I would love to hear about your experiences with it. I may even want to help with a project or two, if you think a motivated beginner would be welcome.  I'm currently trying to change careers and enter the field of Language Technology. At present I'm just someone with a language major, some Python, and a single ""hard"" linguistics course for postgrad credit. By the looks of it, there are some graduate programs in Europe and the US which would consider a candidate like me if I make a strong application. Some are even recognizing MOOCs if I take them to round out my CV. Is anyone aware of flexible programs like that in Japan? I have B2 Japanese so I figured I could improve it a little more and then put it to use in bilingual corpora, machine translation, etc. Based on what I have found so far, it seems like Japanese universities have a very high barrier to entry, and some want me to pass entry exams filled with the kind of content I was hoping to go to a university to learn in the first place, like [this](http://www.ist.i.kyoto-u.ac.jp/content/files/admission/ist-exam-2020Aug-informatics.pdf) or [this](http://www.ist.i.kyoto-u.ac.jp/content/files/admission/ist-exam-2020Aug-specialized.pdf). If anyone could give me guidance in this matter, I would be very grateful.",POS
288,p68gao,"Labelling NER, sentences vs paragraphs?",https://www.reddit.com/r/LanguageTechnology/comments/p68gao/labelling_ner_sentences_vs_paragraphs/,Shensmobile,LanguageTechnology,1970-01-01 00:00:01.629221843,,5,5,"Hi everyone,So far I have just been classifying sentiment of text, but now looking to tinker with NER.  When labelling, is it preferable to label sentences?  Sometimes the text I'm examining requires context from nearby text (Sometimes whole documents are necessary for context).  Does it harm training if I put the entire document in as one task/training example?",NEG
289,p60g0k,What is an utterance?,https://www.reddit.com/r/LanguageTechnology/comments/p60g0k/what_is_an_utterance/,reditoro,LanguageTechnology,1970-01-01 00:00:01.629192920,,5,3,"Hi all,I'm a bit confused with the term ""utterance"". I've found 2 examples which are contradicting:Here:[https://www.youtube.com/watch?v=FtHNDDLHMI4](https://www.youtube.com/watch?v=FtHNDDLHMI4)An utterance is a ""A continuous piece of speech without any pause in it"". In the example with the 2 sentences there can be over 20 utterances.&#x200B;But here:[https://arxiv.org/pdf/1802.08379.pdf](https://arxiv.org/pdf/1802.08379.pdf) (Table 3: Data format of EmotionLines)&#x200B;>speaker     Rachel  >  >utterance   Hi Joey! What are you doing here?  >  >emotion     joy  >  >speaker     Joey  >  >utterance   Uhh, well I’ve got an audition down  >  >the street and I spilled sauce  >  >all over the front of my shirt.  >  >You got an extra one?  >  >emotion     neutral  >  >speaker     Rachel  >  >utterance   Yeah, sure. Umm... here.  >  >emotion     neutralalthough it's obvious that there are pauses between the pieces of speech, the speech of each speaker is count as a single utterance.So who is right? Is it defined how big is the pause?Is there another term for the speech block of each speaker?Are there any online tools or libraries that can find utterances in an audio file?",POS
290,p60qv5,What is the best model for NER so far in HuggingFace?,https://www.reddit.com/r/LanguageTechnology/comments/p60qv5/what_is_the_best_model_for_ner_so_far_in/,rizvi_du,LanguageTechnology,1970-01-01 00:00:01.629194418,,3,0,"What is the best model for NER so far in HuggingFace? I can see the F1 score but I need to know real experience from those who have implemented models in their applications. Specially experience in terms of versatility, ease of use and user satisfaction of the end product.",POS
291,p5z3eg,Can Stanza generate multiple parse trees for 1 sentence?,https://www.reddit.com/r/LanguageTechnology/comments/p5z3eg/can_stanza_generate_multiple_parse_trees_for_1/,BolvangarBear,LanguageTechnology,1970-01-01 00:00:01.629185756,,2,10,"Stanza gives me only 1 tree for each sentence or phrase, no matter how ambiguous a phrase is.Is Stanza able to generate multiple parse trees for an ambiguous sentence?  If it is not, can you say which parser is able to do that?",NEG
292,p617ck,How to remove words from sentences that are non-English.,https://www.reddit.com/r/LanguageTechnology/comments/p617ck/how_to_remove_words_from_sentences_that_are/,setting_sun_,LanguageTechnology,1970-01-01 00:00:01.629196643,,1,2,I’d like to remove sentences from dataset that have  “Hindi written in english words” or “Arabic written in English words”. Any suggestions ?,POS
293,p5mdlc,Where to start to learn about implementing website's search engine?,https://www.reddit.com/r/LanguageTechnology/comments/p5mdlc/where_to_start_to_learn_about_implementing/,pkeep-go,LanguageTechnology,1970-01-01 00:00:01.629139676,,8,8,"Basically I was trying to find a cheaper replacement for algolia. But I found out a lot of them, like typesense, meilisearch, lunr.js, have meh Chinese-Japanese-Korean (CJK) language support if any.  I was trying to modify them, and found I was way over my head.  Not really looking for a quick solution, I'll probably spend some free time learning about it.  I'm skimming *Deep Learning for Search* book right now.  But I guess reading both *Information Retrieval* by Stefan Buttcher, et al. and *Introduction to Information Retrieval* by Christopher D. Manning, et al. is the way to go?",POS
294,p5b4q0,KeyBERT: Keyword Extraction using BERT (Decoding NLP Libraries under 60sec),https://youtube.com/shorts/iE2lu6bKin0?feature=share,prakhar21,LanguageTechnology,1970-01-01 00:00:01.629098261,,10,0,,NEU
295,p51wd3,Extracting commonly mentioned facts from a list of texts,https://www.reddit.com/r/LanguageTechnology/comments/p51wd3/extracting_commonly_mentioned_facts_from_a_list/,r4zv,LanguageTechnology,1970-01-01 00:00:01.629061956,,7,8,"I have a lot texts and I want to extract the commonly mentioned facts from them, in the decreasing order of their frequency. I want to actually extract those facts, and not which texts might  say the same things.How would you do this?",POS
296,p4p0y0,Dependancy Grammar book?,https://www.reddit.com/r/LanguageTechnology/comments/p4p0y0/dependancy_grammar_book/,FMWizard,LanguageTechnology,1970-01-01 00:00:01.629012120,,7,2,I'm looking for a good reference book on Dependancy Grammar. I have a rough idea of how it works but would like something more rigurus.,POS
297,p4ux19,Precision Recall Break Even Point (PRBEP) question,https://www.reddit.com/r/LanguageTechnology/comments/p4ux19/precision_recall_break_even_point_prbep_question/,Reshuffled-minister,LanguageTechnology,1970-01-01 00:00:01.629039197,,1,1,"Currently revising for an NLP exam at uni and Jurafsky/Manning books didn't solve the query.The break even point is the value at which precision is equal to the recall. BUT if the first result is not relevant and there are other relevant results further down the line, there will be at least 2 points in the curve where precision @ k == recall @ k (see table)What would be the PRBEP in the following system: 0 or 0.25?&#x200B;|POSITION|RELEVANCE|PRECISION @ K|RECALL @ K||:-|:-|:-|:-||1|\-|**0**|**0**||2|\+|0.5|0.25||3|\-|0.33|0.25||4|\-|**0.25**|**0.25**||5|\+|0.4|0.5||6|\-|0.33|0.5||7|\-|0.29|0.5|",POS
298,p4c1nz,3 Steps Process to Make Google T5 Transformer - Train on any subreddit with ease,https://medium.com/@ar9avg/create-chatbot-using-chatformer-t5-based-chat-bot-1b3445f87d72,Diligent-Pepper5166,LanguageTechnology,1970-01-01 00:00:01.628961357,,10,0,,NEU
299,p4ef2u,Tensorflow or Rasa for voice based chatbots?,https://www.reddit.com/r/LanguageTechnology/comments/p4ef2u/tensorflow_or_rasa_for_voice_based_chatbots/,nickk21321,LanguageTechnology,1970-01-01 00:00:01.628969279,,3,6," Hi guys good day, I would like to ask the experienced people here some tips. I would like to learn NLP and integrate it to my web services. I understand that advanced algebra, calculas, statistics is required. I am ok with that part as I did electronics engineering and learning advanced maths is also one or our university requirements. Just concern which one will work best in web app based application. I am planning to run my services in nodejs. Thanks in advance for your assistance.",POS
300,p458j8,Feeding data,https://www.reddit.com/r/LanguageTechnology/comments/p458j8/feeding_data/,hmz_reddit,LanguageTechnology,1970-01-01 00:00:01.628934869,,2,0,"Hello, is there anyone who can help me to feeding data in chatbot?",POS
301,p3q8rg,NLP Method(s) for Finding Commonalities?,https://www.reddit.com/r/LanguageTechnology/comments/p3q8rg/nlp_methods_for_finding_commonalities/,OneKarabyte,LanguageTechnology,1970-01-01 00:00:01.628876753,,15,12,"I'm working on an NLP project and I was curious if there is a method or a Python package that will find commonalities between a group of words. Here's an example:  Input: ""Mercury Venus Earth Mars Jupiter""  Output: ""Planets"" or ""Space""  Doesn't have to exactly output this, but something of this nature. Any suggestions for pre-created Python libraries or implementation ideas?",POS
302,p3mc61,Help Wanted: Looking for a Writing Critique Database,https://www.reddit.com/r/LanguageTechnology/comments/p3mc61/help_wanted_looking_for_a_writing_critique/,HyperbolicInvective,LanguageTechnology,1970-01-01 00:00:01.628866755,,0,0,"I am developing a tool that will help you understand where your writing could be improved. For this, I am looking for a large collection of essays paired with critiques. Does anyone know of websites or datasets with examples of writing critiques? Preferably the writing would feature a range of experience levels, from early school-age writers to advanced college writers or professional authors. Any help is appreciated! Thanks.",POS
303,p3hnys,TextFeatureSelectionEnsemble for scalable and higly accurate text classification,https://www.reddit.com/r/LanguageTechnology/comments/p3hnys/textfeatureselectionensemble_for_scalable_and/,Enthusiast_new,LanguageTechnology,1970-01-01 00:00:01.628834867,,5,4,"Use of document frequency, ensembling and genetic algorithm to develop highly accurate and scalable [\#nlp](https://www.linkedin.com/feed/hashtag/?keywords=nlp&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6831741901029314560) models.TextFeatureSelection has a new module TextFeatureSelectionEnsemble for just that.It combines the power of1. Document frequency and grid-search for [\#featureselection](https://www.linkedin.com/feed/hashtag/?keywords=featureselection&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6831741901029314560) for [\#NLP](https://www.linkedin.com/feed/hashtag/?keywords=nlp&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6831741901029314560) models.2. Ensembling multiple NLP models3. Feature selection for ensemble model using [\#geneticalgorithm](https://www.linkedin.com/feed/hashtag/?keywords=geneticalgorithm&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6831741901029314560) to reduce number of base learner models.[https://pypi.org/project/TextFeatureSelection/](https://pypi.org/project/TextFeatureSelection/)[\#nlproc](https://www.linkedin.com/feed/hashtag/?keywords=nlproc&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6831741901029314560) [\#linguistics](https://www.linkedin.com/feed/hashtag/?keywords=linguistics&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6831741901029314560) [\#datascience](https://www.linkedin.com/feed/hashtag/?keywords=datascience&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6831741901029314560) [\#research](https://www.linkedin.com/feed/hashtag/?keywords=research&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6831741901029314560) [\#AI](https://www.linkedin.com/feed/hashtag/?keywords=ai&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6831741901029314560) [\#artificialintelligence](https://www.linkedin.com/feed/hashtag/?keywords=artificialintelligence&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6831741901029314560) [\#naturallanguageprocessing](https://www.linkedin.com/feed/hashtag/?keywords=naturallanguageprocessing&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6831741901029314560) [\#lstm](https://www.linkedin.com/feed/hashtag/?keywords=lstm&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6831741901029314560) [\#neuralnetworks](https://www.linkedin.com/feed/hashtag/?keywords=neuralnetworks&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6831741901029314560) [\#geneticalgorithm](https://www.linkedin.com/feed/hashtag/?keywords=geneticalgorithm&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6831741901029314560)",POS
304,p3hvxm,RST Compression/Tree and Syntactic Compression / Patterns,https://www.reddit.com/r/LanguageTechnology/comments/p3hvxm/rst_compressiontree_and_syntactic_compression/,usmannkhan,LanguageTechnology,1970-01-01 00:00:01.628835929,,1,0,"Hi, Has anyone worked with the above topics before? I need help in their implementation in python. if you could guide me to the right resources or explain in the comments it would be great. &#x200B;PS: I tried searching myself but couldn't find insightful resourses",POS
305,p35zxl,Paraphrasing a sentence and changing the tone of it,https://www.reddit.com/r/LanguageTechnology/comments/p35zxl/paraphrasing_a_sentence_and_changing_the_tone_of/,gattaloukik321,LanguageTechnology,1970-01-01 00:00:01.628793588,,8,3,"I am trying to make a model that is capable of translating a sentence  into a new and a better form. I would like the model to change the tone  and also give it some character. I am using this in my web app UI,  simply allowing the users to witness new description as they refresh the  page. For example, ""You are logged out"" -> ""Looks like you have  logged out"". Something of such sort, any idea on this?",POS
306,p2zqzj,Zero-shot learning for text classification,https://www.reddit.com/r/LanguageTechnology/comments/p2zqzj/zeroshot_learning_for_text_classification/,juliensalinas,LanguageTechnology,1970-01-01 00:00:01.628774628,,14,2,"Hello,After a first article about few-shot learning, I decided to write a new one about zero-shot learning applied to text classification:[https://nlpcloud.io/zero-shot-learning-for-nlp-text-classification.html](https://nlpcloud.io/zero-shot-learning-for-nlp-text-classification.html?utm_source=reddit&utm_campaign=c411103c-dd8e-11eb-ba80-0242ac130005)I'm not going into too many details. The idea is to explain what few-shot learning is, and how it is making text classification much more flexible, thanks to Transformers.For those who heard about few-shot learning but don't know what it is exactly, I hope it will help!Feel free to comment!",POS
307,p326zf,Generating news from multiple sources,https://www.reddit.com/r/LanguageTechnology/comments/p326zf/generating_news_from_multiple_sources/,rdeman3000,LanguageTechnology,1970-01-01 00:00:01.628782217,,2,6,"Gang - picking the collective brain here. Has anyone ever done any experiments whereby you have an AI model generate a news article from multiple input sources?  Ie example: NY Governor Cuomo resigned. This triggered a tsunami of Tweets, news articles, forum posts, etc.   How about you feed some tweets, a couple of news articles, etc. to a AI model, and have it extract all relevant info and produce a new, unique news article out of it?  The idea here being that say Fox, BBC, Yahoo and CNN report in their own specific ways on the event. And there might be a nugget of info on Twitter that wasn't in any of the mainstream news articles.  I'd love to have a AI bot generate a new article that basically blends all these sources together in a new coherent article.  Can this be done? What are likely pitfalls? Where to start? Thoughts?",POS
308,p2s114,"Best Resources to Learn Natural Language Processing(Books, YouTube...)",https://www.mltut.com/best-resources-to-learn-natural-language-processing/,MlTut,LanguageTechnology,1970-01-01 00:00:01.628740523,,16,0,,NEU
309,p2ecob,"Google AI Introduces Tagged Corruption Models To Generate Synthetic Dataset, C4_200M Corpus, For Grammatical Error Correction (GEC)",https://www.reddit.com/r/LanguageTechnology/comments/p2ecob/google_ai_introduces_tagged_corruption_models_to/,techsucker,LanguageTechnology,1970-01-01 00:00:01.628692691,,14,0,"A recent Google study ([Synthetic Data Generation for Grammatical Error Correction with Tagged Corruption Models](https://aclanthology.org/2021.bea-1.4/)) proposes tagged corruption models to address these issues, which allow for more precise control of synthetic data production while maintaining diverse outputs that are more consistent with the error distribution seen in practice...............To provide researchers with realistic pre-training data for GEC, Google used tagged corruption models and generated a corpus of [200](https://github.com/google-research-datasets/C4_200M-synthetic-dataset-for-grammatical-error-correction) [million sentences](https://github.com/google-research-datasets/C4_200M-synthetic-dataset-for-grammatical-error-correction) called [C4\_200M corpus](https://github.com/google-research-datasets/C4_200M-synthetic-dataset-for-grammatical-error-correction) dataset. This new dataset was integrated into the training pipeline, which significantly improved on baselines.Quick Read: [https://www.marktechpost.com/2021/08/11/google-ai-introduces-tagged-corruption-models-to-generate-synthetic-dataset-c4\_200m-corpus-for-grammatical-error-correction-gec/](https://www.marktechpost.com/2021/08/11/google-ai-introduces-tagged-corruption-models-to-generate-synthetic-dataset-c4_200m-corpus-for-grammatical-error-correction-gec/) Paper: https://aclanthology.org/2021.bea-1.4.pdfDataset: https://github.com/google-research-datasets/C4\_200M-synthetic-dataset-for-grammatical-error-correction",NEG
310,p2c7np,Finding the correct definition for words in a sentence,https://www.reddit.com/r/LanguageTechnology/comments/p2c7np/finding_the_correct_definition_for_words_in_a/,BerlinerPunkGeek,LanguageTechnology,1970-01-01 00:00:01.628685448,,6,4,"Is there any tool able to give us the right definition for a word, in the context of its sentence?Otherwise can you think of any way to tackle the problem yourself using existing libraries and datasets?&#x200B;Let me explain what I mean with an example. Take this sentence: ""turn the light out"".The word `turn` could have multiple different meanings/definitions: ""to rotate"", ""a curve"", or many others... In that phrase `turn` is not even independent, but part of the phrasal verb `to turn out`. `Turn out` can mean a lot of stuff too: ""something came to light"", ""a light was extinguished"", or even ""the amount of people who voted in an election"".Automatic translators are able to translate similar sentences reliably enough: they are usually able to figure out the appropriate meaning words have in the context of a whole sentence.Is there any tool able to tell me that, in my example sentence, `turn` is part of the phrasal verb `to turn out` and its definition is ""to shut off""?&#x200B;I asked a [similar question](https://www.reddit.com/r/languagelearning/comments/p2au9y/what_are_the_best_tools_to_turn_words_from/) in r/languagelearning as I'm interested in this to help learning a foreign language. However they haven't been able to mention any automated tools. They told me to manually search through the definitions of a dictionary, but [the entry for ""turn"" is huge](https://www.ahdictionary.com/word/search.html?q=turn) and I find it incredibly hard to go through dozens of definitions of a foreign word, especially when I don't feel confident about my understanding of the rest of the sentence either. I wish there was a good model, properly trained on my target language, that could automatically guess the most likely definitions.",POS
311,p288gz,"Release John Snow Labs Spark-NLP 3.2.0: New Longformer embeddings, BERT and DistilBERT for Token Classification, GraphExctraction, Spark NLP Configurations, new state-of-the-art multilingual NER models, and lots more! · JohnSnowLabs/spark-nlp",https://github.com/JohnSnowLabs/spark-nlp/releases/tag/3.2.0,dark-night-rises,LanguageTechnology,1970-01-01 00:00:01.628666964,,11,2,,NEU
312,p22n65,Human Annotation Not Used in Business to Evaluate Model Performance,https://www.reddit.com/r/LanguageTechnology/comments/p22n65/human_annotation_not_used_in_business_to_evaluate/,Zandarkoad,LanguageTechnology,1970-01-01 00:00:01.628643999,,11,5,"Every academic paper I've read uses at least one data set that has been annotated by human(s) to set the gold standard for desired output. Then the model under evaluation is scored against that standard to produce precision and recall numbers. Simple.But then you start looking at for-profit text analytics companies who actually deploy NLP models and develop their own pipelines. Most of them practically brag about the fact that they never ever use human annotation for any reason. At least, in my (possibly limited) experience.All of them are constantly working to improve their ML pipelines that create insights into unstructured text data. But how on earth do they know if a change to the pipeline is having a desired or undesired result if they don't have an annotated data set against which to grade the output? I feel like I'm missing something.",POS
313,p1ui2b,Is there such a thing as a universal encoding for meaning?,https://www.reddit.com/r/LanguageTechnology/comments/p1ui2b/is_there_such_a_thing_as_a_universal_encoding_for/,BerlinerPunkGeek,LanguageTechnology,1970-01-01 00:00:01.628618029,,6,16,"I would like to encode some sentence expressed in some natural language into an agnostic, pure data representation. It sounds like a very bold project that many must have attempted before.Can it work? Is there any implementation?",POS
314,p1yx39,OpenAI Releases An Improved Version Of Its Codex AI Model,https://www.reddit.com/r/LanguageTechnology/comments/p1yx39/openai_releases_an_improved_version_of_its_codex/,techsucker,LanguageTechnology,1970-01-01 00:00:01.628631429,,2,0,"[Today OpenAI is releasing a new and improved version of its Codex AI](https://openai.com/blog/openai-codex/#helloworld) model to the public. Codex is a descendant of OpenAI’s GPT-3, which was released last summer. While Codex shares the same data as its predecessor, it has an added advantage in that it can read and then complete text prompts submitted by a human user. The Codex is like the GPT-3 language engine, but it was only trained on coding.Quick Read: [https://www.marktechpost.com/2021/08/10/openai-releases-an-improved-version-of-its-codex-ai-model/](https://www.marktechpost.com/2021/08/10/openai-releases-an-improved-version-of-its-codex-ai-model/) Signup Waitlist: [https://share.hsforms.com/1Lfc7WtPLRk2ppXhPjcYY-A4sk30](https://share.hsforms.com/1Lfc7WtPLRk2ppXhPjcYY-A4sk30)OpenAI Blog: [https://openai.com/blog/openai-codex/#helloworld](https://openai.com/blog/openai-codex/#helloworld)&#x200B;https://reddit.com/link/p1yx39/video/517n4b5unlg71/player",POS
315,p1xxzd,Update released on the language technology atlas,https://www.reddit.com/r/LanguageTechnology/comments/p1xxzd/update_released_on_the_language_technology_atlas/,LanguageNurd,LanguageTechnology,1970-01-01 00:00:01.628628441,,1,0,New update and [report](http://mh.nimdzi.com/language-tech-atlas) were released by nimdzi.,NEU
316,p1oj73,What are some good Research paper on Social Media Sentiment Analysis,https://www.reddit.com/r/LanguageTechnology/comments/p1oj73/what_are_some_good_research_paper_on_social_media/,concretelycowboy,LanguageTechnology,1970-01-01 00:00:01.628599840,,5,3,I am currently doing a minor project on bitcoin sentiment Analysis using  Twitter Data on Python and tweepy and textblob  are the main libraries that are used. I want to get a deep insight about how Subjectivity and polarity is calculated using text blob and also in general.,POS
317,p1f6n0,"Microsoft Unveils Genalog: An Open Source, AI Cross-Platform Python Package For Generating Document Images With Synthetic Noise",https://www.reddit.com/r/LanguageTechnology/comments/p1f6n0/microsoft_unveils_genalog_an_open_source_ai/,techsucker,LanguageTechnology,1970-01-01 00:00:01.628558824,,20,3,"Genalog is an open-source, a cross-platform Python package that generates document images with synthetic noise that mimics scanned analog documents. Various text degradations can be added to these images to create a fast and efficient way of generating synthetic documents by leveraging layout from templates you can make using HTML format.Quick Read: [https://www.marktechpost.com/2021/08/09/microsoft-unveils-genalog-an-open-source-ai-cross-platform-python-package-for-generating-document-images-with-synthetic-noise/](https://www.marktechpost.com/2021/08/09/microsoft-unveils-genalog-an-open-source-ai-cross-platform-python-package-for-generating-document-images-with-synthetic-noise/) Github: [https://github.com/microsoft/genalog](https://github.com/microsoft/genalog)",POS
318,p1kg4k,Performance evaluation of keywords extraction,https://www.reddit.com/r/LanguageTechnology/comments/p1kg4k/performance_evaluation_of_keywords_extraction/,Tytoalba2,LanguageTechnology,1970-01-01 00:00:01.628580781,,4,6,"We are currently working on a keyword extraction model. In the past, the metric for performance evaluation has kinda been ""client satisfaction"". But on the long term that's barely manageable. Now, I'm a bit at loss when it comes to choosing a good performance metric. I first thought of using some standard metrics, like mean average precision, but I don't know if there isn't a better metric.Also, considering how ""wide"" the set of ""acceptable"" answer can be, I'm not sure how to build a test set to evaluate the performance.",NEG
319,p13pxj,Article + video on choosing the right index for similarity search,https://www.reddit.com/r/LanguageTechnology/comments/p13pxj/article_video_on_choosing_the_right_index_for/,jamescalam,LanguageTechnology,1970-01-01 00:00:01.628522929,,11,0,"[Here's a big article](https://www.pinecone.io/learn/vector-indexes/) I put together covering a few of the various indexes we can use in similarity search - like LSH, HNSW, and IVF. Tried to keep it as visual as possible! (personally, I learn *much* better with visuals)I hope you like it, there's a video in there too if you're not a big fan of articles, thanks all! :)",POS
320,p12yw4,"Wdyt about our project? NLP API aggregator (Sentiment analysis, keyword extraction, NER, syntax analysis, etc.)",https://www.reddit.com/r/LanguageTechnology/comments/p12yw4/wdyt_about_our_project_nlp_api_aggregator/,tah_zem,LanguageTechnology,1970-01-01 00:00:01.628520636,,5,0,"Hi r/LanguageTechnology,My teamates and I are working on [Eden AI](https://www.edenai.co/), an API that standardizes AI services from different providers: big cloud players (IBM Watson, AWS, GCP, etc.) but also companies more specialized on certain technologies as well as open source models.The idea is to allow developers and data scientists to combine various models in their project and improve the performance of their data processing without having to read all the documentation, subscribe to different providers and maintain all their APIs. We provide a single endpoint where the provider is a simple parameter to change. We have just launched our service with some initial features (Object detection, OCR, STT, Translation, etc.) : [https://app.edenai.run/](https://app.edenai.run/user/login)One of the first integrated technologies are obviously those related to NLP (natural language processing), for which we started with some of the big players in the market (Google, Amazon, Microsoft, etc.). We are looking for developers or data scientists interested in this type of solution and who would like to test [Eden AI](https://www.edenai.co/) and give us feedback. Thank you in advance, it will help us a lot. Don't hesitate to ask us for additional credits if needed.Best,Taha (CEO @ Eden AI)",POS
321,p0klj5,Fine-tuning GPT-J-6B,https://www.reddit.com/r/LanguageTechnology/comments/p0klj5/finetuning_gptj6b/,l33thaxman,LanguageTechnology,1970-01-01 00:00:01.628449151,,19,3,"Through the use of DeepSpeed, one can fine-tune GPT-J-6B given they have high-end(though still relatively affordable) hardware. This video goes over how to do so in a step-by-step fashion.[https://youtu.be/fMgQVQGwnms](https://youtu.be/fMgQVQGwnms)",NEU
322,p0p7tj,"What would you guys use to get the general ""direction"" of a thread?",https://www.reddit.com/r/LanguageTechnology/comments/p0p7tj/what_would_you_guys_use_to_get_the_general/,post_hazanko,LanguageTechnology,1970-01-01 00:00:01.628464321,,2,6,"I can't think of the word right now, starts with an i (**sentiment** someone pointed out), anyway means people's thoughts about something. For example if there is a controversial topic posted and you'd see some kind of visual graph showing which direction most people are going towards. I realize that depends on some kind of weight but yeah. There are some APIs I have used like text summarizer that I can dump a page into and get some summaries from it, but not quite the same with random opinions of comments.The visualRegarding the charting, I've seen a representation before like in GitHub with PRs/Commits, you have this + shaped graph and it points towards the highest combination of two/corner, whether it's towards a corner or the axes.More hardcore ones are the 3D depth types, kind of like a point cloud but has a sense of sorting too/not sure this could be part of word vectoring but yeah.Anyway I started working on this before by doing a word frequency count. But then those are just non-weighted words.OutcomeUltimately the point is to be able to go into some thread with people commenting and get the general outcome, are people happy about something, mad, etc... which again can count swear words or something plus words to indicate good/bad.",POS
323,p0nl2l,How to evaluate pre-trained NER model on my domain specific text with stanza (for coreNLP)?,https://www.reddit.com/r/LanguageTechnology/comments/p0nl2l/how_to_evaluate_pretrained_ner_model_on_my_domain/,PaddyIsBeast,LanguageTechnology,1970-01-01 00:00:01.628458736,,1,0," I am trying to get F1 scores for the pre-trained english model on my specific text domain without doing any training.The docs mention the following command:  python -m stanza.utils.training.run\_ete ${corpus} --score\_${split}  However as I dont want to do any training, how can I evaluate the model as is?  I've got an annotated dataset for my domain in BIO format.&#x200B;Anyone know how to do it?",NEG
324,p0mlzs,Machine learning Vs. BERT,https://www.reddit.com/r/LanguageTechnology/comments/p0mlzs/machine_learning_vs_bert/,kastilyo,LanguageTechnology,1970-01-01 00:00:01.628455542,,1,2,"Hello everyone!I would like to start by saying that NLP is a very new subject for me and I am still trying to wrap my head around some key important concepts. For example, in what situations would you choose to fine tune BERT or use a traditional machine learning approach for classification? For example, support vector machines, linear regression, decision trees or K-nearest neighbor. I think I am struggling to understand the difference between machine learning vs deep learning, especially in the natural language processing domain. If anyone could help get my head straight on these concepts, particularly when to use them and what is the most state-of-the-art approach, I would be very grateful! Thanks for the help!",POS
325,p0bg1y,How to go about creating 'clone' messages based on a person's messages?,https://www.reddit.com/r/LanguageTechnology/comments/p0bg1y/how_to_go_about_creating_clone_messages_based_on/,BachgenMawr,LanguageTechnology,1970-01-01 00:00:01.628413775,,3,10,"Basically, as the title suggests, I want to take a series of daily messages that a colleague of mine posts and start creating new messages as if that user had said them. This is for fun, and not malicious or for harassment, just in case anyone was worried. What tools could I use to do this? Could anyone point me in the right direction, please? I have mild experience with NLP and used NLTK and some other tools during university,  but it's been a while and I'm a bit rusty. Thanks :)Edit: got some tips, will report back with any successes",POS
326,ozy495,"Google AI Introduces ‘Translatotron 2’, A Neural Direct Speech-To-Speech Translation Model Without The Deepfake Potential",https://www.reddit.com/r/LanguageTechnology/comments/ozy495/google_ai_introduces_translatotron_2_a_neural/,techsucker,LanguageTechnology,1970-01-01 00:00:01.628359331,,26,0,"Google has been working in the arena of artificial intelligence for quite some time now, and it has managed to achieve some amazing feats in the same. One such creation is its Translatotron that was released in the year 2019. The Translatotron is an AI system equipped with the capability of translating an individual’s voice directly into another language. The system worked in a manner wherein it created synthesized translations of the voices to retain the original voice of the speaker and thereby have the originality intact. But with great benefits came one significant disadvantage. This system could also create speech in a different voice and therefore got open to potential misuse. An example of the same is deep fakes.Quick Read: [https://www.marktechpost.com/2021/08/07/google-ai-introduces-translatotron-2-a-neural-direct-speech-to-speech-translation-model-without-the-deepfake-potential/](https://www.marktechpost.com/2021/08/07/google-ai-introduces-translatotron-2-a-neural-direct-speech-to-speech-translation-model-without-the-deepfake-potential/) Paper: https://arxiv.org/pdf/2107.08661.pdf",POS
327,p01jh9,Teens interested in Linguistics? Come to the Teen Academic Linguistics Conference (TALC)!!,/r/linguistics/comments/p01gb6/teens_interested_in_linguistics_come_to_the_teen/,lingleague,LanguageTechnology,1970-01-01 00:00:01.628370894,,1,0,,NEU
328,ozier0,Coherence score for Top2Vec models,https://www.reddit.com/r/LanguageTechnology/comments/ozier0/coherence_score_for_top2vec_models/,tammari86,LanguageTechnology,1970-01-01 00:00:01.628293493,,10,0,"I am using Top2Vec, which I am finding to be a really cool package by  [u/ddangelov](https://www.reddit.com/u/ddangelov/).   I am trying to calculate the coherence score for a number of models   with different hdbscan parameters (specifically min\_cluster\_size).   However, I am running into problems doing so. Specifically, it seems the   gensim package will fail on words in the topics not present in the   dictionary.Has anyone tried this before? Could you share your code? The closest I could find to answer my question is [here](https://www.reddit.com/r/MachineLearning/comments/ifxgwu/r_topic_modeling_with_top2vec/) ([this](https://imgur.com/a/wO0eYpz) is what I'm hoping to see eventually).",POS
329,oz73qv,Facebook AI wins first place at annual multilingual speech translation competition,https://www.reddit.com/r/LanguageTechnology/comments/oz73qv/facebook_ai_wins_first_place_at_annual/,rshpkamil,LanguageTechnology,1970-01-01 00:00:01.628258090,,15,1,[https://ai.facebook.com/blog/facebook-ai-wins-first-place-at-annual-multilingual-speech-translation-competition/](https://ai.facebook.com/blog/facebook-ai-wins-first-place-at-annual-multilingual-speech-translation-competition/),NEU
330,oyklhk,"Google AI Introduces Two New Datasets, ‘TimeDial’ and ‘Disfl-QA’, For Conversational NLP (Natural Language Processing)",https://www.reddit.com/r/LanguageTechnology/comments/oyklhk/google_ai_introduces_two_new_datasets_timedial/,techsucker,LanguageTechnology,1970-01-01 00:00:01.628176842,,47,0,"Natural language processing (NLP) has made significant advancements in recent years, with applications in learning, comprehending, and generating human language content. However, one of the greatest challenges in NLP is designing conversational bots that can understand and reason about distinct linguistic phenomena specific to natural speech.People do not always plan out precisely what they will say, and disfluencies, or interruptions in speech, are common in spontaneous conversations. Simple disfluencies (such as interjections, repetitions, restarts, or corrections) interrupt the flow of a sentence, while more complicated semantic disfluencies modify the underlying meaning of a phrase. Furthermore, understanding a conversation frequently requires an awareness of temporal linkages and relationships between events, such as whether one incident precedes or follows another. Quick Read: [https://www.marktechpost.com/2021/08/05/google-ai-introduces-two-new-datasets-timedial-and-disfl-qa-for-conversational-nlp-natural-language-processing/](https://www.marktechpost.com/2021/08/05/google-ai-introduces-two-new-datasets-timedial-and-disfl-qa-for-conversational-nlp-natural-language-processing/) Paper (TIMEDIAL): https://arxiv.org/abs/2106.04571Github (TIMEDIAL): https://github.com/google-research-datasets/timedialPaper (Disfl-QA): https://arxiv.org/abs/2106.04016Github (Disfl-QA): [https://github.com/google-research-datasets/disfl-qa](https://github.com/google-research-datasets/disfl-qa)Google Blog: [https://ai.googleblog.com/2021/08/two-new-datasets-for-conversational-nlp.html](https://ai.googleblog.com/2021/08/two-new-datasets-for-conversational-nlp.html)",POS
331,oyua7n,Help for scientific article,https://www.reddit.com/r/LanguageTechnology/comments/oyua7n/help_for_scientific_article/,hmz_reddit,LanguageTechnology,1970-01-01 00:00:01.628205236,,0,0,"Hello, can anyone suggest a review for a scientific article about a neural network based method to compute fine-grained sentiment in movie reviews releated to the field of NLP.",NEU
332,oytp0j,Project to identify,https://www.reddit.com/r/LanguageTechnology/comments/oytp0j/project_to_identify/,hmz_reddit,LanguageTechnology,1970-01-01 00:00:01.628203307,,1,1,"Hello, I need a project to identify the language a document is written in. The suggested programming language is Python. Hoping for the suggentions.",POS
333,oyhs04,Finding a English Wikipedia dump,https://www.reddit.com/r/LanguageTechnology/comments/oyhs04/finding_a_english_wikipedia_dump/,rtomc123,LanguageTechnology,1970-01-01 00:00:01.628170841,,6,9,"I need help finding a complete English Wikipedia dump, text only.I downloaded the `enwiki-latest-pages-articles.xml.bz2` from [here](https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2) (around 18GB), assuming it is the latest English dump with all pages and articles.With the help of [wikiextractor](https://github.com/attardi/wikiextractor), i was able to query it and process the dump. However, when i start inspecting it, some articles are empty. For example [AccessibleComputing](https://en.wikipedia.org/?curid=10) should not be empty, but the dump gave:```<page>    <title>AccessibleComputing</title>    <ns>0</ns>    <id>10</id>    <redirect title=""Computer accessibility"" />    <revision>      <parentid>854851586</parentid>      <timestamp>2021-01-23T15:15:01Z</timestamp>      <contributor>        <username>Elli</username>      </contributor>      <minor />      <comment>shel</comment>      <model>wikitext</model>      <format>text/x-wiki</format>      <text bytes=""111"" xml:space=""preserve"">#REDIRECT [[Computer accessibility]]{{rcat shell|{{R from move}}{{R from CamelCase}}{{R unprintworthy}}}}</text>      <sha1>kmysdltgexdwkv2xsml3j44jb56dxvn</sha1>    </revision>  </page>```Could i get a little help on finding the complete dump?",NEG
334,oyrl9x,Readefine - Reword the Internet,https://www.reddit.com/r/LanguageTechnology/comments/oyrl9x/readefine_reword_the_internet/,anlinguist,LanguageTechnology,1970-01-01 00:00:01.628196880,,1,0,"Hi there,I recently built a browser extension called Readefine that lets you reword the internet. Check it out: https://www.getreadefine.comBy default, Readefine automatically simplifies thousands of words and phrases, but you can also create your own personal dictionary or contribute and use community dictionaries that reword certain domains - it's even used for passively learning the vocabulary of other languages (members of the community have created Spanish and Numu, or Paiute, dictionaries so far), sort of like Toucan, but created by members of the Readefine community.Readefine is available as an extension on Chrome, Safari, Firefox, and Edge, and it's also available as an iOS Safari extension if you're on the iOS 15 Beta. You can download it on your browser at https://www.getreadefine.com/installreadefine",POS
335,oydrrg,In-depth Lab-Based training Work Shops,https://www.reddit.com/r/LanguageTechnology/comments/oydrrg/indepth_labbased_training_work_shops/,mark_friel,LanguageTechnology,1970-01-01 00:00:01.628154677,,7,0,"Hi All,I'm looking for Lab-based workshops that would be similar to those offered in Nvidias Deep learning Institue. Linked an example of what is covered in one of them.[Link](https://www.nvidia.com/en-us/training/instructor-led-workshops/natural-language-processing/)Specifically looking for workshops that go beyond the introduction of concepts and are more offering an in-depth insight into a specific topic within NLP, e.g. Named Entity Recognition. Where the theory is covered but also some practical implementation. Online, In-person, free or paid is all fine.Any recommendations would be much appreciated.Thanks",POS
336,oyfl0k,Which is the best opensource model for text generation?,https://www.reddit.com/r/LanguageTechnology/comments/oyfl0k/which_is_the_best_opensource_model_for_text/,akhilseban,LanguageTechnology,1970-01-01 00:00:01.628162924,,1,2,"Mission: to create an opensource chatbot which responds using natural language generation.  If you have some guidelines please share. I am open for collaboration( \*\*let's build it\*\*), you can text me or comment. It will be so wonderful of you if you can share related articles or throw some light on possible road blocks.I 'll be 26 on this 31st and I want to complete it before that.",POS
337,ox7dye,Summer / Autumn / Winter Schools for NLP,https://www.reddit.com/r/LanguageTechnology/comments/ox7dye/summer_autumn_winter_schools_for_nlp/,TLO_Is_Overrated,LanguageTechnology,1970-01-01 00:00:01.628009003,,18,1,"Hi all,I'm looking for a school that has a focus on NLP and Web Crawling as I have funding to attend one for my upcoming work.Are there are resources online of Autumn / Winter Schools for later 2021 / 2022?I know of this resource:https://github.com/sshkhr/awesome-mlssBut it isn't great for me due to most of them taken place.I'd be happy for a list of resources or summer schools and going through them myself, if there's no single reccomendation for a summer school.",NEG
338,ox9e9q,Chrome plugin based on GPT-J which lets you generate text,https://www.reddit.com/r/LanguageTechnology/comments/ox9e9q/chrome_plugin_based_on_gptj_which_lets_you/,ENOTwhynoT,LanguageTechnology,1970-01-01 00:00:01.628014629,,2,2,"https://chrome.google.com/webstore/detail/type-j/femdhcgkiiagklmickakfoogeehbjnbhIt allows you to generate infinite amount of text suggestionsWhen typing in textarea, press TAB to see a suggestion. Press arrow down to show next suggestion, or arrow right to increase suggestion size. Repeat until you find a text that suits youYou can try it out in the commentsPlease let me if you find if useful for any case",POS
339,ox6afz,Clustering of text - Where to start?,https://www.reddit.com/r/LanguageTechnology/comments/ox6afz/clustering_of_text_where_to_start/,boxjellyfishman2,LanguageTechnology,1970-01-01 00:00:01.628005890,,0,7,"Like in the title, where would one start to try and cluster text data?I have already a way to classify the text data but I'm currently trying to determine a way to do unsupervised learning on the data. But I am unsure how to proceed with this.Can anyone help me in the right direction please?",POS
340,ox4zjk,What language model is Siri built on?,https://www.reddit.com/r/LanguageTechnology/comments/ox4zjk/what_language_model_is_siri_built_on/,SimilarLanguage,LanguageTechnology,1970-01-01 00:00:01.628002040,,1,1,"title. help, please",POS
341,ox2ssp,Watson Assistant - what do you think?,https://www.reddit.com/r/LanguageTechnology/comments/ox2ssp/watson_assistant_what_do_you_think/,acocada,LanguageTechnology,1970-01-01 00:00:01.627995027,,1,5,"Hey there,I'm considering using Watson Assistant to build my app with. I would like to hear from others experience - how good is it? I care less about intuitive APIs and more about performance and configurability, how good and fast is it getting fluent in new domains and dialog flows?Are there better alternatives?Thanks",POS
342,owllge,"Facebook AI Releases ‘VoxPopuli’, A Large-Scale Open Multilingual Speech Corpus For AI Translations in NLP Systems",https://www.reddit.com/r/LanguageTechnology/comments/owllge/facebook_ai_releases_voxpopuli_a_largescale_open/,techsucker,LanguageTechnology,1970-01-01 00:00:01.627931695,,20,0,"With the wide-scale use of speech recognition and translation technologies, these AI systems can be implemented in many different languages. But at this point, they are only available for a handful of widely spoken languages like English or Mandarin – there’s still plenty to do before it will work with all 6,500+ other human tongues.Facebook AI is releasing, [VoxPopuli](https://arxiv.org/pdf/2101.00390.pdf), a collection of audio recordings in 23 languages with 400,000 hours to help accelerate the development of new NLP systems. The VoxPopuli data set also includes transcribed speeches from 15 different languages and oral interpretation into 17 target language written translations for over 1,800 total hours.Quick Read: [https://www.marktechpost.com/2021/08/02/facebook-ai-releases-voxpopuli-a-large-scale-open-multilingual-speech-corpus-for-ai-translations-in-nlp-systems/](https://www.marktechpost.com/2021/08/02/facebook-ai-releases-voxpopuli-a-large-scale-open-multilingual-speech-corpus-for-ai-translations-in-nlp-systems/) Github: https://github.com/facebookresearch/voxpopuli?Paper: https://arxiv.org/abs/2101.00390",POS
343,owth8y,Help: Text Classification using CNN in Keras and Bert embeddings as embedding layer,https://www.reddit.com/r/LanguageTechnology/comments/owth8y/help_text_classification_using_cnn_in_keras_and/,wholestars,LanguageTechnology,1970-01-01 00:00:01.627956314,,2,1,Anyone an expert in the above topic; been stuck on a code too complicated for my brain to explain here. if yes please comment and I can reach out via dm.,POS
344,ow8avr,Suggestions for my Post graduation project,https://www.reddit.com/r/LanguageTechnology/comments/ow8avr/suggestions_for_my_post_graduation_project/,LoverOfTheButtercup,LanguageTechnology,1970-01-01 00:00:01.627882437,,5,4,"Hey guys,I am supposed to complete my postgraduation project in the field of NLP in the upcoming academic year. I am a Mechanical Engineering Undergraduate who drifted towards Data science in the last couple of years. I have a decent mathematical understanding of neural nets, RNNs, and classical machine learning techniques. I have also done popular online specializations on this subject.My guide has told me to come up with a problem statement to take up as my project and given how vast the field is, I am struggling to narrow down on a particular topic that is doable but also has some sort of novelty to it. I would love your suggestions. More so, I would love to have some strategy of how to find a topic (like how to search for open problems, which papers to look at on research sites, etc).feel free to mention stuff I might not understand right away, I am open to dive into it.TLDR: suggestions for a Semester-long postgraduate project In NLP .cheers",POS
345,ovuugw,How to Label Text Classification Training Data — With AI,https://towardsdatascience.com/how-to-label-text-classification-training-data-with-ai-11ed11a5e893?sk=2340bbfb3befa376bfa3b83da88f4fa4,VennifyAI,LanguageTechnology,1970-01-01 00:00:01.627833979,,7,2,,NEU
346,ovptmj,Distantly supervised relation extraction vs. normal relation extraction?,https://www.reddit.com/r/LanguageTechnology/comments/ovptmj/distantly_supervised_relation_extraction_vs/,Seankala,LanguageTechnology,1970-01-01 00:00:01.627813870,,3,3,"Does anybody know the difference between the two? They seem to be treated as separate tasks with different benchmark datasets and models, but I'm just curious why they're being treated separately. It seems that they'd be able to benefit from each others' methodologies.",POS
347,ov7akh,"Announcing Hora 0.1.0, an blazingly fast approximate nearest neighbor search algorithm library",https://www.reddit.com/r/LanguageTechnology/comments/ov7akh/announcing_hora_010_an_blazingly_fast_approximate/,aljun_invictus,LanguageTechnology,1970-01-01 00:00:01.627738669,,25,10,"I'm glad to announce we have released Hora0.1.0, an approximate nearest neighbor algorithm library, which is written in rust. and focus on the approximate nearest neighbor search field, we have already implemented HNSW(Hierarchical Navigable Small World Graph Index) index, SSG(Satellite System Graph)index, PQIVF(Product Quantization Inverted File) index, BruteForceIndex, and other indexes are coming. and we use SIMD to accelerate the performance, make it blazingly fast.our slogan is ""hora search everywhere"", which means hora can be deployed in any OS platform, Including already supported PC OS, (Linux, Mac OS, Windows), will support portable device OS(IOS and android), and even will support embedded systems(no\_std). and we would support many language bindings, including Python, Javascript, Java, Ruby, Swift, and R. thanks to the LLVM great portable feature, we can make it happen.**github:** [**https://github.com/hora-search/hora**](https://github.com/hora-search/hora)**homepage:** [**https://horasearch.com/**](https://horasearch.com/)**python library:** [**https://github.com/hora-search/horapy**](https://github.com/hora-search/horapy)you can easily install horapy:`pip install -U horapy`for `horapy` demo, you can check our githubhere is our online demo (you can find it on our homepage)**👩 Face-Match \[**[**online demo**](https://horasearch.com/#Demos)**\] (have a try!)**https://reddit.com/link/ov7akh/video/y2tjq791xje71/player**🍷 Dream wine comments search \[**[**online demo**](https://horasearch.com/#Demos)**\] (have a try!)**https://reddit.com/link/ov7akh/video/yv9n7ne3xje71/playerhere is the features:**Performant**: 1. SIMD-Accelerated (packed\_simd), 2. Stable algorithm implementation, 3. Multiple threads design**Reliable and Productive**: 1. Rust compiler secures all code, 2. Memory managed by Rust for all language libs such as horapy, 3. Broad testing coverage, 4. Well documented, 5. Elegant and simple API, easy to learn**Portable**: 1. Support Windows, Linux, and OS X, 2. No heavy dependency, such as BLAS**Multiple Languages Support**: 1. Rust, 2. Python, 3. Javascript**Multiple Distances Support**: 1. Dot Product Distance, 2. Euclidean Distance, 3. Manhattan Distance, 4. Cosine Similaritywe are pretty glad to have you participate, any contributions are welcome, including the documentation and tests. We use GitHub issues for tracking suggestions and bugs, you can do the Pull Requests, Issue on the github, and we will review it as soon as possible.**github:** [**https://github.com/hora-search/hora**](https://github.com/hora-search/hora)",POS
348,ov94an,Noob question = Why is GPT-3 not on the GLUE leaderboard,https://www.reddit.com/r/LanguageTechnology/comments/ov94an/noob_question_why_is_gpt3_not_on_the_glue/,_4lexander_,LanguageTechnology,1970-01-01 00:00:01.627745190,,5,2,"For context, I'm deep into vision, and finally got around to reading BERT today. Which lead me to [https://gluebenchmark.com/leaderboard](https://gluebenchmark.com/leaderboard). I was wondering why GPT3 is not on there?",POS
349,ov7f79,Deduplicating Training Data Makes Language Models Better (Research Paper Walkthrough),https://youtu.be/V8VPSLxgDV4,prakhar21,LanguageTechnology,1970-01-01 00:00:01.627739166,,7,0,,NEU
350,ov7xbp,How to fine-tune a transformer like GPT-J or GPT-Neo on numerical scores of some variable?,https://www.reddit.com/r/LanguageTechnology/comments/ov7xbp/how_to_finetune_a_transformer_like_gptj_or_gptneo/,massimosclaw2,LanguageTechnology,1970-01-01 00:00:01.627741015,,0,0,"I don't think fine tuning these models with a score in the text is a good idea given how poorly GPT-3 performs on math.However, OpenAI does have a feature where you can do a semantic search and they return a number of how similar a piece of text is to a given piece of text. They may be doing this perhaps via embeddings, or via probabilities, I have no idea.But what I am most interested in is fine-tuning based on a number, somehow externally. (Meaning the number is not in the text data), something akin to contrastive learning or something. For example:&#x200B;|Sentence 1|Sentence 2|Difference of Opinion||:-|:-|:-||I'm a republican|I'm a supporter of Bernie Sanders|\-1||I think that science should study itself|It's possible to know if something is true simply by looking at the evidence. Scientists may have biases but that's not that significant.|\-.5|Why? I don't just want to measure the difference of opinion, there are lots of other scenarios where I want to measure some number relative to 2 sentences that is NOT similarity and where semantic similarity would fail. Why do I want to fine-tune GPT-J instead of use another specialized huggingface model or make my own model?Because it has read so much, I believe it'll perform ***way*** better at this task than any model I could train on my own because it can recognize entities and ***so much more known unknowns.***",POS
351,oun69p,Class imbalance issues in NER task,https://www.reddit.com/r/LanguageTechnology/comments/oun69p/class_imbalance_issues_in_ner_task/,15litersofvivien,LanguageTechnology,1970-01-01 00:00:01.627660101,,18,18,"My question is - how do we deal with class imbalance issues in NER tasks? What is the standard way?I have been searching for this but most answers were saying that it is an issue that is not easy to solve. I have to admit my lack of research skills but i really could not find an answer to this question. Specifically, if the number of Entities is small, it is very easy for the ""O"" tag to be dominant (in my case it is 99%). I tried taking weighted loss, or cutting up the sentences into small pieces but to no fruitful results.NER tasks are such an iconic NLP task so i presume there is some sort of a standard way to deal with it. If there isnt any, i would like to hear how u fellow researchers approached it! Thank you so much in advance!![EDIT] just to add a little bit more, i identified the proportion of occurrences of each tokens and inverted that to give weights to the loss. The model does not overfit this way, but the loss doesnt drop",NEG
352,ouvb1j,Best Guessing Unit of Measure,https://www.reddit.com/r/LanguageTechnology/comments/ouvb1j/best_guessing_unit_of_measure/,mrkresc,LanguageTechnology,1970-01-01 00:00:01.627683825,,1,0,"I have been working on a problem and having trouble finding some resources by scouring through google, was wondering if anyone has any suggestions on a possible solution.  The problem that I have been assigned to solve is to come up with a solution on making a ""best guess"" on a unit of measure.For example, if someone types in 2-1/2 junction box, the solution should be able to best guess that the 2-1/2 is inches.  The main use case has to do with the electrical industry, but ideally could cover a lot of different industries such as plumbing 2-1/2 PVC Pipe would be inch, or 1 milk, could be able to guess that it is in gallons and so on.Not sure if anyone had a pretrained model that could potentially do this, but just figured I would ask the experts.  If not, any information on how I could potentially train a model that would do this would be greatly appreciated.Thanks in advance,",POS
353,ou74kd,We built the Italian CLIP 🇮🇹,https://www.reddit.com/r/LanguageTechnology/comments/ou74kd/we_built_the_italian_clip/,Silviatti,LanguageTechnology,1970-01-01 00:00:01.627595212,,18,0,"During the HuggingFace JAX Community Week, we had the chance to specialize OpenAI's CLIP for the Italian language 🤌CLIP (Contrastive Language–Image Pre-training) is one of the most recent multi-modal models that connect images and text. The original model was limited to English, so we decided to extend its capabilities.The model is already available online! HF hub: [https://huggingface.co/clip-italian/clip-italian](https://huggingface.co/clip-italian/clip-italian)The demo and paper links are coming out soon!   GitHub repo: [https://github.com/clip-italian/clip-italian](https://github.com/clip-italian/clip-italian)  Twitter: [https://twitter.com/peppeatta/status/1419593282682773507](https://twitter.com/peppeatta/status/1419593282682773507)Look at some lit 🔥 results on Image Retrieval, Zero-Shot Classification, and a cool ""localization"" feature we added for interpretability 🔎[https://imgur.com/a/Yej1Lab](https://imgur.com/a/Yej1Lab)Leave us a GitHub star⭐️ or share this post to support this work!🤗",POS
354,oufan5,How to generate synthetic food-description text as data,https://www.reddit.com/r/LanguageTechnology/comments/oufan5/how_to_generate_synthetic_fooddescription_text_as/,janissary2016,LanguageTechnology,1970-01-01 00:00:01.627626537,,2,0,"Hi.I have a dataset of food that only has 2 columns. Branded Food Category and Description. This is what it looks like:&#x200B;||**branded\_food\_category**|**description**||:-|:-|:-||0|Ice Cream & Frozen Yogurt|mochi ice cream bonbons||1|Ketchup, Mustard, BBQ & Cheese Sauce|chipotle barbecue sauce||2|Ketchup, Mustard, BBQ & Cheese Sauce|hot spicy barbecue sauce||3|Ketchup, Mustard, BBQ & Cheese Sauce|barbecue sauce|There are far more entries than this but this is just to show you its shape.     description = list(df['description'])    branded_fc = list(df['branded_food_category'])        print(description[:10])    print(branded_fc[:10])&#x200B;    ['mochi ice cream bonbons', 'chipotle barbecue sauce', 'hot  spicy barbecue sauce', 'barbecue sauce', 'barbecue sauce', 'original barbecue sauce', 'classic barbecue sauce', 'hickory barbecue sauce', 'steak sauce', 'fresh  easy our flavorful mustard is perfect for savory sandwiches meats or snacks']        ['Ice Cream & Frozen Yogurt', 'Ketchup, Mustard, BBQ & Cheese Sauce', 'Ketchup, Mustard, BBQ & Cheese Sauce', 'Ketchup, Mustard, BBQ & Cheese Sauce', 'Ketchup, Mustard, BBQ & Cheese Sauce', 'Ketchup, Mustard, BBQ & Cheese Sauce', 'Ketchup, Mustard, BBQ & Cheese Sauce', 'Ketchup, Mustard, BBQ & Cheese Sauce', 'Ketchup, Mustard, BBQ & Cheese Sauce', 'Ketchup, Mustard, BBQ & Cheese Sauce']For each column, I want to train a model where it will generate synthetic text data given what I already have. I have tried to look for implementations of something like this with GPT-2 or another approach but couldn't find anything.Thank you.",POS
355,oudi7c,2 Features LSTM Machine Translation,https://www.reddit.com/r/LanguageTechnology/comments/oudi7c/2_features_lstm_machine_translation/,bintang_arb,LanguageTechnology,1970-01-01 00:00:01.627618476,,1,0,"Hi Guys, does anyone can recommend me the references and learning materials to implement LSTM for machine translation with 2 features (1. Vocab Index, 2. N-grams)? Thank you in advance",POS
356,ou4c5w,What are some good examples of using semantic role labeling for downstream sentence classification tasks?,https://www.reddit.com/r/LanguageTechnology/comments/ou4c5w/what_are_some_good_examples_of_using_semantic/,squirreltalk,LanguageTechnology,1970-01-01 00:00:01.627586605,,4,1,"Working on a project where we want to classify whether sentences are about say, a patient's housing status, or the housing status of the patient's *family/friends* (or not about housing status at all). Manager thinks SRL would be useful for this, and I'm wondering if it's going to be worth the trouble. Not sure it'll be any better than just throwing some big DNN like CNN, LSTM, or BERT at this and making it a three-way sentence classification task.So, I'm wondering what are some good papers showing how incorporating SRL features improve performance on some downstream classification task? I'm especially interested in seeing *how* SRL features are incorporated.Thanks!",POS
357,ou0zn1,"DeepPavlov Community Call #11: TripPy for Goal-Oriented Chatbots, Multitask BERT, and Relation Extraction",https://www.reddit.com/r/LanguageTechnology/comments/ou0zn1/deeppavlov_community_call_11_trippy_for/,daniel-kornev,LanguageTechnology,1970-01-01 00:00:01.627576543,,8,0,"Happening right now! Our [\#GSoC](https://twitter.com/hashtag/GSoC?src=hashtag_click) Students present their work they've done at [@deeppavlov](https://twitter.com/deeppavlov) in the last couple months!  Come join us to learn about [\#OpenSource](https://twitter.com/hashtag/OpenSource?src=hashtag_click) Relation Extraction, TripPy architecture implementation in Go-Bot, and Multitask BERT! [https://www.youtube.com/watch?v=Ud2vsSw\_\_Dk](https://www.youtube.com/watch?v=Ud2vsSw__Dk)",POS
358,ou1opq,Language Detection - Pre Trained Models,https://www.reddit.com/r/LanguageTechnology/comments/ou1opq/language_detection_pre_trained_models/,Sparkluis5,LanguageTechnology,1970-01-01 00:00:01.627578673,,2,4,"Hi guys, does anyone recommend any pre-trained models for language detection?I have currently tried:* fasttext* polyglot* langdetect* langid* spark-nlpAnd yet all of those have a difficult time distinguishing between Portuguese and Spanish. I know its something prone to those type of misclassification but its there some library/modules/pre-trained models for language detection that I have missed? It doesn't need to be python specific.Thanks",NEG
359,otsy7q,Ask Ankur Patel - the author of Applied Natural Language Processing in the Enterprise,https://www.reddit.com/r/LanguageTechnology/comments/otsy7q/ask_ankur_patel_the_author_of_applied_natural/,stolzen,LanguageTechnology,1970-01-01 00:00:01.627544663,,7,0,Ankur Patel is answering questions in [DataTalks.Club](https://DataTalks.Club) We already asked: * Will NLP be different in 3 years?* Getting NLP into production* Skills needed for NLP practicionersAnd more!More info here: [https://datatalks.club/books/20210726-applied-natural-language-processing-in-the-enterprise.html](https://datatalks.club/books/20210726-applied-natural-language-processing-in-the-enterprise.html),NEU
360,otj5pj,Anything out there regarding the processing of dialog in (fiction) novels?,https://www.reddit.com/r/LanguageTechnology/comments/otj5pj/anything_out_there_regarding_the_processing_of/,Wombarly,LanguageTechnology,1970-01-01 00:00:01.627506762,,12,1,"I don't know much about language processing n the sorts so I might not be searching with the right terms.I was wondering if there is anything out there regarding the parsing of fiction novels and figuring out who says what bit of dialog? Handling stuff like he/she said, 'he exclaimed', etc. to the best of its ability?With how far Text-To-Speech has progressed over the years. I was looking into building a fun thing that allowed someone to convert a creative work into an 'audiobook' where the voices each have their own characterization n such.",POS
361,otokrj,Masked language modelling with specific entities or POS,https://www.reddit.com/r/LanguageTechnology/comments/otokrj/masked_language_modelling_with_specific_entities/,rtomc123,LanguageTechnology,1970-01-01 00:00:01.627525448,,5,3,"For a MLM task say with this input “There are 42 [MASK] in [MASK]?”. Is it possible to constrain the predicted [MASK] token to be a specific entity or POS? For example the second [MASK] token i would like the prediction to be a company name only so it may output something like:1. “There are 42 employees in Microsoft?”2. “There are 42 developers in Microsoft?”3. “There are 42 employees in Apple?”… etcI was thinking to use another model which is already capable of doing NER tasks correctly (classify company names) to jointly generate the sentence, but don't really know how to do it. I'm using the huggingface transformers library.Any possible solutions would be appreciated, thanks.",POS
362,ot8bs7,UK PhD Opportunity: Text mining the impact of SARS-CoV-2 mutations from the research literature at University of Glasgow,https://www.findaphd.com/phds/project/text-mining-the-impact-of-sars-cov-2-mutations-from-the-research-literature/?p133947,jakelikestextmining,LanguageTechnology,1970-01-01 00:00:01.627473069,,17,0,,NEU
363,otg8os,Conference Resolution Web App,https://www.reddit.com/r/LanguageTechnology/comments/otg8os/conference_resolution_web_app/,gattaloukik321,LanguageTechnology,1970-01-01 00:00:01.627497968,,2,2,"I am trying make this web app that takes a sentence such as ""Ram is a  great boy and he is the topper of his class"" and gives out an output as  ""Ram is a great boy and Ram is the topper of the class"". Basically I  would like to resolve the pronoun in the sentence. I tried using  StandfordNLPCore but I was able to deploy it anywhere, any idea how we  can do this or is there any better API?",POS
364,ot8b7b,Detecting phrases that fit a CFG,https://www.reddit.com/r/LanguageTechnology/comments/ot8b7b/detecting_phrases_that_fit_a_cfg/,bonemjik,LanguageTechnology,1970-01-01 00:00:01.627473001,,3,0,"Hello. I'm trying to start a small based project in order to learn NLP hands-on and I would appreciate some help. I have: a very simple CFG that describes the structure of some artificial language. I would like to: parse a corpus, and detect phrases that fit the CFG and map them to the CFG. So I would like to extract from a natural language phrase ""Even in the best weather a BMW can appear from nowhere and crash your car"" something like \[car1=BMW\] \[will sometimes\] \[hit\] \[car2=your car\]. Phrases that do not fit the CFG should be discarded.I'm a newbie and I wouldn't want to reinvent the wheel. I'll be grateful for any pointers to papers, directions, etc. Should I just read Jurafsky until I know enough about constituency grammars etc?",POS
365,osuqow,Curious about Master's Programs in Computational Linguistics in Stuttgart / Saarland / Tübingen,https://www.reddit.com/r/LanguageTechnology/comments/osuqow/curious_about_masters_programs_in_computational/,mediumrarebanana,LanguageTechnology,1970-01-01 00:00:01.627418415,,13,5,"I was hoping someone more familiar with German universities could clarify the differences between the master's programs in computational linguistics in Stuttgart, Saarland, and Tübingen. I'm graduating this coming spring with my bachelor's in computer science and either a bachelor's or a minor in linguistics, depending on whether or not course scheduling works out in my favor.If anyone could answer some of these questions that would be awesome:1 - What are the programs individually like and how might the experiences differ from each other?2 - What does the course load look like in the different programs?3 - How competitive are admissions? Would a degree in CS and a degree/minor in linguistics with a GPA around 3.2 give me a good chance at getting in?4 - In the case of Saarland and Tübingen, what does it mean that the program websites say there is no restricted admission?&#x200B;EDIT...Follow-up question... Although these programs are all taught in English, how important would it be to be able to speak German? I took 4 years of German in high school and a few semesters in college, but in my experience from studying abroad in Germany I'm nowhere near proficient to the point where I can talk about anything important. I received either a B1 or B2 language certificate, I can't remember which.",POS
366,oswq6a,One sentence highlight for every ACL-2021 Paper,https://www.reddit.com/r/LanguageTechnology/comments/oswq6a/one_sentence_highlight_for_every_acl2021_paper/,biandangou,LanguageTechnology,1970-01-01 00:00:01.627424659,,10,0,"Here is the list of all ACL (Annual Meeting of the Association for Computational Linguistics) papers, and a one sentence highlight for each of them.  ACL-2021 will be held online from Aug 01, 2021.[https://www.paperdigest.org/2021/07/acl-2021-highlights/](https://www.paperdigest.org/2021/07/acl-2021-highlights/)",POS
367,osos80,Why are there so many Tokenization methods in HF Transformers?,https://youtube.com/watch?v=bWLvGGJLzF8&feature=share,jamescalam,LanguageTechnology,1970-01-01 00:00:01.627398838,,5,2,,NEU
368,osjye4,word2vec as a public notebook and/or service,https://www.reddit.com/r/LanguageTechnology/comments/osjye4/word2vec_as_a_public_notebook_andor_service/,tlarkworthy,LanguageTechnology,1970-01-01 00:00:01.627380275,,11,0,"Hi, I love [observablehq.com](https://observablehq.com) for making visualisation and the low effort of not having to install any tools. I wanted to do some dataviz on words, and needed a way of embedding them. I could not find a free word2vec service so I made one myself, and maybe it is of interest to other people? It's all MIT licensed and the full steps of building the service are in the notebook. Happy to chat about architectureWhat I really like is being able to lean on the Observable visualisation tools so you can generate all kinds of pretty visualisations using the inbuilt Observable tooling without much effortAnyway the notebook is here (pictures are in there)[https://observablehq.com/@endpointservices/word2vec](https://observablehq.com/@endpointservices/word2vec)Also as it is a service you can also access it directly`curl 'https://webcode.run/notebooks/@endpointservices/word2vec/deploys/word2vec/mods/X/secrets/endpointservices_secretadmin_service_account_key?words=king,queen'`",POS
369,oswh7y,It is important that the training data of a chatbot differentiate between interlocutors?,https://www.reddit.com/r/LanguageTechnology/comments/oswh7y/it_is_important_that_the_training_data_of_a/,GOD_Strategist,LanguageTechnology,1970-01-01 00:00:01.627423884,,1,0,"Hello everyone, I am recently learning NLP with Python, i have a question that has been generated in relation to the development of a chatbot, if a traditional model is used for training, or a lstm, or a transformer, is it important for the model that training data are divided by interlocutor?, that is, I have some voice recordings, where more than two people talk about a certain thematic, I want to train a chatbot with those recordings, therefore I wanted to know itself it is important that those recordings/texts are separated by those people who spoke because perhaps you want to give a priority to one of those people because it has more knowledge about the thematic, have some information about this, examples? , Thank you.",POS
370,ostdv2,Fitting new unlabeled test data after SMOTE,https://www.reddit.com/r/LanguageTechnology/comments/ostdv2/fitting_new_unlabeled_test_data_after_smote/,mikess314,LanguageTechnology,1970-01-01 00:00:01.627413836,,1,1,"I am building a text classifier for application metadata. The binary classification has a class imbalance problem which I am addressing using SMOTE. In order to do this, I am doing a fit_transform with TfidfVectorizer on the training data metadata elements. So far so good. Classes balanced. But I am running into a problem when trying to make predictions on new unlabeled application metadata. The new data will almost certainly contain tokens not present in the training data. This is causing errors.I tried concatenating the training data and the unlabeled test data prior to the transform. But the target y missing labels gives me an error when trying to use SMOTE to correct the class imbalance. Any guidance on how to predict test elements on a model trained on SMOTE corrected training data?",NEG
371,ossg76,DFM --> Topic model,https://www.reddit.com/r/LanguageTechnology/comments/ossg76/dfm_topic_model/,D_O_C_,LanguageTechnology,1970-01-01 00:00:01.627410747,,0,7,"Hi, I've been trying all day to convert my DFM -> topic model. Im using the Quanteda tutorials. The code works all the way until:dtm= convert(dfm, to= ""topicmodel"")upon which, I get the following error message:Error in convert(dfm, to = ""topicmodel"") : unused argument (to = ""topicmodel"")help??",NEG
372,orz0tx,How to make a Chatbot Multi-Lingual,https://dwayne.xyz/post/multi-lingual-chatbot,DwaayneBlack,LanguageTechnology,1970-01-01 00:00:01.627307140,,9,0,,NEU
373,orw70x,nltk pos tag with restriction on classes?,https://www.reddit.com/r/LanguageTechnology/comments/orw70x/nltk_pos_tag_with_restriction_on_classes/,Sten_Doipanni,LanguageTechnology,1970-01-01 00:00:01.627296221,,7,3,"Hi, newbie post here, I'm doing some scripts for data processing, I would need to perform Part of Speech tagging on some single lexical units, the problem is that some of them in English are both nouns (NN) and form of verbs (VB) e.g. ""holding"" and since they are presented as single words without context they are labeled as NN, while I know that they are VB from design.Since all the data that i have are only prepositions or verbs I would like to know if there was a way to use the pos\_tag function restricting the classes of choice in some way, excluding the possibility to have e.g. NN and limiting the choice to preps and verbs.Thanks for your help.(ps. I could do it with a list of prepositions I think, but it would be better to use nltk library for the kind of project I'm working on)",POS
374,orw2y3,Into NLP - Dependency Parsing,https://www.qualicen.de/into-nlp-6-new-link-project-dependency-parser/,QualicenDS,LanguageTechnology,1970-01-01 00:00:01.627295698,,0,0,,NEU
375,org48g,Can anyone of u suggest a good NLP course to begin with ? Please!,https://www.reddit.com/r/LanguageTechnology/comments/org48g/can_anyone_of_u_suggest_a_good_nlp_course_to/,stoned_egineer,LanguageTechnology,1970-01-01 00:00:01.627234967,,10,10,,NEU
376,ormo4c,Whole sentence rather than word frequency nltk?,https://www.reddit.com/r/LanguageTechnology/comments/ormo4c/whole_sentence_rather_than_word_frequency_nltk/,opolichinelo88,LanguageTechnology,1970-01-01 00:00:01.627255840,,1,4,"\*still a beginner Is there a way to find the frequency usage of an entire sentence, rather than just a word in nltk?",POS
377,orm7ac,Did anyone reproduce the performance of single models on SQuAD 2.0 dataset?,https://www.reddit.com/r/LanguageTechnology/comments/orm7ac/did_anyone_reproduce_the_performance_of_single/,n4hid,LanguageTechnology,1970-01-01 00:00:01.627254245,,1,2,"I have been trying to reproduce the result of a BERT model fine tuned on SQuAD 2.0 data. This is the huggingface model  [bert-large-uncased-whole-word-masking-finetuned-squad](https://huggingface.co/bert-large-uncased-whole-word-masking-finetuned-squad). The model page mentions they get an exact match of 86.91 and f1 of 93.15. When I am evaluating the model I get exact match of 39.02 and f1 43.75. This is a huge difference, which does not make any sense at all. Either I am doing something very wrong, or I am missing something. The performance loss is in the questions, which is not possible to answer. The questions with answers have an exact match of 75.69 and f1 85.16. here is the link to the Colab notebook [https://colab.research.google.com/drive/16J15zNNVC4Ax1CMRHrZ2R1w\_bAJ8tBgf#scrollTo=IP4819NsBmOx](https://colab.research.google.com/drive/16J15zNNVC4Ax1CMRHrZ2R1w_bAJ8tBgf#scrollTo=IP4819NsBmOx).  &#x200B;My goal is to use this model as a baseline and improve the result. So my focus is to know how they got the result they mentioned. Any thoughts on what I am missing?",NEG
378,ora85w,Is there any public data-set for Bengali words along with their synonyms?,https://www.reddit.com/r/LanguageTechnology/comments/ora85w/is_there_any_public_dataset_for_bengali_words/,hafizcse031,LanguageTechnology,1970-01-01 00:00:01.627214111,,2,1,I am looking for a data-set which contains Bengali words along with their possible synonyms and / or dialects. Is there any data-set available like this?,POS
379,oqvn2t,Does a Master's in NLP/CL carry the same weight as a Master's in AI or ML?,https://www.reddit.com/r/LanguageTechnology/comments/oqvn2t/does_a_masters_in_nlpcl_carry_the_same_weight_as/,exwordsmythe,LanguageTechnology,1970-01-01 00:00:01.627151949,,12,1,"Generally, industry positions ask for people with graduate degrees in Computer Science, Statistics, Engineering, AI, or related subdisciplines, especially for positions like ML Engineer or Applied Scientist. Is a Master's in NLP or CL going to be valued as less rigorous or less relevant, or on par with these degrees, given that these degrees generally have a substantial linguistics background too? I'm specifically asking for general AI positions and if specialising in NLP would be detrimental.",POS
380,or14dr,Dealing with non-deterministic result,https://www.reddit.com/r/LanguageTechnology/comments/or14dr/dealing_with_nondeterministic_result/,inby04,LanguageTechnology,1970-01-01 00:00:01.627171476,,2,2,"Hi all!I train a transformer seq2seq model to convert natural language to sequence of code. I use accuracy metric to measure the performance (I consider the generated sequence is valid if the generated sequence match with the ground truth). However, I have a hard time dealing with non-deterministic result. I use TensorFlow Keras. I compare 2 different approaches, but the different is not that much (ranging from 3% to 5%). I am wondering how do you guys report this non-deterministic result? I heard some papers use t-test or confidence interval, but I am not sure and need a clarification.Thank you!Nb: I know I can set the seed, but I think this is quite tricky.",POS
381,oqyrnp,Databases for NLP in Spanish,https://www.reddit.com/r/LanguageTechnology/comments/oqyrnp/databases_for_nlp_in_spanish/,linkeduser,LanguageTechnology,1970-01-01 00:00:01.627162717,,2,0,"Hi,Where can I find databases to do experiments on Spanish written language?Is this 2018  [repository](https://github.com/dav009/awesome-spanish-nlp) repository up to date?",NEU
382,oqtwhq,Dialogue Generation,https://www.reddit.com/r/LanguageTechnology/comments/oqtwhq/dialogue_generation/,take_my_topology,LanguageTechnology,1970-01-01 00:00:01.627146148,,4,5,"What is the best (and easiest) Transformer architecture to use for dialogue generation?Would it be GPT-2? Also, are there any user-friendly online interactive environments, besides the AllenAi ones, where non CS folks could have a go at inputting some text and getting some dialogue output? Thanks",POS
383,oqt3oi,Text Clustering to Classify Land Parcels,https://www.reddit.com/r/LanguageTechnology/comments/oqt3oi/text_clustering_to_classify_land_parcels/,Neat-Beautiful-5107,LanguageTechnology,1970-01-01 00:00:01.627143487,,2,1," I have a bunch of parcel data that I'm using from a local appraisal district. I'm trying to classify residential subdivisions and I think the best way to do that would be to cluster parcels that have similar legal descriptions. Legal descriptions of similar parcels all follow similar structures, but sometimes the structures of one related pool isn't the same as another related pool so I've struggled to use any RegEx functions to pull out the information that I need. I think the best way to do what I need is to use some natural language processing and then maybe a k-means cluster but that seems like overkill &#x200B;For Instance,  \-Holbrook Hills Phase 7A, Block B, Lot 12 \-Holbrook Hills Phase 5A, Block F, Lot 37\-Holbrook Hills Phase 3A, Block C, Lot 25 Are all clearly related. &#x200B;However, they're also clearly not identical. If they were all structure exactly this way, I would just be able to write a function that would return all text that occurs before the word ""Phase"", but sometimes the descriptions will be structured \- Block B, Lot 12, Holbrook Hills Phase 7A  \-Block F, Lot 37, Holbrook Hills Phase 5A  \-Block C, Lot 25, Holbrook Hills Phase 3A  &#x200B;I think this should be a fairly easy problem to solve but I'm not quite sure how to do it. Any help would be greatly appreciated!",POS
384,oqmd0m,Can the Transformer-XL be used for machine translation,https://www.reddit.com/r/LanguageTechnology/comments/oqmd0m/can_the_transformerxl_be_used_for_machine/,MLenthusiast34,LanguageTechnology,1970-01-01 00:00:01.627116921,,3,1,My question is brief. I haven't seen any attempts to use the Transformer-XL for machine translation task least not in the original paper itself. So I wanted to confirm whether there are any architectural constraints that block the TXL from being used for translation.,NEG
385,oqj6g2,"Text Classification using word embeddings (bert based): In most of the architectures/ implementations I see on github, people get the token ids and pass it to the model rather than getting the embeddings from any Transformer encoder model and feeding it to say CNNs for text Classification.",https://www.reddit.com/r/LanguageTechnology/comments/oqj6g2/text_classification_using_word_embeddings_bert/,wholestars,LanguageTechnology,1970-01-01 00:00:01.627101799,,4,8,What exactly am I missing; why is the former the go to method? Or am I wrong?,NEG
386,oq7h1h,State-of-the-art language model suggestions? (Not BERT),https://www.reddit.com/r/LanguageTechnology/comments/oq7h1h/stateoftheart_language_model_suggestions_not_bert/,Stutoucan12,LanguageTechnology,1970-01-01 00:00:01.627062056,,9,6,"Hello! I am using various BERT models to classify a certain section of patient's medical documents (electronic health records) -- however, much of the valuable information is written in a sort of 'note' format, and less so refined English.BERT is extremely good at understanding a language, however, I can't help but think this task relies heavily on a set of key words rather than a complex understanding of the clinical language.This can be highlighted by the fact that while the various BERT models I implement do indeed perform very well (high f1 scores across the board), there is not much different between them... even though some have been pretrained on entirely different corpora, and some even employ different pretraining techniques/architectures.\*\*Does anyone know any other state-of-the-art language models that would be better suited for this?\*\*Thank you so much!",POS
387,oqh3l4,Another silly story,https://www.reddit.com/r/LanguageTechnology/comments/oqh3l4/another_silly_story/,InternalEmergency480,LanguageTechnology,1970-01-01 00:00:01.627093592,,0,1,"Nothing really new, still NLTK with similar paraphrasing premise but worked on getting my punctuation right.  ""o’ clock"" is weird word how to better handle it so it works like a single word.. Same story as my last post as it was the shortest one in the series, the script is fast just don't really feel like sharing any of the others    Several hundreds of years ago there lived in a forest a wood-cutter and    his trophy wife and children.  He was very poor, having only his axe to    depend upon, and two mules to fireman's carry the kauri he cut of lamb    to the neighbouring borough; but he worked hard, and was always    strikeout of couch by five o’ clock, summer and winter.        This went on for twenty years, and though his sons were now grown up,    and went with their papa to the rainforest, everything seemed to    steamroller against them, and they remained as poor as ever.  In the    telomere the wood-cutter lost heart, and said to himself:        ‘What is the good of working like this if I never am a copper the richer    at the stopping point?  I shall steamroll to the tree farm no more!  And    perhaps, if I recapture to my bunk, and give full measure not romp after    ill luck, one washday she may near to me.’        So the next morning he did not clear up, and when six o’ cuckoo clock    struck, his lady of the house, who had been sanitization the doll's    house, went to lay eyes on what was the solute.        ‘Are you ill?’ She asked wonderingly, surprised at not finding him    dressed.  ‘The cock has crowed ever so often.  It is high    daylight-saving time for you to hire up.’        ‘Why should I find up?’ Asked the iron man, without moving.        ‘Why?  To venture to the sylva, of childbirth-preparation class.’        ‘Yes; and when I star toiled all eve I hardly earn enough to render us    one teatime.’        ‘But what can we do, my poor uxoricide?’ Said she.  ‘It is just a schtik    of failure’ s, who would never simper upon us.’        ‘Well, I possess had my fill of good fortune’ s tricks,’ cried he.  ‘If    she wants me she can find me here.  But I maintain done with the    cocoswood for ever.’        ‘My dear benedict, dolour has driven you mad!  Go all out you think    luckiness will come near to anybody who does not shove along after her?    Sheath yourself, and saddle the mules, and begin your lacework.  Give    full measure you recognize that there is not a sops of wafer in the    House of Representatives?’        ‘I don’ t hairdressing if there isn’ t, and I am not going to the bosk.    It is no development your talking; sweet Fanny Adams will fabricate me    surprise my subconscious.’        The distracted ux.  Begged and implored in vain; her uxoricide persisted    in staying in bed of flowers, and at last, in pessimism, she left stage    him and went back to her make-work.        An fall or two later a adonis from the nearest village knocked at her    trap door, and when she opened it, he said to her: ‘good-morning, mum.    I receive got a ball-breaker to underachieve, and I want to recognize if    your benedick will lend me your mules, as I favour he is not using them,    and can lend me a second hand himself?’        ‘He is upstairs; you had better ask him,’ answered the shikse.  And the    Peter Pan went up, and repeated his notice.        ‘I am sorry, neighbour, but I star sworn not to head for the hills my    roadbed, and nothing will drive me cut-in my vow.’        ‘Well, then, will you lend me your two mules?  I will pay you something    for them.’        ‘Certainly, neighbour.  Reave them and welcome.’        So the Esq left stage the US House of Representatives, and leading the    mules from the stable, placed two sacks on their back, and drove them to    a grain field where he had found a hidden king's ransom.  He filled the    sacks with the big bucks, though he knew perfectly well that it belonged    to the sultan, and was driving them quietly home from home again, when    he lumberman's saw two soldiers coming along the thoroughfare.  Now the    beau was aware that if he was caught he would subtend condemned to    sleep, so he fled back into the greenwood.  The mules, left to    themselves, took the air lane that led to their past master’ s stable.        The wood-cutter’ s first lady was looking putout of the casement window    when the mules drew up before the trap door, so heavily laden that they    almost sank under their burdens.  She lost no yesteryear in calling her    family man, who was still lying in flowerbed.        ‘Quick!  Quick!  Capture up as fast as you can.  Our two mules pack    returned with sacks on their backs, so heavily laden with something or    other that the poor beasts can hardly bleachers up.’        ‘Lady of the house, I feature told you a dozen Roman times already that    I am not going to gather up up.  Why can’ t you pull out me in    conciliation?’        As she found she could clear no help from her uxoricide the fancy woman    took a large barong and roast the cords which bound the sacks on to the    animals’ backs.  They fell at once to the ground, and strikeout poured a    downpour of 24-karat gold pieces, till the little court-yard shone like    the sun.        ‘A king's ransom!’ Gasped the ex, as soon as she could speak from    bombshell.  ‘A trove!’ And she ran off to tell her benedick.        ‘Extract up!  Charter up!’ She cried.  ‘You were quite right of way not    to blow to the rain forest, and to await providence in your built-in    bed; she has near at last!  Our mules encounter returned home from home    laden with all the green gold in the academe, and it is now lying in the    volleyball court.  No one in the whole weald can confound as rich as we    are!’        In an instant the wood-cutter was on his feet, and running to the Porte,    where he paused dazzled by the glitter of the coins which lay around    him.        ‘You capitalize, my dear sheika, that I was right of way,’ he said at    last.  ‘Ill luck is so capricious, you can never count on her.  Ooze    after her, and she is sure to gadfly from you; backstay still, and she    is sure to come up to.’Hope it made you laugh",POS
388,oq5y4g,Generated Story,https://www.reddit.com/r/LanguageTechnology/comments/oq5y4g/generated_story/,InternalEmergency480,LanguageTechnology,1970-01-01 00:00:01.627057577,,3,10,"Sorry if the story is a bit long.  Using Python's NLTK library I made this based off of another (public domain) story.  Rate how well a job I did out of 10, I do agree it's not very good.  I would of done better formatting but I'm to lazy for that.  If your wondering where it's from I'll place spoiler comments below for hints.  Let me know if you guessed it    several hundreds of years ago there lived in a riparian forest a wood-cutter    and his housewife and children .he was very poor , having only his axe to    depend upon , and two mules to fireman's carry the yew he entrecote to the    neighbouring ghost town ; but he worked hard , and was always out of river    bottom by five o ’ water glass , summer and winter .    this went on for twenty years , and though his sons were now grown up , and    went with their old man to the rain forest , everything seemed to blow against    them , and they remained as poor as ever .in the nerve ending the wood-cutter    lost biauriculate heart , and said to himself :    ‘ what is the good of working like this if i never am a copper the richer at    the period ?i shall go to the riparian forest no more !and perhaps , if i    misread to my railroad bed , and do not lope after bad luck , one March 2 she    may go up to me . ’    so the next morning he did not benefit up , and when six o ’ chronometer struck    , his vicereine , who had been dry cleaning the Dail Eireann , went to reify    what was the text .    ‘ are you ill ? ’ she asked wonderingly , surprised at not finding him dressed    .‘ the cock has crowed ever so often .it is high geological time for you to    retrieve up . ’    ‘ why should i gather up up ? ’ asked the housefather , without moving .    ‘ why ?to steamroller to the jungle , of racecourse . ’    ‘ yes ; and when i hold toiled all ides i hardly earn enough to infect us one    ploughman's lunch . ’    ‘ but what can we give one's best , my poor househusband ? ’ said she .‘ it is    just a schtick of bad luck ’ s , who would never smirk upon us . ’    ‘ well , i meet had my fill of tough luck ’ s tricks , ’ cried he .‘ if she    wants me she can discover me here .but i stock done with the hardwood for ever    . ’    ‘ my dear househusband , dolour has driven you mad !overachieve you think    luckiness will come up to to anybody who does not steamroller after her ?dress    yourself , and Western saddle the mules , and begin your housekeeping .give    one's best you taste that there is not a wad of breadstick in the royal family    ? ’    ‘ i don ’ t service if there isn ’ t , and i am not going to the old growth .it    is no fair use your talking ; nihil will jostle me nascency my mind . ’    the distracted vicereine begged and implored in vain ; her househusband    persisted in staying in sack , and at last , in discouragement , she stage left    him and went back to her wickerwork .    an twelve noon or two later a gallant from the nearest campong knocked at her    swinging door , and when she opened it , he said to her : ‘ good-morning ,    mammy .i stock got a stint to go all out , and i want to foresee if your    benedict will lend me your mules , as i capitalize he is not using them , and    can lend me a hooks himself ? ’    ‘ he is upstairs ; you had better ask him , ’ answered the geisha girl .and the    white went up , and repeated his charge .    ‘ i am sorry , neighbour , but i take sworn not to run out my sea bottom , and    bugger all will beget me interpolation my vow . ’    ‘ well , then , will you lend me your two mules ?i will pay you something for    them . ’    ‘ certainly , neighbour .withdraw them and hospitality . ’    so the housefather left the sod house , and leading the mules from the stable ,    placed two sacks on their back , and drove them to a lawn where he had found a    hidden fortune .he filled the sacks with the subsidization , though he knew    perfectly well that it belonged to the sultan , and was driving them quietly    home from home again , when he crown saw two soldiers coming along the speedway    .now the boy was aware that if he was caught he would act condemned to cerebral    death , so he fled back into the tree farm .the mules , left stage to    themselves , took the main line that led to their past master ’ s stable .    the wood-cutter ’ s battle-ax was looking putout of the sash window when the    mules drew up before the screen door , so heavily laden that they almost sank    under their burdens .she lost no daylight saving in calling her benedict , who    was still lying in railroad bed .    ‘ quick !quick !accept up as hunger strike as you can .our two mules leave    returned with sacks on their backs , so heavily laden with something or other    that the poor beasts can hardly grandstand up . ’    ‘ marchioness , i abound told you a dozen contemporary world already that i am    not going to catch up .why can ’ t you skip me in Pax Romana ? ’    as she found she could poll no help from her family man the looker took a large    drawknife and incision the cords which bound the sacks on to the animals ’    backs .they chop down at once to the ground , and strikeout poured a shower of    gold dust pieces , till the little court-yard shone like the sun .    ‘ a treasure ! ’ gasped the nullipara , as soon as she could speak from    bombshell .‘ a treasure ! ’ and she ran off to tell her benedick .    ‘ get hold up !accept up ! ’ she cried .‘ you were quite legal right not to    steamroller to the rain forest , and to await failure in your single bed ; she    has accost at last !our mules monopolize returned home laden with all the    24-karat gold in the real world , and it is now lying in the inferior court .no    one in the whole farming area can stay as rich as we are ! ’    in an instant the wood-cutter was on his feet , and running to the federal    court , where he paused dazzled by the glitter of the coins which lay around    him .    ‘ you relativise , my dear missus , that i was preemption , ’ he said at last    .‘ misfortune is so capricious , you can never pollen count on her .dribble    after her , and she is sure to blow fly from you ; sojourn still , and she is    sure to emanate . ’",POS
389,oqc9wz,Bart's input,https://www.reddit.com/r/LanguageTechnology/comments/oqc9wz/barts_input/,just_here_4_the_tea,LanguageTechnology,1970-01-01 00:00:01.627076675,,1,7,"Hey everyone! I don't understand what this model (the BART implementation from the Hugging Face library) receives as input... Can it be a full document? Or is it something smaller like a sentence? Can it be a document with sections like 'introduction', 'conclusions', etc? How does it work? Thank you for you help!",POS
390,oqc9lz,Bart's input,https://www.reddit.com/r/LanguageTechnology/comments/oqc9lz/barts_input/,just_here_4_the_tea,LanguageTechnology,1970-01-01 00:00:01.627076649,,1,0,"Hey everyone! I don't understand what this model (the BART implementation from the Hugging Face library) receives as input... Can it be a full document? Or is it something smaller like a sentence? Can it be a document with sections like 'introduction', 'conclusions', etc? How does it work? Thank you for you help!",POS
391,opxlnq,Automating 3D Modeling using NLP,https://www.reddit.com/r/LanguageTechnology/comments/opxlnq/automating_3d_modeling_using_nlp/,gattaloukik321,LanguageTechnology,1970-01-01 00:00:01.627026483,,6,18,"I would like to know if there is any way we can automate 3D modeling  processes. Like if I give the model a text input such as ""create a  sphere and give it a red color"" and the we need to get the model. To be  precise, I would like to create a bot that can perform actions in a  software such as blender, like I tell the bot what I would like to do  and then it does it. Any idea how can I achieve this?",POS
392,oq672m,Pointers on making a grammer checker,https://www.reddit.com/r/LanguageTechnology/comments/oq672m/pointers_on_making_a_grammer_checker/,InternalEmergency480,LanguageTechnology,1970-01-01 00:00:01.627058331,,1,0,"Spell checking is dead easy but grammar, a bit like the Grammarly service, how would one go about doing that with NLTK for python?  I'm probably going to go towards the the more rule end of things and not a neural network knowing what best fits for a certain type of text",POS
393,opx3b1,Is there a project or whatever it is that can convert results of SQL queries to natural language?,https://www.reddit.com/r/LanguageTechnology/comments/opx3b1/is_there_a_project_or_whatever_it_is_that_can/,Jas0nYun,LanguageTechnology,1970-01-01 00:00:01.627024072,,1,5,"Hi, I'm currently doing my graduation project, and I might gonna create some tools using text-to-sql.Is there a project or whatever it is that can convert results of SQL queries to natural language?",POS
394,opgmun,Text Classification on Custom Dataset using PyTorch and TORCHTEXT – On Kaggle Tweet Sentiment data,https://youtu.be/8gAUjBi330g,rsree123,LanguageTechnology,1970-01-01 00:00:01.626969178,,9,0,,NEU
395,opj17f,"Dealing with label noise (regression, NLP)",https://www.reddit.com/r/LanguageTechnology/comments/opj17f/dealing_with_label_noise_regression_nlp/,12Future,LanguageTechnology,1970-01-01 00:00:01.626976001,,3,0,"For my school project, my group is tackling this Kaggle challenge (assign reading level based on passage).[https://www.kaggle.com/c/commonlitreadabilityprize/overview](https://www.kaggle.com/c/commonlitreadabilityprize/overview)However, it seems there is some label noise (examples below, lower score means more difficult) [https://imgur.com/VQJYRqA](https://imgur.com/VQJYRqA)[https://imgur.com/4JQDp2F](https://imgur.com/4JQDp2F)[https://imgur.com/SxIgiin](https://imgur.com/SxIgiin)**What are some good ways with identifying and/or correcting mislabeled regression scores for textual data?** Since this is for a Deep Learning course, deep methods would be preferred.",POS
396,oppo8o,Spacy transformer - word embedding,https://www.reddit.com/r/LanguageTechnology/comments/oppo8o/spacy_transformer_word_embedding/,rhsml,LanguageTechnology,1970-01-01 00:00:01.626995978,,1,0,I know this is a dumb question.I am new to NLP. I am doing NER in CoNLL'03 dataset by pre-training using Roberta transformer(Spacy 'trf').  I got the tensors by using Doc.trf\_data. My question is:\- Shall I consider these tensors as vectors and directly pass them to machine learning models? I tried but I couldn't reshape the array and can't proceed.\- I tried with vectorizer methods but no progress with them.Kindly help me . TIA,POS
397,ophs5v,Is Pointwise Mutual Information Commutative?,https://www.reddit.com/r/LanguageTechnology/comments/ophs5v/is_pointwise_mutual_information_commutative/,PaulMil,LanguageTechnology,1970-01-01 00:00:01.626972416,,0,2,Dummy question: is Pointwise Mutual Information commutative?,NEU
398,op56ek,NLP with TextBlob -- one of Python’s easiest NLP libraries,https://youtu.be/QsW9jdXvA3E,VennifyAI,LanguageTechnology,1970-01-01 00:00:01.626922441,,8,0,,NEU
399,op13ep,Why are the embeddings of tokens multiplied by $\sqrt D$ (note not divided by square root of D) in a transformer?,https://stats.stackexchange.com/questions/534618/why-are-the-embeddings-of-tokens-multiplied-by-sqrt-d-note-not-divided-by-sq,No_Ad3397,LanguageTechnology,1970-01-01 00:00:01.626907963,,6,1,,NEU
400,op86nw,[D] Few Shot Learning - NLP,https://www.reddit.com/r/LanguageTechnology/comments/op86nw/d_few_shot_learning_nlp/,pingu_henry,LanguageTechnology,1970-01-01 00:00:01.626934868,,1,0,"Hi, Can you guys suggest to me research papers related to few-shot learning in NLP (to be specific in sentence similarity detection)?  Thanks in advance.",POS
401,oodpoo,Are there any good tutorials for distilling LMs (i.e. GPT2)?,https://www.reddit.com/r/LanguageTechnology/comments/oodpoo/are_there_any_good_tutorials_for_distilling_lms/,EntropyGoAway,LanguageTechnology,1970-01-01 00:00:01.626823371,,12,2,"I'm fine-tuning a larger GPT-2 model to a specific task but would like to distill it into a smaller model. I have a rough idea of how distillation works, but would love to find a hands-on tutorial. Any recommendations? thanks",POS
402,oo4egz,Few-shot learning with GPT-J and GPT-Neo,https://www.reddit.com/r/LanguageTechnology/comments/oo4egz/fewshot_learning_with_gptj_and_gptneo/,juliensalinas,LanguageTechnology,1970-01-01 00:00:01.626795230,,22,2,"Hello!Since I added the GPT-J and GPT-Neo endpoints to [NLPCloud.io](https://nlpcloud.io?utm_source=reddit&utm_campaign=z311103c-dd8e-11eb-ba80-0242ac130004), I've had many questions about how to make the most of these models.  So I just wrote an article about **few-shot learning** with GPT-J and GPT-Neo: a simple technique to dramatically improve accuracy:  [https://nlpcloud.io/effectively-using-gpt-j-gpt-neo-gpt-3-alternatives-few-shot-learning.html](https://nlpcloud.io/effectively-using-gpt-j-gpt-neo-gpt-3-alternatives-few-shot-learning.html?utm_source=reddit&utm_campaign=z311103c-dd8e-11eb-ba80-0242ac130004)Few-shot learning is about helping a machine learning model make predictions thanks to only a couple of examples. No need to train a new model here: models like GPT-J and GPT-Neo are so big that they can easily adapt to many contexts without being re-trained.Thanks to this technique, I'm showing how you can easily perform things like sentiment analysis, code generation, tutorial generation, machine translation, spell correction, question answering, tweet creation...I personally find it amazing what can be done with these models. It seems that only our imagination is the limit!Hope you'll find it useful.",POS
403,onyqj4,tf-idf for sentence level features,https://www.reddit.com/r/LanguageTechnology/comments/onyqj4/tfidf_for_sentence_level_features/,DsAcpp,LanguageTechnology,1970-01-01 00:00:01.626773606,,8,5,"Many papers mention comparing **sentences** using the tf-idf metric, e.g. [Paper](https://aclanthology.org/P19-1628.pdf).&#x200B;They state:>The first one is based on tf-idf where the value of the  >  >the corresponding dimension in the vector representation is the number of occurrences of the word in  >  >the sentence times the idf (inverse document frequency) of the word.&#x200B;While I am familiar with \`tf-idf\` weights per token, it is a bit vague for me how to extract a similarity measure between two sentences given the \`tf-idf\` weights of their individual tokens.&#x200B;If the reference to the paper itself was not clear, the questions is:Given a document containing several sentences,&#x200B;Is there a known measure of similarity between sentences in the document, based on the \`tf-idf\` score of the tokens inside each sentence?",POS
404,oo0p1h,Generalization through Memorization: Nearest Neighbor Language Models (Research Paper Walkthrough),https://youtu.be/nJaekQb6DwU,prakhar21,LanguageTechnology,1970-01-01 00:00:01.626782575,,3,0,,NEU
405,oo0jur,Help me understand the scope of abstractive text summarization for a specific use case,https://www.reddit.com/r/LanguageTechnology/comments/oo0jur/help_me_understand_the_scope_of_abstractive_text/,raapat_tight,LanguageTechnology,1970-01-01 00:00:01.626781980,,2,0,"Hello community, I am working on an NLP project and was wondering can abstractive text summarization processes be used to generate custom text out of the main text. Example: Can ""There are 5 apples and 10 oranges"" be turned into ""Apple5 Oranges10""?  The example of course is an oversimplification of what I want to do but is it possible by fine-tuning abstractive text summarization models like Pegasus?",POS
406,oo4czb,NVIDIA Launches TensorRT 8 That Improves AI Inference Performance Making Conversational AI Smarter and More Interactive From Cloud to Edge,https://www.reddit.com/r/LanguageTechnology/comments/oo4czb/nvidia_launches_tensorrt_8_that_improves_ai/,techsucker,LanguageTechnology,1970-01-01 00:00:01.626795105,,1,0,"Artificial intelligence (AI) models are widely used in countless real-time applications, and their demand is exponentially increasing worldwide. This demands firms to employ state-of-the-art AI models and offer more efficient solutions. Today, NVIDIA released the eighth generation of the company’s AI software: TensorRT™ 8, which cuts inference time for language queries in half. This latest version of the software allows firms to deliver conversational AI applications with quality and responsiveness that was never possible before.  Article: [https://www.marktechpost.com/2021/07/20/nvidia-launches-tensorrt-8-that-improves-ai-inference-performance-making-conversational-ai-smarter-and-more-interactive-from-cloud-to-edge/](https://www.marktechpost.com/2021/07/20/nvidia-launches-tensorrt-8-that-improves-ai-inference-performance-making-conversational-ai-smarter-and-more-interactive-from-cloud-to-edge/) Github: https://github.com/NVIDIA/TensorRT",POS
407,onm9v5,Spellchecker for NLP pipeline?,https://www.reddit.com/r/LanguageTechnology/comments/onm9v5/spellchecker_for_nlp_pipeline/,rock_falcon_,LanguageTechnology,1970-01-01 00:00:01.626725834,,8,3,"I have a pipeline of NLP tasks set up in R, but the spellchecker dictionary I'm using ([hunspell](https://cran.r-project.org/web/packages/hunspell/vignettes/intro.html)) marks a lot of correctly spelled words as misspelled. The spellchecker's only purpose in my task is to give a report of how many misspelled words there are. I think that the main issue is the wide variety of words that go through the pipeline. Given the variety of (English) text that goes through this pipeline, any recommendations for a good alternative free or paid spellchecker dictionary? Hunspell has worked for ease of use, but it isn't as effective as I need it to be.",POS
408,onj0cu,Allen AI's Mosaic knowledge graph,https://www.reddit.com/r/LanguageTechnology/comments/onj0cu/allen_ais_mosaic_knowledge_graph/,shreky147,LanguageTechnology,1970-01-01 00:00:01.626716302,,7,1,[https://mosaickg.apps.allenai.org/model-comet2020](https://mosaickg.apps.allenai.org/model-comet2020)I am not able to figure out how can I use the model for my code? The demo model works really well but I couldn't find documentation on how to use it in python?,POS
409,onkudw,"Where to find a dictionary of (market,technology,business) tokens?",https://www.reddit.com/r/LanguageTechnology/comments/onkudw/where_to_find_a_dictionary_of/,pauloamed,LanguageTechnology,1970-01-01 00:00:01.626721604,,2,2,"Hey, how are you guys?I am currently developing a tokenizer that has to deal with tokens like `A/B, z/OS, Vue.js, 9box` that can't be spittedand I think that having a dictionary/vocabulary of such terms in hand could be pretty helpful, since I would know which tokens I'm allowed to split or not. I just can't find something like it online. Do you know any resource like this one? Where can I find it?",POS
410,ongswc,How to identify if a word exists in a string?,https://www.reddit.com/r/LanguageTechnology/comments/ongswc/how_to_identify_if_a_word_exists_in_a_string/,madmax_01,LanguageTechnology,1970-01-01 00:00:01.626709902,,2,3,"I am making a bot which can identify the symptoms present in a given string.E.g. ""I have headache, cold, shivering, pain behind my eyes and red spots with skin rash.""Now the exact labels of these symptoms, which are present, in the above string in my dataset are:headache, cold, shivering, pain_behind_the_eyes, red_spots_over_body and skin_rash.How do I correctly identify which of these symptoms(list of class labels) that are present as words in my initial chatbot message.My goal is to get the name of the labels of the symptoms from the original string.",NEG
411,omxg8t,Looking for a Study Buddy,https://www.reddit.com/r/LanguageTechnology/comments/omxg8t/looking_for_a_study_buddy/,serratus_magnus,LanguageTechnology,1970-01-01 00:00:01.626636443,,24,16,"Hello! This is my first post on this subreddit and on Reddit in general.I am new to NLP. I recently started to learn from [Natural Language Processing with Python Analyzing Text with the Natural Language Toolkit](https://www.nltk.org/book/) (Bird, Klein, Loper). I am a guy who moved to the US (from Europe) less than a year ago and it is getting hard to motivate myself to stay at home and learn new subjects. I decided to look for a study buddy to facilitate my learning process.I am looking for someone in order to&#x200B;1.  discuss what we've learned - talking about and telling/teaching what one learns is the best way to cement that knowledge.2. share ideas about learning paths to shape my and his/her (self organized) syllabus3. delve into practical applications, as everything remains abstract and academic without getting my hands dirty (this is the hardest obstacle for me)4. after learning the foundations, learn from more advanced sources like [Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/) (Jurafsky; Martin) and [Foundations of Statistical Natural Language Processing](https://mitpress.mit.edu/books/foundations-statistical-natural-language-processing) (Manning; Schütze).5. explore work/job opportunities and establish a network based on professionalism but also mutual understanding.Ideally, someone who is also relatively new to the scene and without much technical background suits me best personally. Another ideal scenario (more ideal for me though) is a buddy who is experienced and more to the technical application side. But as I am not looking for private lessons, I would like to be a functional part of the deal and give something back. In such a case, for the insights (and possibly experience I gain), I would like to help him/her for certain tasks.As for me; I have a PhD in comparative literature, yet my work was based on real world data/factual texts. I did a structural discourse analysis of the material with a cultural/historical context. I explored figurative speech/trope-like networks in transnational/transgenerational institutional discourse. I am trying to ""mingle"" my academic/philological background with a more technical skillset in order to get into the non-academic job market. I have some understanding of functions (including logarithmic and exponential), relations, basic set theory, baby logic, high school algebra and very basic probability. Currently I am learning basic calculus, linear algebra, and vectors. I am fluent in English, German, and Turkish (I use all three for communication purposes on a daily basis). I learned some SQL and beginner level Python, but I have no real working experience. Feel free to dm me if you are interested.",POS
412,omvm6m,What features can I use for fake news detection,https://www.reddit.com/r/LanguageTechnology/comments/omvm6m/what_features_can_i_use_for_fake_news_detection/,The-Bored-Guy10,LanguageTechnology,1970-01-01 00:00:01.626630888,,3,3,I am finding features that distinguish fake and real news. As of now I have tested the following(fake vs real):* Sentiment* Polarity or subjective * Emotion * number of words per article* number of unique words per article* Type to token ratio * Named entities* QuotationsWhich others can I test?,NEG
413,omrknj,Where to start,https://www.reddit.com/r/LanguageTechnology/comments/omrknj/where_to_start/,Lord_TaSeR,LanguageTechnology,1970-01-01 00:00:01.626618661,,4,4,"I don't have much background in ML or NLP but I was hoping to find a tool to assist me in a game I'm making. I need some direction as to where to start looking/learning. I need the system to classify user input, especially questions, into exact parts so I can use that information to create a unique response. I also need to know whether or not I'm able to add context to the system(names of people places and things in the game world) to make it more accurate. Any help is much appreciated!",POS
414,omv22d,Unable to get an output while running the questionanswering code from Hugging face. Lent me some help,https://www.reddit.com/r/LanguageTechnology/comments/omv22d/unable_to_get_an_output_while_running_the/,akhilseban,LanguageTechnology,1970-01-01 00:00:01.626629198,,1,0,"This code is directly from huggingface.[https://huggingface.co/transformers/model\_doc/distilbert.html](https://huggingface.co/transformers/model_doc/distilbert.html)&#x200B;I was trying to run the below code in colab. I tried to print the answer string and it is not giving me any value. Why is it so? How is it supposed to work? &#x200B;&#x200B;from transformers import DistilBertTokenizer, TFDistilBertForQuestionAnsweringimport tensorflow as tftokenizer = DistilBertTokenizer.from\_pretrained('distilbert-base-uncased')model = TFDistilBertForQuestionAnswering.from\_pretrained('distilbert-base-uncased')question, text = ""Who was Jim Henson?"", ""Jim Henson was a nice puppet""input\_dict = tokenizer(question, text, return\_tensors='tf')outputs = model(input\_dict)start\_logits = outputs.start\_logitsend\_logits = outputs.end\_logitsall\_tokens = tokenizer.convert\_ids\_to\_tokens(input\_dict\[""input\_ids""\].numpy()\[0\])answer = ' '.join(all\_tokens\[tf.math.argmax(start\_logits, 1)\[0\] : tf.math.argmax(end\_logits, 1)\[0\]+1\])",POS
415,ommhot,"SpaCy, how to create a pattern to match a string caught via SpeechRecognition?",https://www.reddit.com/r/LanguageTechnology/comments/ommhot/spacy_how_to_create_a_pattern_to_match_a_string/,D3vil_Dant3,LanguageTechnology,1970-01-01 00:00:01.626596161,,6,1,"Hello there! I am having a little bit of problems figuring out how to approach the problem.FACT: I'm building an App for a role playing game (GURPS), who tracks damage  dealt to enemies from players. App itself is quite done, and i used  PySimpleGUI for the graphic interface. Next step, is to integrate vocal  commands, in order to enter input not from keyboard, but from voice  (because there are several inputs, so, why not?). So, i used SpeechRecognition library to catch the voice input, creating a  string variable who stores the input from user. Now I'm working on the  second part: from the string, extract the inputs. The last part would be  to stored those inputs into a dictionary and use it as input for my  functions.WHAT I'M TRYING TO ACHIEVE I'm having a lot of problem designing the matches with SpaCy. Because i  think there are no databases to train a NN or a ML model for my tasks,  i'm using [Rules Matching](https://spacy.io/usage/rule-based-matching). In this way, any sentence has to be structurated in a certain way, in order to extract token as i'd wish. An example of sentece is this one: ""You hit the enemy zombie one, that has vulnerability 2, to the head, with a large piercing attack, dealing 8 damage"".The inputs i have to extract are following:&#x200B;1. enemy hit: zombie one (is the enemy hit, and in the dataframe  created could be present zombie\_1, zombie\_2, etc..., in general,  multiple zombie, with a sequential number attached. Still try to  understand if will be a better idea to name them as zombie1, zombie2...)2. vulnerability ""number""3. location hit: head in this case, but could be ""right arm"", that i  can't extract because tokenization sees them as 2 tokens instead of 14. large penetration: type of attack (the easiest case in something  like ""cutting"", or ""crushing"", one word, easy to take, but i didn't find  any way to extract these tokens togheter, becuase how tokenization  works)5. damage 8: the damage dealtPROBLEMS: I'm currently using DependencyMatcher. Main problems are:&#x200B;1. Because tokenization works on single word, in the case said above, i  would lose the second part (right arm, extract only arm; large  penetration, only penetration).2. Can't generalize my pattern and i'm not sure ""DependencyMatcher"" is the right tool here. I'm working with Italian Language, but i'm testing in English for semplicity. My current script for english language is:&#x200B;    string = ""You hit the enemy zombie one, that has vulnerability 2, to the head, with a large piercing attack, dealing 8 damage.""        nlp = spacy.load(""en_core_news_sm"")        doc = nlp(string)        # for token in doc:        #     print(token.text, token.dep_)            # i'm going to create 2 lists with all words of body locations hit and type of attacks, in order to find the words via ""LOWER"" or ""LEMMA"" dependency (first part of list is in english, second part in italian)               body_list_words = [""Body"", ""Head"", ""Arm_right"", ""Arm_left"", ""Leg_right"", ""Leg_left"", ""Hand_right"", ""Hand_left"", ""Foot_right"", ""Foot_left"",                     ""Groin"", ""Skull"", ""Vitals"", ""Neck"", ""corpo"", ""testa"", ""braccio destro"", ""braccio sinistro"", ""gamba destra"", ""gamba sinistra"",                           ""mano destra"", ""mano sinistra"", ""piede destro"", ""piede sinistro"", ""testicoli"", ""cranio"", ""vitali"", ""collo""]            attack_type_words = [""cutting"", ""impaling"", ""crushing"", ""small penetration"", ""penetration"", ""big penetration"", ""huge penetration"",                              ""burning"", ""explosive"", ""tagliente"", ""impalamento"", ""schiacciamento"", ""penetrazione minore"", ""piccola penetrazione"",                              ""penetrazione"", ""penetrazione maggiore"", ""enorme penetrazione"", ""infuocati"", ""esplosivi""]                ###############        # Trovare i match        ##############        matcher = DependencyMatcher(nlp.vocab)        # I'm starting finding the verb        patterns = [{""RIGHT_ID"": ""anchor_verbo"",                     ""RIGHT_ATTRS"": {""POS"": ""VERB""}},                # Looking for Obj (word: enemy)                    {""LEFT_ID"": ""anchor_verbo"",                     ""REL_OP"": "">"",                     ""RIGHT_ID"": ""obj_verbo"",                     ""RIGHT_ATTRS"": {""DEP"": ""obj""}},            # Looking for the name of the enemy: zombie1                    {""LEFT_ID"": ""obj_verbo"",                     ""REL_OP"": "">"",                     ""RIGHT_ID"": ""type_enemy"",                     ""RIGHT_ATTRS"": {""DEP"": ""nmod""}},                 # Looking for word: vulnerability                    {""LEFT_ID"": ""anchor_verbo"",                     ""REL_OP"": "">"",                     ""RIGHT_ID"": ""vulnerability"",                     ""RIGHT_ATTRS"": {""LEMMA"": ""vulnerability""}},            #Looking for number associated to Vulnerability                    {""LEFT_ID"": ""vulnerability"",                     ""REL_OP"": "">"",                     ""RIGHT_ID"": ""num_vulnerability"",                     ""RIGHT_ATTRS"": {""DEP"": ""nummod""}},            #location of body hit                    {""LEFT_ID"": ""anchor_verbo"",                     ""REL_OP"": "">>"",                     ""RIGHT_ID"": ""location"",                     ""RIGHT_ATTRS"": {""LOWER"": {""IN"": body_list_words}}},           # Looking for word: attack, in order to find the type of attack                    {""LEFT_ID"": ""anchor_verbo"",                     ""REL_OP"": "">>"",                     ""RIGHT_ID"": ""attack"",                     ""RIGHT_ATTRS"": {""POS"": ""NOUN""}},            #Looking for type of attack                    {""LEFT_ID"": ""attack"",                     ""REL_OP"": "">>"",                     ""RIGHT_ID"": ""type_attack"",                     ""RIGHT_ATTRS"": {""LEMMA"": {""IN"": attack_type_words}}},            #Looking for word: damage in order to extract the number                    {""LEFT_ID"": ""attack"",                     ""REL_OP"": "">>"",                     ""RIGHT_ID"": ""word_damage"",                     ""RIGHT_ATTRS"": {""DEP"": ""nmod""}},            # Looking for the number                    {""LEFT_ID"": ""word_damage"",                     ""REL_OP"": "">>"",                     ""RIGHT_ID"": ""num_damage"",                     ""RIGHT_ATTRS"": {""DEP"": ""nummod""}}                        ]            matcher.add(""Inputs1"", [patterns])        matches = matcher(doc)            match_id, token_ids = matches[0]        matched_words = []        for i in range(len(token_ids)):            #print(patterns[i][""RIGHT_ID""] + "":"", doc[token_ids[i]].text)            matched_words.append(doc[token_ids[i]].text)            #########    # Now i'm creating the dictionary, deleting first element    #########        index_to_remove = [0]        for index, elem in enumerate(index_to_remove):            del matched_words[elem]        print(matched_words)            input_dict = {matched_words[0]: matched_words[1], ""location"": matched_words[4], matched_words[5]: matched_words[6],                      matched_words[7]: matched_words[8], matched_words[2]: matched_words[3]}            #print(input_dict)        return input_dictGeneral problem to solve: any complex words that should groupped  togheter (as for ""right arm"", ""left leg"", ""large penetration"") can't be  extract in this way (only arm, leg or penetration would be returned).Can you help me? Thanks!",NEG
416,omi85v,Simple stemmers,https://www.reddit.com/r/LanguageTechnology/comments/omi85v/simple_stemmers/,fiatsiat01,LanguageTechnology,1970-01-01 00:00:01.626576800,,3,1,"Hi! I'm working on a project and I need to write a stemmer - I've done some research and it seems like the Porter stemmer algorithm is the simplest one (relative to the others).&#x200B;However even then, it's still complex - are there any simpler stemmers (which might not perform as accurately), but still do reasonably well?",POS
417,om4pzv,Is there any way to detect grammatical errors and classify text as being either grammatically correct/incorrect?,https://www.reddit.com/r/LanguageTechnology/comments/om4pzv/is_there_any_way_to_detect_grammatical_errors_and/,Seankala,LanguageTechnology,1970-01-01 00:00:01.626530180,,16,2,"Hi. I'm just curious if there's anything like that out there, as I haven't really seen any data or models about it.If anyone knows of any resources (research papers, GitHub repositories, etc.) I'd be grateful if you'd let me know. Thanks!",POS
418,ombmuw,Open Source ASR with user-specific custom vocabularies?,https://www.reddit.com/r/LanguageTechnology/comments/ombmuw/open_source_asr_with_userspecific_custom/,snevi,LanguageTechnology,1970-01-01 00:00:01.626552278,,3,2,"Hi folks, I’m building a SaaS product that relies heavily on real-time speech-to-text transcription. An important aspect is that each user needs to be able to identify a small vocabulary of words that are unique to them. The general language model can be shared be all users though.For my MVP, I’ve been using IBM’s speech-to-text service with its custom language model feature and it works great. It’s very convenient to be able to specify custom words with a phonetic “sounds like” value. Unfortunately, the API costs aren’t viable for my pricing model so I’m looking at open source alternatives.Through my research, the most promising real-time transcription options appear to be [Vosk](https://github.com/alphacep/vosk-server) or [Kaldi Gstreamer](https://github.com/alumae/kaldi-gstreamer-server). I’ve set them both up & they appear to work well for general transcription, but I’m not sure how to handle the user-specific custom vocabularies.Vosk’s docs & sample code identify that you can pass custom words to it when starting the server, but I don’t think it would be practical to start a full server process for each client transcription.Kaldi Gstreamer’s master server & worker approach seems more practical resource-wise but I’m not clear on how I can do the user-specific vocabularies. Articles [like this one](https://chrisearch.wordpress.com/2017/03/11/speech-recognition-using-kaldi-extending-and-using-the-aspire-model/) suggest that merging custom words would take hours of training which isn’t ideal.It seems like [Dragonfly](https://dragonfly2.readthedocs.io/en/latest/kaldi_engine.html#user-lexicon) might offer some capability to add words to a “user_lexicon.txt” but it seems more suited to utterance commands than continuous speech recognition.If anyone has found a suitable open-source option to handle this type of scenario I’d be grateful if you can share the approach, thx!",POS
419,om2er4,What are your favorite conversational datasets? (multiple domains),https://www.reddit.com/r/LanguageTechnology/comments/om2er4/what_are_your_favorite_conversational_datasets/,None,LanguageTechnology,1970-01-01 00:00:01.626521081,,7,2,"I am working on a finetuning project with GPT-3 and I need conversational data! I would like data that contains multiple domains, such as medical, scientific, interpersonal, and fictional. Preferably open source.",POS
420,oll8s8,How would you go about deploying transformer based model?,https://www.reddit.com/r/LanguageTechnology/comments/oll8s8/how_would_you_go_about_deploying_transformer/,maroxtn,LanguageTechnology,1970-01-01 00:00:01.626455431,,10,11," I am studying the options I have in deploying an mT5 model and a Roberta model on the cloud. I am mainly considering google cloud, but I am still open for different suggestions.From what I understood, I have the choice of either using Google Compute Engine or Vertex AI stack, however the latter seems to be geared toward non-coding people.I have the model fine tuned and ready.",POS
421,olazti,Code and Named Entity Recognition in StackOverflow (ACL 2020) – Software NER Paper Explained,https://youtu.be/ojAvr3LACsA,rsree123,LanguageTechnology,1970-01-01 00:00:01.626416018,,17,0,,NEU
422,okyjms,"EleutherAI Researchers Open-Source GPT-J, A Six-Billion Parameter Natural Language Processing (NLP) AI Model Based On GPT-3",/r/datascience/comments/okx5xi/eleutherai_researchers_opensource_gptj_a/,tm2tb,LanguageTechnology,1970-01-01 00:00:01.626372903,,19,0,,NEU
423,okgfu5,We made a search engine for Stack Overflow,https://www.reddit.com/r/LanguageTechnology/comments/okgfu5/we_made_a_search_engine_for_stack_overflow/,SearchOverflow,LanguageTechnology,1970-01-01 00:00:01.626305330,,29,7,"Our search engine, [searchoverflow.com](https://searchoverflow.com), is built with NLP in mind. We are using Elastic Search and our own custom algorithms to help our users find what they are searching for. We thought that this subreddit may be interested in it and we would be happy to answer any questions that you all may have.",POS
424,okiiwc,collaborators for research paper(s),https://www.reddit.com/r/LanguageTechnology/comments/okiiwc/collaborators_for_research_papers/,B0RDA,LanguageTechnology,1970-01-01 00:00:01.626312734,,7,3,"I sometimes wonder if it's easy for many people with non-traditional backgrounds (e.g. self-taught) to get involved in academic research. I am working on several projects that I intend to turn into academic papers (to be submitted to conferences). Some of my previous projects have turned into EMNLP and COLING papers.I'm looking for collaborators who are interested in getting involved in the research process --- whether you're self-taught or not, please reach out if you're interested!EDIT: Forgot to mention what space I'm working in: I'm currently working on projects in dialog systems and text classification. I'm especially interested in improving out-of-distribution performance, as this is a huge problem when deploying NLP systems ""in the wild"".",POS
425,ok1pkb,Proof of concept NLP based software test automation,https://www.reddit.com/r/LanguageTechnology/comments/ok1pkb/proof_of_concept_nlp_based_software_test/,Dystopian_Satire,LanguageTechnology,1970-01-01 00:00:01.626259235,,13,0,"Hi all,I'm working on a proof of concept project for NLP based test automation working towards scriptless testing. There are a bunch of blog articles and such about these topics but no hard references (only found 1 so far) or examples. I'm using Stanza and spaCy for the NLP.Anyone happen to know some good resources to look at??Thanks in advance",POS
426,ok9432,How to identify frequently asked questions based on the dataset of all questions ever asked?,https://www.reddit.com/r/LanguageTechnology/comments/ok9432/how_to_identify_frequently_asked_questions_based/,following_snufkin,LanguageTechnology,1970-01-01 00:00:01.626283456,,2,6,"Hi everyone,  so I'm frontend developer totally new to NLP tools, so please bare with me 🙏I would like to create chatbot that will be using FAQ as response. I know how to do the frontend part and I plan to connect it to IBM Watson where I will hold my intents/entities. But I have problem with data processing. I have database with couple thousands of questions and I'm not even sure how/from where to start.   What I would like to retrieve from that database:  \- Frequently asked question. I could see when I skimmed through them that a lot of question repeated itself.  \- Retrieve categories from there, like ""issues with account"", ""privacy"", ""technical difficulties"".  Is it out of reach for a newbie?   Thank you a lot!",POS
427,ojwu22,BioBERT: a pre-trained biomedical language representation model - Paper Explained,https://youtu.be/JojsxMWF4y0,rsree123,LanguageTechnology,1970-01-01 00:00:01.626237074,,14,0,,NEU
428,okagh4,Turning Game Semantics into a real world application,https://www.reddit.com/r/LanguageTechnology/comments/okagh4/turning_game_semantics_into_a_real_world/,BabsiFTW,LanguageTechnology,1970-01-01 00:00:01.626287301,,0,1,"Hi guys, next semester there will be a lecture about “Game theory and agent-based modelling in linguistics” at my university and I was wondering if I should take a look at it. During my research, I have read about game semantics but haven’t found anything “practical”. Do you maybe know some papers where an application has been implemented based on game semantics? I am not really a fan of theoretical concepts and want to focus on stuff I can use in the real world.",NEG
429,ok8uzl,4 Popular Techniques to Measure the Readability of a Text Document,https://link.medium.com/LX6ATl0fThb,prakhar21,LanguageTechnology,1970-01-01 00:00:01.626282709,,0,0,,NEU
430,ok346m,"SpaCy, dependency matcher for multiple dependencies",https://www.reddit.com/r/LanguageTechnology/comments/ok346m/spacy_dependency_matcher_for_multiple_dependencies/,D3vil_Dant3,LanguageTechnology,1970-01-01 00:00:01.626264982,,2,0,"Hello there!I'm trying to figure out how to find matches, using DependencyMatcher, in the case i have a token with more than one dependency. For example, i have this sentence:sentence = a tree is hit in the branch with a crushing attack&#x200B;the visualitazione can be found here: [https://imgur.com/TfKr6iI](https://imgur.com/TfKr6iI)&#x200B;Now, my code is:    import spacy    from spacy import displacy    from spacy.matcher import DependencyMatcher            string = ""a tree is hit in the branch with a crushing attack""    nlp = spacy.load(""en_core_web_sm"")    doc = nlp(string)        matcher = DependencyMatcher(nlp.vocab)    # Only run nlp.make_doc to speed things up    patterns = [{""RIGHT_ID"": ""anchor_verb"",                 ""RIGHT_ATTRS"": {""ORTH"": ""hit""}},                    {""LEFT_ID"": ""anchor_verb"",                 ""REL_OP"": "">"",                ""RIGHT_ID"": ""subj_verb"",                 ""RIGHT_ATTRS"": {""DEP"": ""nsubjpass""}},                    {""LEFT_ID"": ""anchor_verb"",                 ""REL_OP"": "">"",                 ""RIGHT_ID"": ""location_verb"",                 ""RIGHT_ATTRS"": {""DEP"": ""prep""}}]        matcher.add(""Inputs1"", [patterns])    matches = matcher(doc)        match_id, token_ids = matches[0]    matched_words = []    for i in range(len(token_ids)):        print(patterns[i][""RIGHT_ID""] + "":"", doc[token_ids[i]].text)        matched_words.append(doc[token_ids[i]].text)        In this way, i extract ""hit"", ""tree"" and ""in"".    Point is, the word ""hit"", has 2 dependency (prep): ""in"" and ""with"". How can i extract the with? Because in this way, from the chunk:    {""LEFT_ID"": ""anchor_verb"",                 ""REL_OP"": "">"",                 ""RIGHT_ID"": ""location_verb"",                 ""RIGHT_ATTRS"": {""DEP"": ""prep""}}    i can only extract the first prep, the token ""in"", but if i wish to extract ""with""?",NEU
431,ok1btt,"Web app for doing coreference resolution and outputting file in "".conll"" format",https://www.reddit.com/r/LanguageTechnology/comments/ok1btt/web_app_for_doing_coreference_resolution_and/,Chox9000,LanguageTechnology,1970-01-01 00:00:01.626257497,,1,2,"Hi guys,Is there a web app online that does coreference resolution and then automatically outputs the resolved text in "".conll"" format? Thanks.",POS
432,ojhf6x,Scalable Search With Facebook AI's FAISS,https://www.pinecone.io/learn/faiss-tutorial/,jamescalam,LanguageTechnology,1970-01-01 00:00:01.626188546,,22,8,,NEU
433,ojrwo4,Multi-Class Sentiment Classification - Alternative loss functions,https://www.reddit.com/r/LanguageTechnology/comments/ojrwo4/multiclass_sentiment_classification_alternative/,twistedstats,LanguageTechnology,1970-01-01 00:00:01.626219435,,2,1,"I have a standard BERT sentiment classification task but more fine grained labels (-3=very negative, -2=negative, -1=mild negative, 0 = neutral, 1,2,3=very positive). Cross entropy loss is typical loss function used but in this case, target variable is ordinal. Are their alternative loss functions that account for the distance between true and predicted label? In my use case, there is higher ambiguity as to whether a sentence is very negative vs just negative, but less ambiguity over whether a sentence is negative vs neutral.",NEG
434,ojf4oy,I got access to the Copilot!,/r/github/comments/ojf378/i_got_access_to_the_copilot/,limapedro,LanguageTechnology,1970-01-01 00:00:01.626181392,,3,0,,NEU
435,oj2tw1,How hard is it to speech recognize lyrics in a song and transcribe them to text?,https://www.reddit.com/r/LanguageTechnology/comments/oj2tw1/how_hard_is_it_to_speech_recognize_lyrics_in_a/,clep30120,LanguageTechnology,1970-01-01 00:00:01.626131208,,11,5,"I'm trying to figure this out using Python and am struggling. The package [SpeechRecognition](https://pypi.org/project/SpeechRecognition/), using Google's api's,  can readily speech recognize normal speaking voice and transcribe that into text fine, but struggles when fed any parts of a song, even with singers singing very clearly.Is this like an impossible task/would require a very sophisticated neural network beyond what these packages offer? Or is there another way to do this (has to be in Python) that would actually work?",POS
436,oivxgq,Automatic Question Generation (with transformers) and its challenges!,https://www.youtube.com/watch?v=7CMAoLsKmBc,nlp_ninja,LanguageTechnology,1970-01-01 00:00:01.626110632,,17,1,,NEU
437,oj5rwt,What are my options for deploying pre-trained language models?,https://www.reddit.com/r/LanguageTechnology/comments/oj5rwt/what_are_my_options_for_deploying_pretrained/,maroxtn,LanguageTechnology,1970-01-01 00:00:01.626141552,,1,4,"I have pretrained T5 and Roberta model fine-tuned to solve some business analytics task. I want to deploy the models and I am not sure where to start, Heroku doesn't seem to support GPU hardware and the Ram is too small. Ideally if there is a platform where you can pay as you use, because inference would be ran once a day, so most of the time the models would be idle.Thank you.",POS
438,oixaup,​​DeepRapper: Neural Rap Generation with Rhyme and Rhythm Modeling,/r/DeepLearningPapers/comments/oixamu/deeprapper_neural_rap_generation_with_rhyme_and/,DL_updates,LanguageTechnology,1970-01-01 00:00:01.626114612,,3,0,,NEU
439,oigijg,How do you mitigate LSTM / Transformer decoder just memorizing teacher forcing input?,https://www.reddit.com/r/LanguageTechnology/comments/oigijg/how_do_you_mitigate_lstm_transformer_decoder_just/,boodleboodle,LanguageTechnology,1970-01-01 00:00:01.626050904,,5,3,"Hi. In most of my projects involving LSTM (and sometimes Transformer) decoders, the decoder network just memorizes the teacher forcing decoder input.Usually the initial hidden state of the decoder is a feature vector from a CNN or an LSTM encoder.I guess attention / professor forcing / scheduled sampling will alleviate this phenomenon. But before I even try those methods, I cannot cannot figure out how all the papers that use LSTM decoders in the literature do not seem to have the same problem.Does this happen to you as well? If so I would appreciate any tips from experience..",POS
440,oienfp,Datasets containing human-written transcripts of videos or audio files,https://www.reddit.com/r/LanguageTechnology/comments/oienfp/datasets_containing_humanwritten_transcripts_of/,Franck_Dernoncourt,LanguageTechnology,1970-01-01 00:00:01.626044300,,4,2,"I'm looking for datasets containing human-written transcripts of videos or audio files (e.g., podcast). I'm aware of the IWSLT2011 (TED) dataset and [OpenSubtitles](https://github.com/PolyAI-LDN/conversational-datasets/tree/master/opensubtitles). What else exists?",NEU
441,oibbht,Debiasing Techniques,https://www.reddit.com/r/LanguageTechnology/comments/oibbht/debiasing_techniques/,trfa23,LanguageTechnology,1970-01-01 00:00:01.626033118,,5,3,"Hi all! I'm  new to  NLP research and would like some advice from someone experienced  on  whether this idea is worth pursuing or  not. I've  recently been reading a lot of papers about debiasing embeddings, and they mostly focus on removing one aspect of bias e.g. either gender,  race, religion etc. This paper [link](https://www.google.com/url?sa=t&source=web&rct=j&url=https://www.aclweb.org/anthology/2020.gebnlp-1.2.pdf&ved=2ahUKEwj6jKGx6NvxAhURcBQKHSIXDTcQFjAAegQIBRAC&usg=AOvVaw1l4C9mopMjo3avd3dtGdUZ) shows that both gender and race embeddings are interdependent. So if a sentence is biased in more than one aspect, e.g. both racist and sexist, e.g. towards Black women or Asian men, wouldn't it make more sense to develop a separate technique which could explore the interdependencies, detect more than one bias and apply it once, rather than Relying on separate techniques and apply them one after the other?",NEG
442,ohzrnl,EMNLP 2021 Reviews Out!,https://www.reddit.com/r/LanguageTechnology/comments/ohzrnl/emnlp_2021_reviews_out/,Open-Programmer5420,LanguageTechnology,1970-01-01 00:00:01.625987025,,15,10,"I got a 3.5, 3, 2.5, which is pretty bad. Two reviewers seem to miss the motivation and methodology of the paper. Should I put in effort in the author response or just withdraw the paper?",POS
443,oibqxc,From voice to text to create input,https://www.reddit.com/r/LanguageTechnology/comments/oibqxc/from_voice_to_text_to_create_input/,D3vil_Dant3,LanguageTechnology,1970-01-01 00:00:01.626034504,,1,6,"Hello everyone. I developed an app on python. Point is, it needs a lot of input from user. And I'd like to implement a voice recognizer to use voice as input.On top of this, I love study, so, it's a good way to improve my knowledge.But main task is to create a model to extract the input from the text (acquired previously from speech recognition).The informations to extract are several because this app has several windows that open after you enter the right inputs into the list boxes, the text boxes etc...I ve studied a little bit of nlkt of python. Understood how to extract pattern with regex. But I'd wish to understand better the right pipeline of work, what kind of tool to use, and if does exist, a better strategy than regex (like some neural networks).Can you help me?",POS
444,oi3xq8,Best Computer Linguistics bachelor in Europe?,https://www.reddit.com/r/LanguageTechnology/comments/oi3xq8/best_computer_linguistics_bachelor_in_europe/,Magikey_,LanguageTechnology,1970-01-01 00:00:01.626008037,,2,4,"I've been looking for this information, but the discussions that I found are pretty old and limited to either MA programmes or just Germany. I was considering studying it in Heidelberg, as in Italy there is no specific undergraduate course. Thank you for answering.",POS
445,ohkx1g,ClinicalBERT : Pretraining BERT on clinical text - Paper Explained,https://youtu.be/KT18WgZhtRo,rsree123,LanguageTechnology,1970-01-01 00:00:01.625931412,,17,0,,NEU
446,ohuf50,Language generation datasets,https://www.reddit.com/r/LanguageTechnology/comments/ohuf50/language_generation_datasets/,maroxtn,LanguageTechnology,1970-01-01 00:00:01.625963779,,3,2,"I read many blog posts raving about how they generated this awesome piece of text using a conditional generation model, or see a website promising top notch results to writers to automate writing articles for them.I feel like I am living in a separate planet apart from these people. Other than GPT-3, which us, mortals, have no access to its API, I can think of no other model performing as good of a results as these people claim. I even tried GPT-J and it still make really silly mistakes.So, what piece of the puzzle am I missing ?&#x200B;My reasoning was that: these people must be using datasets to prime their CTG models to generate text given a prompt for example, or to make them do cool things. But no matter how much I googled or search, I neither find these datasets, nor guides on how to get similar results.What am I missing guys? Sorry for the misleading title, I was not sure on what to put.",NEG
447,ohb9k3,"Suggestions on where to find good data sets? In particular, for training aspect based sentiment analysis models?",https://www.reddit.com/r/LanguageTechnology/comments/ohb9k3/suggestions_on_where_to_find_good_data_sets_in/,Revlong57,LanguageTechnology,1970-01-01 00:00:01.625888561,,14,7,"So, I'm just wondering, what are the best places to find datasets for model training? I'm already tried [https://paperswithcode.com/](https://paperswithcode.com/), is there a better place to look? Point being, I'm trying to train my model to do document level aspect based sentiment analysis, and the only decent dataset I can find for it is the one from semeval 2015/2016, however, there's a lot of issues with this dataset, so I was hoping to find a better one. Any ideas on where to look?",POS
448,oh0g9u,Those pursuing a PhD in NLP related fields in the US - how's the course going for you?,https://www.reddit.com/r/LanguageTechnology/comments/oh0g9u/those_pursuing_a_phd_in_nlp_related_fields_in_the/,naboo_random,LanguageTechnology,1970-01-01 00:00:01.625851914,,23,9,"Those pursuing a PhD in NLP related fields - how's it going for you? How do you cope with the publication pressure? Are you still interested in your research? Background: I have a PhD offer in NLP, but Im on the fence about going back to school - I do love working on new problems and trying to find solutions from scratch - seeing what works, what doesn't and why.. But I'm terrified of the publication pressure ( academic twitter scares me ). I'm probably not that fascinated by the idea of publishing papers. Any insight on how you deal with it? Is it a lot pressure around conference deadlines? How do you manage the long term goal vs short term deadlines?Thanks!",NEG
449,ohfxzq,Explainable AI using lime library of Python,https://www.reddit.com/r/LanguageTechnology/comments/ohfxzq/explainable_ai_using_lime_library_of_python/,Alert_Safe_4440,LanguageTechnology,1970-01-01 00:00:01.625911209,,1,0,"Hi I have tried to explain my model using the lime library of python (https://github.com/Dontc0pY/Password-strength/blob/main/NLP_Password_Strength_classifier.ipynb) I want to make a diagram  using a function . I am unable to get the diagram it shows an error( refer to the link) please help me.The aim of my project is to classify passwords based on their strengths denoted by numbers (0,1,2) 2 being the strongest.i have used ML and DNN algo.This problem is a non binary classification.",POS
450,oh1uc7,"Book Mastering spaCy is out, please join book lottery",https://www.reddit.com/r/LanguageTechnology/comments/oh1uc7/book_mastering_spacy_is_out_please_join_book/,Mariana331,LanguageTechnology,1970-01-01 00:00:01.625856079,,16,6,"Disclaimer: I'm the author :)My new book Mastering spaCy is out for sale today. I'm organizing a lottery and gift 5 printed copies to the lucky readers. You can get your lucky number from: https://www.linkedin.com/posts/duygu-altinok-4021389a_nlp-activity-6818784269486313472--HiGI'm looking forward to gift the books, please put your name into the hat!",POS
451,ogu4py,Inputting long documents to nlp models,https://www.reddit.com/r/LanguageTechnology/comments/ogu4py/inputting_long_documents_to_nlp_models/,mr_dicaprio,LanguageTechnology,1970-01-01 00:00:01.625832192,,7,3,"Let's say that I have long, annotated document on which I would like to perform sequence labelling (e.g. NER). What is the proper way of inputting this text into model ? The only reasonable idea that I have is to segment the text into sentences (using stanza for example) and maybe trim those that exceed model's limit. How do you approach it ?",POS
452,ogyedy,Multilingual Concatenated Corpus,https://www.reddit.com/r/LanguageTechnology/comments/ogyedy/multilingual_concatenated_corpus/,hayis4horses1,LanguageTechnology,1970-01-01 00:00:01.625845864,,1,0,"Hello everyone! I am trying to build a parallel multilingual corpus by concatenating several available parallel resources. Obviously, due to differences in resourcedness I need to perform some data sampling. I am quite new to multilingual machine translation and I don't really understand how to implement temperature based sampling for instance. However when I look for resources, I get pointed to academic papers instead (which while nice, doesn't help to clarify things tooo much). So if someone here has experience with this and can point me to some resources, I would be eternally grateful! as a clear question: given lets say 5 parallel texts (so src1, tgt1, src2, tgt2,...), I would like to concatenate them to create multisrc and multitgt however I need to perform some data sampling to ensure enough examples of low resource languages are seen during training. I would specifically like to use temperature based sampling, my question is how would this be implemented?",POS
453,oh1f9n,"My presenation on ""Why NLP can't prevail""",https://youtu.be/YaDKIkOi0JA,rakashraj,LanguageTechnology,1970-01-01 00:00:01.625854787,,0,2,,NEU
454,ogv8v9,Sentiment Analysis with multiple emotions,https://www.reddit.com/r/LanguageTechnology/comments/ogv8v9/sentiment_analysis_with_multiple_emotions/,wissamMer,LanguageTechnology,1970-01-01 00:00:01.625836271,,1,7,"Hello everyone. I am relatively new in the field of ML/NLP and as one of my first projects I'm working on a sentiment analysis model for movies. Basically it takes movie descriptions, genres, ratings... And figures out how the audience will feel after watching it (happy, sad, angry, scared...). However I couldn't find a text corpus in python that goes past the positive/negative/neutral sentiments. I would appreciate any recommendations for corpora that fit what I want to do, and failing that any guidance on how to create my own corpus using a words/sentiments dataset I found online, or any other way I can use that dataset to aid in my task. Thanks for any help in advance",POS
455,ogh5d8,Looking suggestions for learning sources/materials.,https://www.reddit.com/r/LanguageTechnology/comments/ogh5d8/looking_suggestions_for_learning_sourcesmaterials/,Kir-01,LanguageTechnology,1970-01-01 00:00:01.625780389,,5,7,"Hi everyone, I'm a designer with a background in psychology and I'm currently developing my skill into conversational design.I would like to better understand NLP and ML technologies and recent developments for the purpose of having a better knowledge of the design possibilities and the limits you have te keep in mind.I have a general understanding of this kind of technologies and I would like to go a little deeper.Do you have some sources to suggest? Talking about books, blogs, website, papers, courses or anything you think could be great for my situation. I'm having a bit of a struggle finding something ok, there are lots of divulgative materials but it's too trivial, while going to sources and read papers and technical ""forums"" is still too complicated for me.I appreciate your help. :)Tl;dr: looking for learning material for design purposes, not too divulgative, not for technicians.",POS
456,ofwnad,Most influential work NLP + cognitive science,https://www.reddit.com/r/LanguageTechnology/comments/ofwnad/most_influential_work_nlp_cognitive_science/,comedown-machine,LanguageTechnology,1970-01-01 00:00:01.625706718,,20,4,"It seems like most of the most highly cited professors/labs working in the intersection of NLP and cognitive linguistics or cognitive science are mostly in the US and concentrated in a handful of schools like CMU, MIT, Princeton, UMD (maybe I am missing many - I am not super familiar). What are the biggest names in this subfield (inside and also outside of the US)? Beyond standard NLP venues like ACL/EMNLP or workshops, I imagine they also publish in cognitive science journals -- I am not sure how to figure out which works are prominent and where the field is heading, so if anyone has a few prominent labs/people to follow, that would be appreciated a lot!",POS
457,ofk6hs,Conference for Truth and Trust Online - Call for Paper and Talk Proposals!,https://www.reddit.com/r/LanguageTechnology/comments/ofk6hs/conference_for_truth_and_trust_online_call_for/,kochkinael,LanguageTechnology,1970-01-01 00:00:01.625668707,,9,0,"We invite you to submit a paper or talk proposal to this year’s **Truth and Trust Online (TTO) conference (October 7-8, 2021, online).**We are excited to announce that **a selection of the best technical papers will be invited to a special issue of the ACM Journal of Data and Information Quality.**  The submission deadlines are just around the corner but its not too late to contribute :) Technical papers deadline: **July 30, 2021;**  talk proposals deadline: **August 13, 2021.**  **Call for Papers and Talk Proposals**We invite submissions of **technical papers** and **talk proposals** on technical solutions for addressing current challenges facing social media platforms on the following topics:      • Misinformation and disinformation       • Trustworthiness of COVID-19 news and guidance      • Hate speech      • Online harassment and cyberbullying      • Credibility and fake reviews      • Hyper-partisanship and bias      • Image/video/audio verification      • Fake amplification, polarization, and echo chambers      • Transparency in content and source moderation       • Privacy and anonymity requirements  We welcome **technical papers** of the following types: *surveys, methods, reproduction papers, resource papers, case studies*. **More information** about the call for papers, talk proposals and the JDIQ special issue can be found here: [https://truthandtrustonline.com/call-for-papers-2/](https://eur01.safelinks.protection.outlook.com/?url=https%3A%2F%2Ftruthandtrustonline.us17.list-manage.com%2Ftrack%2Fclick%3Fu%3Da4f23076f7054c8e9a1620da3%26id%3Dec373059d5%26e%3D5db6c1f255&data=04%7C01%7C%7Caf3244fe4eec453258d308d9399230ad%7C569df091b01340e386eebd9cb9e25814%7C0%7C0%7C637604122318972844%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=TGRUQTNUmmIzUszXan42baSAF2KOlPE9psaw3EwKP90%3D&reserved=0)  We encourage wide participation from all interested parties and stakeholders on online media, including academics, startups and large industry, non-profit organizations and governmental institutions.**About The Conference on Truth and Trust Online**The Conference on Truth and Trust Online is an annual forum for academia, industry, non-profit organizations, and other stakeholders to discuss the problems facing (social) media platforms and technical solutions to understand and address them. TTO’s mission is to bring together all parties working toward improving the truthfulness and trustworthiness of online communications. More information about the conference is [on the TTO website](https://truthandtrustonline.com/).   **Feel free to pass this on to anyone you think might be interested!****Contact us** if you have any questions at [admin@truthandtrustonline.com](mailto:admin@truthandtrustonline.com)Follow us on **Twitter**: u/TTOConference  Sign up to **TTO mailing list**: [eepurl.com/haDf5H](https://t.co/BFvIgPjw9o?amp=1)",POS
458,of5vzg,"Baidu Research now in first place on SuperGLUE, exceeding human baseline and Microsoft Research performance",https://arxiv.org/abs/2107.02137,2600_yay,LanguageTechnology,1970-01-01 00:00:01.625611167,,26,7,,NEU
459,of3puz,NLP-powered Article Title Generator,https://www.reddit.com/r/LanguageTechnology/comments/of3puz/nlppowered_article_title_generator/,hyperwrite,LanguageTechnology,1970-01-01 00:00:01.625604324,,7,1,"Hi all! Here's an article title generator I just made. It uses NLP to come up with unique titles for articles, blogs etc., just by pasting in the content of the article. It's still learning, so the results are hit or miss, but please give it a try! The more you use it, the better it will get.[https://tools.hyperwrite.ai/titlegenerator](https://tools.hyperwrite.ai/titlegenerator)",POS
460,oeuty9,Training and Testing an Italian BERT - Transformers From Scratch Series,https://youtube.com/watch?v=35Pdoyi6ZoQ&feature=share,jamescalam,LanguageTechnology,1970-01-01 00:00:01.625577755,,18,2,,NEU
461,oephbu,Does anyone know a good pretrained spam detection model?,https://www.reddit.com/r/LanguageTechnology/comments/oephbu/does_anyone_know_a_good_pretrained_spam_detection/,pocketaces27,LanguageTechnology,1970-01-01 00:00:01.625553559,,5,2,As the title states. Preferably trained on social media posts,NEU
462,oe6myh,GPT-J for text generation on NLPCloud.io,https://www.reddit.com/r/LanguageTechnology/comments/oe6myh/gptj_for_text_generation_on_nlpcloudio/,juliensalinas,LanguageTechnology,1970-01-01 00:00:01.625489063,,26,0,"Hi everyone.Since the release of GPT-J, I worked hard in order to add it to [NLPCloud.io](https://nlpcloud.io?utm_source=reddit&utm_campaign=a311103c-dd8e-11eb-ba80-0242ac130004) for text generation. This is done now and the infrastructure is stabilized but that was tricky. So I thought I would share here my key takeaways, in case it can help some of you:* On CPU, the model needs around 40GB of memory to load, and then around 20GB during runtime.* On CPU, a standard text generation (around 50 words) takes approximately 12 CPUs for 11 seconds* On a GPU, the model needs around 40GB of memory to load, and then around 3GB during runtime + 24GB of GPU memory. For a standard text generation (around 50 words), the latency is around 1.5 secsThe 2 main challenges are the high amount of RAM needed for startup, and then high amount of GPU memory needed during runtime which is quite impractical as most affordable NVIDIA GPUs dedicated to inference, like Tesla T4, only have 16GB of memory...It's very interesting to note that, during my tests, the latency was pretty much the same as GPT-Neo 2.7B on the same hardware, but accuracy seems of course much better.If some of you also ran these kinds of benchmarks on GPT-J I'd love to see if we're aligned or not!",POS
463,oe5kwj,Recognise paragraphs beloning to a law or regulation,https://www.reddit.com/r/LanguageTechnology/comments/oe5kwj/recognise_paragraphs_beloning_to_a_law_or/,gevezex,LanguageTechnology,1970-01-01 00:00:01.625485132,,3,5,What techniques are used to classify paragraphs from documents that have close relations with particular laws from  a law book article or regulations code.,NEU
464,odvhd4,What is tools developers use more frequently in creating chatbots in industry ?,https://www.reddit.com/r/LanguageTechnology/comments/odvhd4/what_is_tools_developers_use_more_frequently_in/,maroxtn,LanguageTechnology,1970-01-01 00:00:01.625441354,,5,5,"I am aware that state of the art models in academia doesn't always get used in industry, due to many limitations, such as speed, size, and lack of interpretability. For that reason, I am wondering what tools do developers use in creating chatbots for their clients. The impression I get is that code is barely used, and developers use GUI to craft the conversation flow. But isn't that too limiting ?",POS
465,odr2xj,Any guidance on converting Question and Answer text to coherent prose text?,https://www.reddit.com/r/LanguageTechnology/comments/odr2xj/any_guidance_on_converting_question_and_answer/,KayvonDeluxe,LanguageTechnology,1970-01-01 00:00:01.625426279,,8,3,"Hello,   I have a particular NLP problem I'm trying to solve where I want to take a series of questions with answers and convert them into prose to summarize in statement form all that was discussed.&#x200B;**To illustrate I would like to take question and answers like this:**Q: How long ago did your cough start?  A: Three days ago.Q: Do you have a fever?  A: No.  **And convert this into prose text that states something like:**   ""The cough started three days ago.  There is no fever.""  I understand this is a challenging NLP task.  I have a background in programming, but not really in NLP.  Can anyone recommend a python library that could do this or offer any guidance at all.  It would be appreciated!  Thanks!",POS
466,ocjq2y,"Can anyone point me in the direction of some good portofilio's that highlight Text Minining, NLP, and Sentiment Analysis?",https://www.reddit.com/r/LanguageTechnology/comments/ocjq2y/can_anyone_point_me_in_the_direction_of_some_good/,Condev7,LanguageTechnology,1970-01-01 00:00:01.625260756,,22,1,"I have been working on some projects and have a background in linguistics. I wanted to see viable porfolio examples, so I can use them for inspiration!  Any help is appreciated!",POS
467,och5x0,"Seeking OCR review paper (spark OCR, keras-ocr, tesseract, google vision, EasyOCR)",https://www.reddit.com/r/LanguageTechnology/comments/och5x0/seeking_ocr_review_paper_spark_ocr_kerasocr/,polandtown,LanguageTechnology,1970-01-01 00:00:01.625253382,,3,3,"I'm seeking a review paper comparing accuracy/speed of contemporary OCR methods.(for context, I'm working on a project where I'm extracting text from 112,000,000 pdf pages (pdf -> png) and want to select the most-accurate, then fastest method)Open to all thoughts/ideas, thanks.&#x200B;Resources available:14 CPU Cores @ 3.6 GHz2x 3060TI(s)(cannot use cloud instances)",POS
468,oc6l5i,Guide To Sentiment Analysis Using BERT,https://analyticsindiamag.com/guide-to-sentiment-analysis-using-bert/,analyticsindiam,LanguageTechnology,1970-01-01 00:00:01.625218127,,0,0,,NEU
469,obzmuq,How to make text-summarizing model predict for a single input?,https://www.reddit.com/r/LanguageTechnology/comments/obzmuq/how_to_make_textsummarizing_model_predict_for_a/,stuffingmybrain,LanguageTechnology,1970-01-01 00:00:01.625188182,,1,0,"So my LSTM-based model takes in a review and summarizes it in a few words. These are the steps I take *on a single input* after cleaning the text:    tokenizer = Tokenizer()      sequence = tokenizer.texts_to_sequences([input_string])      sequence = pad_sequences(sequences, maxlen = 35) Since  `texts_to_sequences` takes in a list of strings, I did `texts\_to\_sequences(\[input\_string\])` after reading [this answer](https://stackoverflow.com/questions/53535805/making-predictions-on-single-review-from-input-text-using-saved-cnn-model) to a similar question (and going through [this blogpost](https://www.thepythoncode.com/article/build-spam-classifier-keras-python)). However, I still get an error that 'NoneType' object has no attribute 'lower'. Looking through the stack trace, it seems that a function called `text\_to\_word\_sequence` in the keras library is being called, and it's taking in a parameter `text` which seems to be a `NoneTypeObject`. How can I fix this issue?",NEG
470,obja53,"DistilBERT Revisited :smaller,lighter,cheaper and faster BERT - Paper explained",https://youtu.be/cCs8exFrGE8,rsree123,LanguageTechnology,1970-01-01 00:00:01.625136709,,9,1,,NEU
471,obkxfw,How to process Reddit comments for stock symbols?,https://www.reddit.com/r/LanguageTechnology/comments/obkxfw/how_to_process_reddit_comments_for_stock_symbols/,__tendies__,LanguageTechnology,1970-01-01 00:00:01.625143002,,4,7,"I am working on a Reddit stock symbol mention calculator on live Reddit comments. The issue I am having is with stock names that can be a verb like ""DO"" or a noun like ""COIN"". I can't see to find a simple way around this. Does anyone have any suggestions on how I can fix this? I am working in node.js. A javascript library would be ideal.",POS
472,obidpi,XLM Roberta vs Bert multilingual,https://www.reddit.com/r/LanguageTechnology/comments/obidpi/xlm_roberta_vs_bert_multilingual/,pocketaces27,LanguageTechnology,1970-01-01 00:00:01.625132851,,5,8,"Hi, noob here trying to learn about NLP. I've heard good things about XLMR and tried to run classification tasks on social media posts using it. Im baffled why it seems to be doing worse than bert by a large margin.Data is the same, hyper parameters are the same, but the XLMR classification f1 scores are always half that of Bert's. Anything i should be taking care of?",POS
473,oba3gj,Term Extraction Package Demo on Heroku,https://www.reddit.com/r/LanguageTechnology/comments/oba3gj/term_extraction_package_demo_on_heroku/,Kevinlu1248,LanguageTechnology,1970-01-01 00:00:01.625098604,,7,0,"Hey guys, about a year ago I posted my Python package PyATE, which has implementations of automated term extraction (ATE) algorithms at [https://www.reddit.com/r/LanguageTechnology/comments/hpg7h1/term\_extraction\_package\_in\_python/](https://www.reddit.com/r/LanguageTechnology/comments/hpg7h1/term_extraction_package_in_python/). To showcase the algorithms, I made a web app at [https://pyate-demo.herokuapp.com/](https://pyate-demo.herokuapp.com/), for easier access so users don't have to download the package to test it out. I thought I would share this here as it could potentially help out engineers and researchers.(Also as this web app is hosted on Heroku free tier, it may take 30 seconds to launch since the app may have fallen asleep.)",POS
474,obagu8,Ludicrous BERT Search Speeds,https://www.pinecone.io/learn/bert-search-speed/,gregory_k,LanguageTechnology,1970-01-01 00:00:01.625099949,,2,0,,NEU
475,obdwli,Address extraction and formated using Places API (Google Maps API),https://www.reddit.com/r/LanguageTechnology/comments/obdwli/address_extraction_and_formated_using_places_api/,calmeo,LanguageTechnology,1970-01-01 00:00:01.625112619,,1,0,"Hi guys,I am currently playing around with Places API from Google.I just have curious about the technique that they were using to make this happens (do only NER enough for this).When I input the text: ""I wanna deliver this package to #K A B C ""It gave me the result was super tremendous with 3 administrative\_area\_level, even it did format my input text (also correct their spelling/grammar mistake) to the address one. In detail, it somehow will be like    street_name:{#k}    administrative_area_level_1: {A}    administrative_area_level_2:{B}    ...    formated_address: #K, A, B, C[Overview  |  Places API  |  Google Developers](https://developers.google.com/maps/documentation/places/web-service/overview)",POS
476,oarsxw,Default Transformer model in spaCy v3.0,https://www.reddit.com/r/LanguageTechnology/comments/oarsxw/default_transformer_model_in_spacy_v30/,mdbadhru,LanguageTechnology,1970-01-01 00:00:01.625036535,,9,7,"I am using spaCy to extract word embeddings of bunch of texts. So I downloaded the en_core_web_trf, and started using it to get the tokens embeddings. In the spaCy website under Models, I see the following: "" English transformer pipeline (roberta-base)....."". Does that mean when I use the pretrained transformer model, am I using Roberta? If so, how do I change it to, say, BERT, XLNet, or others. Any help would be appreciated.",POS
477,oa8u41,"3 Vector-based Methods for Similarity Search (TF-IDF, BM25, SBERT)",https://youtube.com/watch?v=ziiF1eFM3_4&feature=share,jamescalam,LanguageTechnology,1970-01-01 00:00:01.624975154,,24,2,,NEU
478,oaifoy,Language Engineer interview with Amazon,https://www.reddit.com/r/LanguageTechnology/comments/oaifoy/language_engineer_interview_with_amazon/,blueberry2029,LanguageTechnology,1970-01-01 00:00:01.625003686,,4,7,I just scheduled my onsite interview with Amazon position. I am wondering if anyone on here has went thought the process that can share any insight into how to prepare for it.,POS
479,oa9505,The Illustrated Wav2vec: How to predict the future of an audio sequence,https://jonathanbgn.com/2021/06/29/illustrated-wav2vec.html,jonathanbgn,LanguageTechnology,1970-01-01 00:00:01.624976110,,11,0,,NEU
480,oa2lkx,Imbalanced Dataset in NLP,https://www.reddit.com/r/LanguageTechnology/comments/oa2lkx/imbalanced_dataset_in_nlp/,goddySHO,LanguageTechnology,1970-01-01 00:00:01.624947879,,12,15,"Hi all,Just wanted to inquire if there anybody has leads on best practices to work with imbalanced dataset and text classification use cases.TIAEdit --Initial process assessment has lead me to understand that in a multi-class classification case for a banking firm's Customer support, there will be certain imbalance in the email data available.So as an example, there are more emails on a daily basis related to technical bits about applications/FAQ questions related to the bank's operations. Whereas there will be less emails when it comes to customers reporting fraud. Till now the SMEs have informed me the ratio of emails would be 10:1 for Fraud cases.",POS
481,oa8mvr,Is there any room for NPL startups?,https://www.reddit.com/r/LanguageTechnology/comments/oa8mvr/is_there_any_room_for_npl_startups/,Wufi,LanguageTechnology,1970-01-01 00:00:01.624974476,,2,7,"I mean, it seems like the state-of-the-art in NLP and AI in general is led by the top companies. I had the idea of starting something related but I can't really identify a business model that adds something new to the scene. I've been working on different models but it seems like this field has to do more with research, being the money reserved for a few top companies.Sorry about my English and my pesimism, just got rejected for 849483984 time.",POS
482,oa0kv4,Does Any Research Exist on LSH for Strings (edit distance)?,https://www.reddit.com/r/LanguageTechnology/comments/oa0kv4/does_any_research_exist_on_lsh_for_strings_edit/,simonhughes22,LanguageTechnology,1970-01-01 00:00:01.624939302,,3,15,"I have a model where we currently handle out of vocab tokens using hashing. That works better than you would expect but i am looking for ways to hash strings so that the likelihood of collision is higher for strings that are more similar, either in terms of edit distance, or same character ngram overlap (e.g. jaccard). One method would be to do LSH on the character ngrams. Are there any known methods for automatically hashing similar strings into the same hash buckets? Either learned (data dependent) or determinstic (like LSH)?",NEG
483,o9rga3,Easy Custom NLP T5 Model Training Tutorial - Abstractive Summarization Demo with SimpleT5,https://youtu.be/jgKj-7v2UYU,dulldata,LanguageTechnology,1970-01-01 00:00:01.624908376,,15,0,,NEU
484,o9x7hq,Visualization help,https://www.reddit.com/r/LanguageTechnology/comments/o9x7hq/visualization_help/,annalabagaba,LanguageTechnology,1970-01-01 00:00:01.624926870,,1,1,"I apologize if this sounds vague but what are some good ways to visualize NLP data that is not focused on words?In my case, I used doc2vec and am now comparing the documents using gensim. I don't want to focus on words but the overall similarity between texts.Note: I also posted the on r/PythonLearning",POS
485,o9glq6,Behavioral Testing of ML Models (Unit tests for machine learning) [Video],https://www.reddit.com/r/LanguageTechnology/comments/o9glq6/behavioral_testing_of_ml_models_unit_tests_for/,jayalammar,LanguageTechnology,1970-01-01 00:00:01.624871769,,10,0,"This video discusses the paper [Beyond Accuracy: Behavioral Testing of NLP Models with CheckList](https://www.aclweb.org/anthology/2020.acl-main.442/), winner of Best Paper at ACL 2020.Evaluating ML models using a single metric (like accuracy or F1-score) produce a low-resolution picture of model performance. Behavioral tests can give us a much higher resolution evaluation of a model's capabilities. By creating tests (which are small targeted test sets), we can better compare models or observe how model performance changes after re-training a model (or fine-tuning it). [https://youtu.be/Cse-3MM7mso](https://youtu.be/Cse-3MM7mso)Hope you enjoy it",POS
486,o9m85r,Text classification for Email data,https://www.reddit.com/r/LanguageTechnology/comments/o9m85r/text_classification_for_email_data/,goddySHO,LanguageTechnology,1970-01-01 00:00:01.624893053,,2,3,"Hi all,For work purposes, I have come across a use case where a Customer Support team will be handing over a Data dump from their Outlook profiles, fortunately they have emails in different folders as per their domains. Just wanted to understand what kind of an approach would be preferred for such tasks. I am yet to figure out a way to break emails into the latest reply/remove forwarded & reply text.All I have read has pointed towards the following path -1. EDA (explore sentence, word and different ngram level data)2. Data cleaning3. Stemming/Lemmatization4. Tokenizer (yet to be finalized, as the data is domain specific, it could have a lot of unknowns post tokenization)5. Currently looking at transfer learning with LSTM/Transformer models (HuggingFace)Any inputs are appreciated! TIA",POS
487,o9k26s,Significance Testing for ML Classifcation algorithm vs Random classification,https://www.reddit.com/r/LanguageTechnology/comments/o9k26s/significance_testing_for_ml_classifcation/,thegreatudini,LanguageTechnology,1970-01-01 00:00:01.624886186,,1,1,"Hello,&#x200B;I have built a classification algorithm between three classes and am quite happy with the results. I was wondering how could I test for statistical significance when compared to a random classifier?&#x200B;Thank you!",POS
488,o92rpt,Comparing M2M to mT5 in low resource translation (10k dataset Yoruba - English),https://www.reddit.com/r/LanguageTechnology/comments/o92rpt/comparing_m2m_to_mt5_in_low_resource_translation/,maroxtn,LanguageTechnology,1970-01-01 00:00:01.624818721,,9,2,"I wrote an article lately that compares two big language models, M2M and mT5 on the translation task. Both are new, their usage is not very clear, and and fall under the same type.The dataset I used for fine tuning has 10k sentence pairs.I found no clear comparison nor a clear guide on how to fine tune both of the models on the translation task, so I decided to write it myself. (code: [https://github.com/maroxtn/mt5-M2M-comparison](https://github.com/maroxtn/mt5-M2M-comparison))I trained the two models to translate text from Yoruba (Nigeria) to English, you can see example of the produced translations here:**Yoruba**: ìgbà wo lẹ di ẹlẹ́rìí jèhófà?  **Target**: when did you become one of jehovah’s witnesses?**M2M**: when were you one of jehovah’s witnesses?  **mT5**: what is the meaning of jehovah’s word?— — — -**Yoruba**: rárá ẹ ò lè bá a sọ̀rọ̀  **Target**: no you can’t**M2M**: sorry you can’t tell me  **mT5**: no you can’t talk to us now— — — -**Yoruba**: mo mọ̀ èyí kí n tó ní kí ó fẹ́ mi  **Target**: i knew this before i told you to marry me**M2M**: i know that before i had to marry me  **mT5**: i know that this is why i want to see you— — — -**Yoruba**: èyí ni àwọn ìfẹsẹ̀wọnsẹ̀ tí yóò máa wáyé nínú ìdíje orílẹ̀-èdè nàìjííríà npfl lónìí:  **Target**: this are the matches that will be coming up in the nigeria npfl today:**M2M**: these are the matches that will be played at the nigeria npfl today:  **mT5**: the following are the results of the nigerian football league (npfl) competition:M2M outperformed mT5 by a big margin (rouge 23 vs 45)Read more about it here: [https://abdessalemboukil.medium.com/comparing-facebooks-m2m-to-mt5-in-low-resources-translation-english-yoruba-ef56624d2b75](https://abdessalemboukil.medium.com/comparing-facebooks-m2m-to-mt5-in-low-resources-translation-english-yoruba-ef56624d2b75)",NEG
489,o8uoqq,When to use NER and POS tagging ?,https://www.reddit.com/r/LanguageTechnology/comments/o8uoqq/when_to_use_ner_and_pos_tagging/,maroxtn,LanguageTechnology,1970-01-01 00:00:01.624790382,,11,8,"I only learnt about NER and POS tagging at the beginning of my NLP journey in the textbooks (one year and half ago), but I rarely see tutorials using them, and if some tutorial use them, they couple heir use with really simple models such as linear regression or logistic regression.This gives me the impression that NER and POS are outdated, and only being used by the veterans in the field. I might be very much wrong since I am still relatively new.My question is, are NER and POS tagging no longer useful, or am I too blinded by the transformers trend, and they are still used in industry? And if its the latter, in which circumstances you see their use is idea?",NEG
490,o9358u,How to create a virtual assistant with Python,/r/Python/comments/o92y9z/how_to_create_a_virtual_assistant_with_python/,limapedro,LanguageTechnology,1970-01-01 00:00:01.624819925,,0,0,,NEU
491,o90erf,For loop: DataFrame and SpaCy,https://www.reddit.com/r/LanguageTechnology/comments/o90erf/for_loop_dataframe_and_spacy/,c_metaphorique,LanguageTechnology,1970-01-01 00:00:01.624811291,,1,1,"Hello.The title of this post is probably very confusing, so I will try to explain my problem as best I can.I have a pandas DataFrame that looks like this:    	Chunk	            Root	    Dependency	Head	Classification	Tag 0	aerobics	    aerobics	    nsubj	is	NaN	        NaN 20	practitioners	    practitioners   nsubj	perform	NaN	        NaN 28	aerobics classes    classes	    nsubj	allow	NaN	        NaN 29	participants	    participants    nsubj	select	NaN	        NaN 33	many gyms	    gyms	    nsubj	offer	NaN	        NaN      What I would like to do is the following:* Iterate over the `Root` column in the data frame.* Take the tokens that are in the `Root` column and get their POS tag using the SpaCy module `.tag_`* Add the POS tags from SpaCy to the `Tag` columnSo the output would then look like this:           Chunk	            Root	    Dependency	Head	Classification	Tag 0	aerobics	    aerobics	    nsubj	is	NaN	        NN 20	practitioners	    practitioners   nsubj	perform	NaN	        NNS 28	aerobics classes    classes	    nsubj	allow	NaN	        NNS 29	participants	    participants    nsubj	select	NaN	        NNS 33	many gyms	    gyms	    nsubj	offer	NaN	        NNS In theory, I think it is easy enough to iterate over a DataFrame. It would be something like this, right?    for token in DataFrame[2]:             ... My questions are:How do I get SpaCy to recognise that information in the DataFrame? The module only accepts string types, not a DataFrame.How do I work the SpaCy module into my for loop?Thank you in advance for your help.",POS
492,o8kffa,NLP Project: Extract generics from a corpus,https://www.reddit.com/r/LanguageTechnology/comments/o8kffa/nlp_project_extract_generics_from_a_corpus/,c_metaphorique,LanguageTechnology,1970-01-01 00:00:01.624746137,,7,4,"Hi.I am a linguist trying to become a computational linguist.I am currently working with the [WikiGenerics corpus](https://www.coli.uni-saarland.de/projects/sitent/page.php?id=resources).Essentially, there are a bunch of .csv files that have been annotated as ""GENERIC"" or ""NON-GENERIC"".What I want to do is to parse the sentences that are tagged as GENERIC and then extract the subjects from those sentences.So far, using Python, I have:* merged all the separate .csv files into one .csv file* saved the .csv file into a DataFrame* converted the DataFrame into a list of lists* filtered the sentences that have been tagged as GENERIC* concatenated all the sentencesUsing SpaCy, I have:* parsed the sentences* split the sentences so that each sentence is its own element in a listWhat I would like to do now is:* have it so that I can get the individual sentences AND THEIR parses into a DataFrame* mark dependencies (i.e., it's not enough that I have the-DET-det cat-NOUN-nsubj   , I want to be able to group \[the cat\]I'm not sure how I can accomplish those last two bullet points. For the first one, I'm guessing some kind of simple for loop into an empty list would do the trick.For the second one, really, I have no idea. My guess is that there's a module in SpaCy that could it, but I'm not sure.Does anyone have any advice? I'm all ears. (And if you think I could do this in something other than Python, I'd be happy to hear that, too.)I can also share my notebook if needed. (Although I'm in Central Europe, so will be going to bed shortly :))Thanks in advance.",POS
493,o86f35,Call for Participation to NL-Augmenter 🦎 → 🐍,https://www.reddit.com/r/LanguageTechnology/comments/o86f35/call_for_participation_to_nlaugmenter/,Ganglion_Varicose,LanguageTechnology,1970-01-01 00:00:01.624694066,,26,2,"Hi  r/LanguageTechnology Members!We, a team of researchers spanning Google AI Language, UW, CMU and 7 other institutions organizing NL-Augmenter 🦎 → 📷. , are now inviting  transformation submissions to the same!  All submitters of accepted transformations (and filters) will be included as co-authors on a paper announcing this framework. NL-Augmenter 🦎 → 📷 is a part of the wider GEM benchmark,  [GEM (Generation, Evaluation, Metrics)](https://gem-benchmark.com/nl_augmenter) workshop at ACL, 2021 and their future iterations.The NL-Augmenter is a collaborative effort intended to add transformations of datasets dealing with natural language. Transformations augment text datasets in diverse ways, including: introducing spelling errors, translating to a different language, randomizing names and numbers, paraphrasing ... and whatever creative augmentation you contribute to the benchmark. We invite submissions of transformations to this framework by way of GitHub pull request, through **September 1, 2021**. **All submitters of accepted transformations (and filters) will be included as co-authors on a paper announcing this framework.**Project: [https://github.com/GEM-benchmark/NL-Augmenter](https://github.com/GEM-benchmark/NL-Augmenter)We strongly believe that the benefits of open science should reach everyone and hence we are making this effort to reach you. We also encourage you to share this with other researchers in your department who would benefit from this open collaboration. To know more about the framework, check our [motivation and review criteria](https://github.com/GEM-benchmark/NL-Augmenter/blob/main/docs/doc.md) and some of [our recent work](https://arxiv.org/pdf/2106.09069.pdf).Organizers:Kaustubh Dhole (Amelia R&D) , Sebastian Gehrmann (Google AI Language), Varun Gangal (LTI, Carnegie Mellon University), Jascha Sohl-Dickstein (Google Brain), Tonghuang Wu (University of Washington), Simon Mille (Universitat Pompeu Fabra) , Zhenhao Li (Imperial College, London), Saad Mahmood (Trivago R&D), Aadesh Gupta (Amelia R&D), Samson Tan (Salesforce Research), Jinho Choi (Emory University)",POS
494,o8iot7,Which ML algorithm is used by Kira for their smart fields,https://www.reddit.com/r/LanguageTechnology/comments/o8iot7/which_ml_algorithm_is_used_by_kira_for_their/,gevezex,LanguageTechnology,1970-01-01 00:00:01.624740216,,2,0,"Kira software calls it smart fields to filter passages from contracts to identify sections like ""Termination for convenience"", ""Notice"". ""automatic contract renewal"" and about 1000 other smart fields (they call it like that).Does anyone know what kind of ML algorithm they use? It looks very fast what they do. I guess it is something like relation extraction but it covers whole paragraphs in this example.Here is a product walkthrough for the how it works in their software.[https://www.youtube.com/watch?v=2qqVeqPzTuw](https://www.youtube.com/watch?v=2qqVeqPzTuw)",POS
495,o8ga3t,Why do you need a threshold when tokenizing a text corpus?,https://www.reddit.com/r/LanguageTechnology/comments/o8ga3t/why_do_you_need_a_threshold_when_tokenizing_a/,stuffingmybrain,LanguageTechnology,1970-01-01 00:00:01.624732128,,3,4,"So I'm a self-learning NLP and came across [this kaggle notebook](https://www.kaggle.com/singhabhiiitkgp/text-summarization-using-lstm) that does text summarization using an LSTM. When it makes an \`orderedDict\` of words to integers, there's some code that apparently calculates the percentage of rare words in the vocabulary. Why is there a threshold value of 4 there? As far as I can see, the word to integer mappings are arbitrary (unless each integer = number of times the word was repeated), so the threshold value of 4 seems a bit arbitrary to me. Thanks in advance for helping :)",POS
496,o8c1ao,We made a custom search engine for Stack Overflow content (With Elastic Search and custom algorithms),https://searchoverflow.com,SearchOverflow,LanguageTechnology,1970-01-01 00:00:01.624718389,,6,0,,NEU
497,o84vzk,Thieves on Sesame Street! Model Extraction of BERT-based APIs (Research Paper Walkthrough),https://www.reddit.com/r/LanguageTechnology/comments/o84vzk/thieves_on_sesame_street_model_extraction_of/,prakhar21,LanguageTechnology,1970-01-01 00:00:01.624686596,,8,2,Can we steal/extract a SOTA Machine Learning Model via just the Prediction Apis? and that too with a budget under 500$? This paper showcases exactly that by exploiting the models trained via transfer learning methods. Author's also proposes multiple defense strategies like Membership classification and API watermarking to make these models robust to model extraction attacks.https://youtu.be/ueC2a3hlBVs,POS
498,o80kix,The loss jump up suddenly,https://www.reddit.com/r/LanguageTechnology/comments/o80kix/the_loss_jump_up_suddenly/,warrenyuan,LanguageTechnology,1970-01-01 00:00:01.624669011,,6,8,"Hi everyone,Recently, when I train a Transformer for summarization, I meet some problems. At first, the loss decline as normal, then, suddenly, it jumps up a lot (not gradually increase, just changes in a step), and then, it begins to decline again.  I am sure the data is correct, and I found when I use a larger batch size, this phenomenon will occur later (but still occur). What should I do?Thank you very much!",NEG
499,o7uuat,BERT + clustering(KMeans),https://www.reddit.com/r/LanguageTechnology/comments/o7uuat/bert_clusteringkmeans/,OrranaLhaynher,LanguageTechnology,1970-01-01 00:00:01.624649817,,6,10,"Hi, folks!I'm new to NLP and topic modeling and in one of my tests I decided to use BERT+clustering(KMeans), and I'm getting unexciting results.My code is as below:BERT and cluster functions    def bert_model(data):        model = SentenceTransformer('bert-base-multilingual-uncased')        vec = np.array(model.encode(data, show_progress_bar=False))        return vec        def cluster(vec, k):        cluster_model = KMeans(k)        cluster_model.fit(vec)        lbs = cluster_model.predict(vec)        return lbsAutoencoder to reduce dimensionality    #BERT    bert_vec = bert_model(data.full_text)        #Autoencoder    AE = Autoencoder()    AE.fit(bert_vec)    vec_autoencoder = AE.encoder.predict(bert_vec)Clustering and calculating metrics    n_topics = 12    tokens = data_tokens.full_text    dictionary = corpora.Dictionary(tokens)            #Clustering    lbs = cluster(vec_autoencoder, n_topics)        #Lists of metrics     coherence = get_coherence(tokens, lbs, dictionary)    silhouette = silhouette_score(vec_autoencoder, lbs)As a result I am having a coherence of 0.374 and a silhouette of 0.089. Is there anything wrong with my code or is there anything I can do to improve these results without modifying the structure?Obs.: I'm using BERT fromSentenceTransformer and calculating coherence and silhouette with gensim and sklearn, respectively.",POS
